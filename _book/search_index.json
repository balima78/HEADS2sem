[
["index.html", "HEADS 2nd semester", " HEADS 2nd semester Bruno A Lima 2020-06-09 "],
["preface.html", "Preface", " Preface This is a book written in Markdown through RStudio. The bookdown package (Xie 2015) can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) In this book, I will try to compile all the homeworks from the 2nd semester 2019/2020 of HEADS PhD programme. An exhaustive explanation using the bookdown package (Xie 2020) can be found at bookdown: Authoring Books and Technical Documents with R Markdown and this is only a sample book, which was built on top of R Markdown and knitr This book is publish through NETLIFY as described by C.M. References "],
["intro.html", "1 Introduction", " 1 Introduction All the files and code of this book are available in the repository: https://github.com/balima78/HEADS2sem. All the students can contribute to this compilation sending me their own homeworks and correcting the mistakes I will certainly do. You can send me your files and code to balima78@gmail.com. When possible you must send me .Rmd files and data as .csv. Here, .Rmd files are named by number from 1000 to 9000 (CLLS - Class LLesson Student) followed by a breave description. You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 6. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 2. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 1.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 1.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 1.1. knitr::kable( head(iris, 6), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 1.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2020) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["compstat.html", "2 COMPSTAT", " 2 COMPSTAT Estatística Computacional Conteúdos programáticos Estatística computacional Porque usar computação em estatística? Ferramentas e software para estatística computacional Estatística computacional utilizando grandes infra-estruturas de dados Sinopses de dados Estatísticas suficientes Histogramas Micro-Clusters Fading Statistics Estimativas de densidade Máxima verosimilhança Expectation-Maximization Kernel Estimation Estimativas e Simulação Métodos Jackknife Validação cruzada Geração de números aleatórios Métodos de Monte Carlo Métodos de Bootstrap Análise numérica Visualização de dados complexos Análise de componentes principais Bivariate smoothing Splines "],
["eda.html", "2.1 EDA", " 2.1 EDA 2020-02-03 Exercises Solutions (just copied teacher’s solutions) Open the database Anesthesia-BD.csv data = read.csv(&quot;2.UploadedData/Anesthesia-BD.csv&quot;) ###load dat str(data) ###see data structure ## &#39;data.frame&#39;: 387 obs. of 9 variables: ## $ Subject : Factor w/ 387 levels &quot;Sbj001&quot;,&quot;Sbj002&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 1 2 2 2 2 2 2 1 ... ## $ Age : int 50 67 71 51 69 78 70 58 80 63 ... ## $ TypeSurgery : Factor w/ 3 levels &quot;CAB&quot;,&quot;CAB + Valve&quot;,..: 3 1 1 3 1 2 1 3 3 2 ... ## $ SurgStatus : Factor w/ 2 levels &quot;Elective&quot;,&quot;Urgent&quot;: 1 1 2 1 2 2 1 2 1 2 ... ## $ Diastolic : num 45.5 52.6 56.7 71.4 58.1 ... ## $ Systolic : num 112.1 109 98.2 150.8 131.5 ... ## $ Cross_Clamp_Time: int 83 67 83 51 63 114 94 138 42 172 ... ## $ AdvEvent : int 0 0 0 0 1 0 0 0 0 0 ... Factor variable AdvEvent data$AdvEvent &lt;- factor(data$AdvEvent, levels = c(0,1),labels=c(&quot;No&quot;,&quot;Yes&quot;)) For the columns which contain numeric values, create a new summary table in which the rows are the mean, the standard deviation, and the median of each of the numeric columns. T &lt;- sapply(data[,sapply(data,is.numeric)],function(x){ return(rbind(mean(x,na.rm = T),sd(x,na.rm = T),median(x,na.rm = T))) } ) rownames(T) = c(&quot;Mean&quot;,&quot;SD&quot;,&quot;Median&quot;) T ## Age Diastolic Systolic Cross_Clamp_Time ## Mean 67.27390 60.93076 132.04705 72.00521 ## SD 11.25574 11.35387 21.22864 32.04680 ## Median 67.00000 60.52591 130.00410 67.00000 Considering only the subjects who had CAB or Valve surgery, how many had adverse events? How many males and females in the two groups (having or not having adverse events)? (count and percentage) # create an index with the observation that comply with the conditions ind = which(data$TypeSurgery==&quot;CAB&quot; | data$TypeSurgery==&quot;Valve&quot; ) # table these data counts table(data[ind,&quot;AdvEvent&quot;]) ## ## No Yes ## 231 80 # and frequencies prop.table(table(data[ind,&quot;AdvEvent&quot;])) ## ## No Yes ## 0.7427653 0.2572347 # now by gender table(data[ind,&quot;Gender&quot;],data[ind,&quot;AdvEvent&quot;]) ## ## No Yes ## Female 60 21 ## Male 171 59 # and their frequencies prop.table(table(data[ind,&quot;Gender&quot;],data[ind,&quot;AdvEvent&quot;])) ## ## No Yes ## Female 0.19292605 0.06752412 ## Male 0.54983923 0.18971061 4.Merge the two datasets (Anesthesia-BD.csv and Anesthesia-BD2.csv) by the subject identification number. data2 = read.csv(&quot;2.UploadedData/Anesthesia-BD2.csv&quot;) ###load data Anesthesia-BD2.csv # I had to rename the first column, I don&#39;t know why names(data2)[1]&lt;-&quot;SubjectID&quot; str(data2) ## &#39;data.frame&#39;: 363 obs. of 7 variables: ## $ SubjectID : Factor w/ 363 levels &quot;Sbj001&quot;,&quot;Sbj002&quot;,..: 3 25 30 32 36 39 46 56 60 63 ... ## $ Diabetes : int 0 0 1 1 0 0 0 1 0 0 ... ## $ RFLastA1cLevel : num 5.7 5.6 7.1 6.7 6.7 6.3 6 6.5 5.3 4.9 ... ## $ ChronicLungDisease: Factor w/ 4 levels &quot;Mild&quot;,&quot;Moderate&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Hypertension : int 1 1 1 1 1 1 1 1 1 1 ... ## $ CHF : int 0 1 1 1 1 1 1 0 1 1 ... ## $ Euroscore : Factor w/ 330 levels &quot;#N/A&quot;,&quot;0.501743134&quot;,..: 8 64 4 92 249 36 195 125 214 224 ... The merge must be such that only the common subjects should be present in the final database (natural join). Ma = merge(data,data2,by.x = &quot;Subject&quot;,by.y=&quot;SubjectID&quot;,all = F) dim(Ma) ## [1] 363 15 The merge must be such that if some subject is not in one of the databases, the subject should be in the database, and missing information must be not available (full outer join). Mb = merge(data,data2,by.x = &quot;Subject&quot;,by.y=&quot;SubjectID&quot;,all = T) dim(Mb) ## [1] 387 15 Consider the following statement: “For people without diabetes, the normal range for the hemoglobin A1c level is between 4% and 5.6%. Hemoglobin A1c levels between 5.7% and 6.4% mean you have a higher chance of getting diabetes. Levels of 6.5% or higher mean you have diabetes.” Create a new variable with three factors (normal, prediabetes, and diabetes), taking into consideration the values the A1c levels. Compare the results obtained with the variable Diabetes (assuming that 1 means to have diabetes and 0 no diabetes). diab &lt;- ifelse(Mb$RFLastA1cLevel&lt;5.7,0,ifelse(Mb$RFLastA1cLevel&lt;6.4,1,2)) Mb$Diabetes2 &lt;- factor(x = diab, levels = 0:2, labels =c(&quot;Normal&quot;,&quot;PreDiabetes&quot;,&quot;Diabetes&quot;)) Mb$Diabetes &lt;- factor(Mb$Diabetes, levels=0:1, labels=c(&quot;Normal&quot;,&quot;Diabetes&quot;)) table(Mb$Diabetes,Mb$Diabetes2) ## ## Normal PreDiabetes Diabetes ## Normal 120 99 11 ## Diabetes 7 35 83 Create a table with the comparison between the groups having or not having adverse events for all the variables available in the combined database. Use the appropriate measures for each type of variable. str(Mb) ## &#39;data.frame&#39;: 387 obs. of 16 variables: ## $ Subject : Factor w/ 387 levels &quot;Sbj001&quot;,&quot;Sbj002&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 1 2 2 2 2 2 2 1 ... ## $ Age : int 50 67 71 51 69 78 70 58 80 63 ... ## $ TypeSurgery : Factor w/ 3 levels &quot;CAB&quot;,&quot;CAB + Valve&quot;,..: 3 1 1 3 1 2 1 3 3 2 ... ## $ SurgStatus : Factor w/ 2 levels &quot;Elective&quot;,&quot;Urgent&quot;: 1 1 2 1 2 2 1 2 1 2 ... ## $ Diastolic : num 45.5 52.6 56.7 71.4 58.1 ... ## $ Systolic : num 112.1 109 98.2 150.8 131.5 ... ## $ Cross_Clamp_Time : int 83 67 83 51 63 114 94 138 42 172 ... ## $ AdvEvent : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 2 1 1 1 1 1 ... ## $ Diabetes : Factor w/ 2 levels &quot;Normal&quot;,&quot;Diabetes&quot;: 2 1 1 1 1 1 1 1 1 2 ... ## $ RFLastA1cLevel : num 7.9 5.5 5.7 6 5.8 NA 5.9 6 5.2 11.2 ... ## $ ChronicLungDisease: Factor w/ 4 levels &quot;Mild&quot;,&quot;Moderate&quot;,..: 3 3 1 3 3 3 3 3 3 3 ... ## $ Hypertension : int 1 1 1 0 1 0 1 1 1 0 ... ## $ CHF : int 1 0 0 0 0 1 1 1 1 1 ... ## $ Euroscore : Factor w/ 330 levels &quot;#N/A&quot;,&quot;0.501743134&quot;,..: 47 206 8 289 159 96 119 98 77 4 ... ## $ Diabetes2 : Factor w/ 3 levels &quot;Normal&quot;,&quot;PreDiabetes&quot;,..: 3 1 2 2 2 NA 2 2 1 3 ... Mb$Hypertension &lt;- factor(Mb$Hypertension, levels=0:1, labels=c(&quot;No&quot;,&quot;Yes&quot;)) Mb$CHF &lt;- factor(Mb$CHF, levels=0:1, labels=c(&quot;No&quot;,&quot;Yes&quot;)) Mb$Subject &lt;- as.character(Mb$Subject) #windows() par(mfrow=c(2,3)) for (i in which(sapply(Mb,is.numeric))){ hist(Mb[,i],xlab=colnames(Mb)[i]) } par(mfrow=c(1,1)) table &lt;- by(Mb[,which(sapply(Mb,is.numeric))], INDICES = Mb$AdvEvent, FUN = function(x) apply(x,2,quantile,na.rm=T)) table ## Mb$AdvEvent: No ## Age Diastolic Systolic Cross_Clamp_Time RFLastA1cLevel ## 0% 25 26.42047 88.70903 15 4.7 ## 25% 59 53.30694 115.03977 51 5.6 ## 50% 67 60.75382 129.50300 68 5.8 ## 75% 76 68.64874 143.12238 83 6.4 ## 100% 92 94.35495 180.95547 241 12.2 ## ------------------------------------------------------------ ## Mb$AdvEvent: Yes ## Age Diastolic Systolic Cross_Clamp_Time RFLastA1cLevel ## 0% 24 39.42266 92.34727 13.0 4.600 ## 25% 62 52.93295 119.55877 48.0 5.500 ## 50% 69 59.94571 131.48461 64.0 5.800 ## 75% 77 65.97349 153.50990 88.5 6.425 ## 100% 88 92.51766 193.75080 298.0 11.700 table$pvalue &lt;- apply(Mb[,which(sapply(Mb,is.numeric))],2, function(x) wilcox.test(x[which(Mb$AdvEvent==&quot;No&quot;)],x[which(Mb$AdvEvent==&quot;Yes&quot;)])$p.value) table ## Age Diastolic Systolic Cross_Clamp_Time RFLastA1cLevel ## 0% 25 26.42047 88.70903 15 4.7 ## 25% 59 53.30694 115.03977 51 5.6 ## 50% 67 60.75382 129.50300 68 5.8 ## 75% 76 68.64874 143.12238 83 6.4 ## 100% 92 94.35495 180.95547 241 12.2 ## ------------------------------------------------------------ ## Age Diastolic Systolic Cross_Clamp_Time RFLastA1cLevel ## 0% 24 39.42266 92.34727 13.0 4.600 ## 25% 62 52.93295 119.55877 48.0 5.500 ## 50% 69 59.94571 131.48461 64.0 5.800 ## 75% 77 65.97349 153.50990 88.5 6.425 ## 100% 88 92.51766 193.75080 298.0 11.700 ## ------------------------------------------------------------ ## Age Diastolic Systolic Cross_Clamp_Time ## 0.09297764 0.41375034 0.09079886 0.76575185 ## RFLastA1cLevel ## 0.78804579 Mb$Euroscore&lt;-as.character(Mb$Euroscore) table2 &lt;- by(Mb[,which(sapply(Mb,is.factor))], INDICES = Mb$AdvEvent, FUN = function(x){sapply(x,table)}) table2 ## Mb$AdvEvent: No ## $Gender ## ## Female Male ## 78 200 ## ## $TypeSurgery ## ## CAB CAB + Valve Valve ## 156 47 75 ## ## $SurgStatus ## ## Elective Urgent ## 148 130 ## ## $AdvEvent ## ## No Yes ## 278 0 ## ## $Diabetes ## ## Normal Diabetes ## 170 93 ## ## $ChronicLungDisease ## ## Mild Moderate No Severe ## 34 4 226 1 ## ## $Hypertension ## ## No Yes ## 49 215 ## ## $CHF ## ## No Yes ## 186 79 ## ## $Diabetes2 ## ## Normal PreDiabetes Diabetes ## 91 100 70 ## ## ------------------------------------------------------------ ## Mb$AdvEvent: Yes ## $Gender ## ## Female Male ## 31 78 ## ## $TypeSurgery ## ## CAB CAB + Valve Valve ## 41 29 39 ## ## $SurgStatus ## ## Elective Urgent ## 64 45 ## ## $AdvEvent ## ## No Yes ## 0 109 ## ## $Diabetes ## ## Normal Diabetes ## 65 33 ## ## $ChronicLungDisease ## ## Mild Moderate No Severe ## 16 0 79 3 ## ## $Hypertension ## ## No Yes ## 20 78 ## ## $CHF ## ## No Yes ## 50 47 ## ## $Diabetes2 ## ## Normal PreDiabetes Diabetes ## 36 35 25 "],
["streams.html", "2.2 Streams", " 2.2 Streams 2020-02-10 Classe Exercises (code given by the professor) library(stream) ## Loading required package: proxy ## ## Attaching package: &#39;proxy&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## as.dist, dist ## The following object is masked from &#39;package:base&#39;: ## ## as.matrix #par(mfrow=c(2,2)) keep sample mean definitions itera &lt;- 2000 lbound &lt;- -3 ubound &lt;- 8 mu1 &lt;- 0 mu2 &lt;- 5 set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=0, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=10, sigma=2) iteration # initialization n &lt;- sum &lt;- 0 plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(-3,13), main=&quot;Sample Mean&quot;) # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) sum &lt;- sum + xi n &lt;- n + 1 points(x=i, y=xi, pch=20, cex=.1, col=&quot;black&quot;) points(x=i, y=sum / n, pch=20, cex=.5, col=&quot;red&quot;) } keep sample mean over a sliding window definitions w &lt;- 100 set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=0, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=10, sigma=2) iteration # initialization n &lt;- sum &lt;- 0 plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(-3,13), main=&quot;Sliding Mean&quot;) window &lt;- NULL # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) sum &lt;- sum + xi n &lt;- n + 1 window &lt;- c(window, xi) if (length(window) &gt; w) { sum &lt;- sum - window[1] n &lt;- n - 1 window &lt;- window[-1] } points(x=i, y=xi, pch=20, cex=.1, col=&quot;black&quot;) points(x=i, y=sum / n, pch=20, cex=.5, col=&quot;red&quot;) } keep sample mean over a alpha weighted sliding window definitions w &lt;- 100 eps &lt;- 0.05 alpha &lt;- eps^(1/w) set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=0, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=10, sigma=2) iteration # initialization n &lt;- sum &lt;- 0 plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(-3,13), main=&quot;Weighted Sliding Mean&quot;) window &lt;- NULL # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) sum &lt;- sum * alpha + xi n &lt;- n * alpha + 1 window &lt;- c(window, xi) if (length(window) &gt; w) { sum &lt;- sum - window[1] * alpha ^ (w-1) n &lt;- n - 1 * alpha ^ (w-1) window &lt;- window[-1] } points(x=i, y=xi, pch=20, cex=.1, col=&quot;black&quot;) points(x=i, y=sum / n, pch=20, cex=.5, col=&quot;red&quot;) } keep sample mean over a alpha fading sliding window definitions w &lt;- 100 eps &lt;- 0.05 alpha &lt;- eps^(1/w) set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=0, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=10, sigma=2) iteration # initialization n &lt;- sum &lt;- 0 plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(-3,13), main=&quot;Fading Sliding Mean&quot;) window &lt;- NULL # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) sum &lt;- sum * alpha + xi n &lt;- n * alpha + 1 points(x=i, y=xi, pch=20, cex=.1, col=&quot;black&quot;) points(x=i, y=sum / n, pch=20, cex=.5, col=&quot;red&quot;) } "],
["assignment-histogram.html", "2.3 assignment Histogram", " 2.3 assignment Histogram lesson 2 (cont.) professor’s solution 2020-02-13 Using the stream package in R, keep current sample histogram of a 1-dimensional data stream using different window models. keep sample histogram defenitions library(stream) #par(mfrow=c(2,2)) # definitions itera &lt;- 2000 lbound &lt;- -3 ubound &lt;- 8 nbins &lt;- 10 mu1 &lt;- 0 mu2 &lt;- 5 iteration # initialization set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=mu1, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=mu2, sigma=2) splits &lt;- seq(from=lbound, to=ubound, by=(ubound-lbound)/nbins) splits &lt;- c(splits, +Inf) # extra split for comparison bins &lt;- rep(0, length(splits)) names(bins) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(lbound,ubound), main=&quot;Sample Distribution Heat Map&quot;) cat(&quot;\\n*************************\\n&quot;, &quot;Sample Distribution&quot;,&quot;\\n&quot;) ## ## ************************* ## Sample Distribution # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) bin &lt;- which(xi &lt;= splits)[1] bins[bin] &lt;- bins[bin] + 1 sapply(2:(length(splits)-1), function(u){ upper &lt;- splits[u] lower &lt;- splits[u-1] lines(x=c(i,i), y=c(lower,upper), lwd=2, col=gray(1 - bins[u] / sum(bins))) }) if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(bins) } } ## ## ### Iteration 500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 ## 13 43 90 141 137 66 9 1 0 0 0 ## &lt;=Inf ## 0 ## ## ### Iteration 1000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 ## 19 64 184 311 279 123 18 2 0 0 0 ## &lt;=Inf ## 0 ## ## ### Iteration 1500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 ## 19 65 184 311 281 142 73 135 158 85 36 ## &lt;=Inf ## 11 ## ## ### Iteration 2000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 ## 19 65 184 311 284 159 121 262 309 198 71 ## &lt;=Inf ## 17 keep sample histogram over a sliding window definitions and iteration # definitions w &lt;- 500 # initialization set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=mu1, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=mu2, sigma=2) splits &lt;- seq(from=lbound, to=ubound, by=(ubound-lbound)/nbins) splits &lt;- c(splits, +Inf) # extra split for comparison bins &lt;- rep(0, length(splits)) names(bins) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) window &lt;- NULL plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(lbound,ubound), main=&quot;Sliding Distribution Heat Map&quot;) cat(&quot;\\n*************************\\n&quot;, &quot;Sliding Distribution&quot;,&quot;\\n&quot;) ## ## ************************* ## Sliding Distribution # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) bin &lt;- which(xi &lt;= splits)[1] bins[bin] &lt;- bins[bin] + 1 window &lt;- c(window, xi) if (length(window) &gt; w) { oldbin &lt;- which(window[1] &lt;= splits)[1] bins[oldbin] &lt;- bins[oldbin] - 1 window &lt;- window[-1] } sapply(2:(length(splits)-1), function(u){ upper &lt;- splits[u] lower &lt;- splits[u-1] lines(x=c(i,i), y=c(lower,upper), lwd=2, col=gray(1 - bins[u] / sum(bins))) }) if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(bins) } } ## ## ### Iteration 500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 ## 13 43 90 141 137 66 9 1 0 0 0 ## &lt;=Inf ## 0 ## ## ### Iteration 1000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 ## 6 21 94 170 142 57 9 1 0 0 0 ## &lt;=Inf ## 0 ## ## ### Iteration 1500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 ## 0 1 0 0 2 19 55 133 158 85 36 ## &lt;=Inf ## 11 ## ## ### Iteration 2000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 ## 0 0 0 0 3 17 48 127 151 113 35 ## &lt;=Inf ## 6 keep sample histogram over a alpha weighted sliding window definitions w &lt;- 500 eps &lt;- 0.05 alpha &lt;- eps^(1/w) iteration # initialization set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=mu1, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=mu2, sigma=2) splits &lt;- seq(from=lbound, to=ubound, by=(ubound-lbound)/nbins) splits &lt;- c(splits, +Inf) # extra split for comparison bins &lt;- rep(0, length(splits)) names(bins) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) window &lt;- NULL plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(lbound,ubound), main=&quot;Weighted Distribution Heat Map&quot;) cat(&quot;\\n*************************\\n&quot;, &quot;Weighted Distribution&quot;,&quot;\\n&quot;) ## ## ************************* ## Weighted Distribution # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) bin &lt;- which(xi &lt;= splits)[1] bins &lt;- bins * alpha bins[bin] &lt;- bins[bin] + 1 window &lt;- c(window, xi) if (length(window) &gt; w) { oldbin &lt;- which(window[1] &lt;= splits)[1] bins[oldbin] &lt;- bins[oldbin] - alpha^w window &lt;- window[-1] } sapply(2:(length(splits)-1), function(u){ upper &lt;- splits[u] lower &lt;- splits[u-1] lines(x=c(i,i), y=c(lower,upper), lwd=2, col=gray(1 - bins[u] / sum(bins))) }) if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(bins) } } ## ## ### Iteration 500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 ## 3.2949751 16.9824961 29.3163498 44.6117439 41.4464742 19.9554876 3.2883843 ## &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 &lt;=Inf ## 0.1384587 0.0000000 0.0000000 0.0000000 0.0000000 ## ## ### Iteration 1000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 ## 1.8919283 5.7544636 34.7204532 49.2432118 47.7857598 15.7306842 3.7872339 ## &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 &lt;=Inf ## 0.1206349 0.0000000 0.0000000 0.0000000 0.0000000 ## ## ### Iteration 1500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 ## 2.536993e-17 1.959926e-01 -2.532696e-15 3.832639e-15 5.080926e-01 ## &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 ## 7.867248e+00 1.678585e+01 3.859057e+01 5.039598e+01 2.697502e+01 ## &lt;=8 &lt;=Inf ## 1.327758e+01 4.438031e+00 ## ## ### Iteration 2000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 ## 1.268497e-18 -1.087978e-17 -1.266348e-16 1.916320e-16 6.979810e-01 ## &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 ## 5.225829e+00 1.513803e+01 4.131708e+01 5.152468e+01 3.188672e+01 ## &lt;=8 &lt;=Inf ## 1.020274e+01 3.041313e+00 keep sample histogram over a alpha fading window definitions w &lt;- 500 eps &lt;- 0.05 alpha &lt;- eps^(1/w) iteration # initialization set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=mu1, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=mu2, sigma=2) splits &lt;- seq(from=lbound, to=ubound, by=(ubound-lbound)/nbins) splits &lt;- c(splits, +Inf) # extra split for comparison bins &lt;- rep(0, length(splits)) names(bins) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(lbound,ubound), main=&quot;Fading Distribution Heat Map&quot;) cat(&quot;\\n*************************\\n&quot;, &quot;Fading Distribution&quot;,&quot;\\n&quot;) ## ## ************************* ## Fading Distribution # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) bin &lt;- which(xi &lt;= splits)[1] bins &lt;- bins * alpha bins[bin] &lt;- bins[bin] + 1 sapply(2:(length(splits)-1), function(u){ upper &lt;- splits[u] lower &lt;- splits[u-1] lines(x=c(i,i), y=c(lower,upper), lwd=2, col=gray(1 - bins[u] / sum(bins))) }) if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(bins) } } ## ## ### Iteration 500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 ## 3.2949751 16.9824961 29.3163498 44.6117439 41.4464742 19.9554876 3.2883843 ## &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 &lt;=Inf ## 0.1384587 0.0000000 0.0000000 0.0000000 0.0000000 ## ## ### Iteration 1000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 ## 2.0566770 6.6035884 36.1862707 51.4737990 49.8580835 16.7284586 3.9516531 ## &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 &lt;=Inf ## 0.1275578 0.0000000 0.0000000 0.0000000 0.0000000 ## ## ### Iteration 1500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 ## 0.1028339 0.5261720 1.8093135 2.5736899 3.0009968 8.7036712 16.9834370 ## &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 &lt;=Inf ## 38.5969517 50.3959783 26.9750167 13.2775825 4.4380306 ## ## ### Iteration 2000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 ## 0.005141693 0.026308601 0.090465677 0.128684497 0.848030834 5.661012898 ## &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 &lt;=Inf ## 15.987200946 43.246924516 54.044482016 33.235467534 10.866619313 3.263214833 "],
["assignment-matrix.html", "2.4 assignment Matrix", " 2.4 assignment Matrix Assignment 1 (deadline Feb 17) 2020-02-15 Using R, keep counters of equal-width grid-cells (base counters for micro-cluster definitions) of a 2-dimensional continuous data stream using different window models (landmark, sliding, weighted, fading). The code should work with an evolving stream from a single record of the PhysioNet Challenge https://physionetchallenges.github.io/2020/ This training set consists of 6,877 (male: 3,699; female: 3,178) 12-ECG recordings lasting from 6 seconds to 60 seconds. Each recording was sampled at 500 Hz. All data is provided in WFDB format with a MATLAB v4 file and a header containing patient sex, age, and diagnosis (Dx) information at the end of the header file. The code should be applicable to one 12-dimensional record file. library(stream) library(R.matlab) ## R.matlab v3.6.2 (2018-09-26) successfully loaded. See ?R.matlab for help. ## ## Attaching package: &#39;R.matlab&#39; ## The following object is masked from &#39;package:stream&#39;: ## ## evaluate ## The following objects are masked from &#39;package:base&#39;: ## ## getOption, isOpen # after unzip the files, I took the first file fA1&lt;-readMat(&quot;2.UploadedData/A0001.mat&quot;)$val fA1&lt;-t(fA1) I selected two columns (4 and 5) corresponding to the readings and make the streams # select columns a&lt;-4 b&lt;-5 # data stream strA1&lt;-DSD_Memory(fA1[,c(a,b)], loop = T) definitions set.seed(1) reset_stream(strA1) # number of iterations itera &lt;- 2000 #defining bounds for matrix lbound &lt;- min(strA1[[2]]) ubound &lt;- max(strA1[[2]]) nbins &lt;- 10 # first we create two equal size vectors splits &lt;- seq(from=lbound, to=ubound, by=(ubound-lbound)/nbins) splits &lt;- c(splits)[-1] # drop first value binsA &lt;- rep(0, length(splits)) names(binsA) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) binsB &lt;- rep(0, length(splits)) names(binsB) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) # and then a matrix 10x10 mx&lt;-matrix(0,10,10) rownames(mx)&lt;-colnames(mx)&lt;-names(binsA) 2.4.1 keep sample matrix iteration # iterative step for (i in 1:itera){ str&lt;-get_points(strA1) ai &lt;- str[1,1] bi &lt;- str[1,2] binA &lt;- which(ai &lt;= splits)[1] binsA[binA] &lt;- binsA[binA] + 1 binB &lt;- which(bi &lt;= splits)[1] binsB[binB] &lt;- binsB[binB] + 1 mx[binA,binB]&lt;-mx[binA,binB] + 1 # with the vectors positions, we update the matrix counts # printing the two vectors just for control if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(binsA) print(binsB) } } ## ## ### Iteration 500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 38 57 121 209 31 8 16 7 ## &lt;=464.6 &lt;=547 ## 5 8 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 26 11 85 310 58 3 7 0 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 1000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 110 105 237 372 50 25 44 19 ## &lt;=464.6 &lt;=547 ## 18 20 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 56 48 130 538 178 29 17 4 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 1500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 145 140 363 543 102 47 70 29 ## &lt;=464.6 &lt;=547 ## 33 28 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 109 64 165 840 232 52 30 8 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 2000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 204 189 459 720 177 56 83 36 ## &lt;=464.6 &lt;=547 ## 39 37 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 128 78 269 1113 311 56 35 10 ## &lt;=464.6 &lt;=547 ## 0 0 heat map with the values of the matrix heatmap(mx, Colv = NA, Rowv = NA, scale=&quot;none&quot;, symm = F, revC = T, main = &quot;Sample Distribution Heat Map&quot;) 2.4.2 keep sample matrix over a sliding window definitions and initialization set.seed(1) reset_stream(strA1) w &lt;- 500 binsA &lt;- rep(0, length(splits)) names(binsA) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) binsB &lt;- rep(0, length(splits)) names(binsB) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) mx&lt;-matrix(0,10,10) rownames(mx)&lt;-colnames(mx)&lt;-names(binsA) windowA &lt;- windowB &lt;- NULL iteration # iterative step for (i in 1:itera){ str&lt;-get_points(strA1) ai &lt;- str[1,1] bi &lt;- str[1,2] binA &lt;- which(ai &lt;= splits)[1] binsA[binA] &lt;- binsA[binA] + 1 binB &lt;- which(bi &lt;= splits)[1] binsB[binB] &lt;- binsB[binB] + 1 mx[binA,binB]&lt;-mx[binA,binB] + 1 windowA &lt;- c(windowA, ai) # defining two windows windowB &lt;- c(windowB, bi) # one for each stream if (length(windowA) &gt; w) { oldbinA &lt;- which(windowA[1] &lt;= splits)[1] binsA[oldbinA] &lt;- binsA[oldbinA] - 1 windowA &lt;- windowA[-1] oldbinB &lt;- which(windowB[1] &lt;= splits)[1] binsB[oldbinB] &lt;- binsB[oldbinB] - 1 windowB &lt;- windowB[-1] mx[oldbinA,oldbinB]&lt;-mx[oldbinA,oldbinB] - 1 # droping the first value from the window } # printing the two vectors just for control if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(binsA) print(binsB) } } ## ## ### Iteration 500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 38 57 121 209 31 8 16 7 ## &lt;=464.6 &lt;=547 ## 5 8 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 26 11 85 310 58 3 7 0 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 1000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 72 48 116 163 19 17 28 12 ## &lt;=464.6 &lt;=547 ## 13 12 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 30 37 45 228 120 26 10 4 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 1500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 35 35 126 171 52 22 26 10 ## &lt;=464.6 &lt;=547 ## 15 8 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 53 16 35 302 54 23 13 4 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 2000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 59 49 96 177 75 9 13 7 ## &lt;=464.6 &lt;=547 ## 6 9 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 19 14 104 273 79 4 5 2 ## &lt;=464.6 &lt;=547 ## 0 0 heat map with the values of the matrix heatmap(mx, Colv = NA, Rowv = NA, scale=&quot;none&quot;, symm = F, revC = T, main = &quot;Sliding Distribution Heat Map&quot;) 2.4.3 keep sample matrix over a alpha weighted sliding window definitions and initialization set.seed(1) reset_stream(strA1) w &lt;- 500 eps &lt;- 0.05 alpha &lt;- eps^(1/w) # defining an alpha value # initialization set.seed(1) binsA &lt;- rep(0, length(splits)) names(binsA) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) binsB &lt;- rep(0, length(splits)) names(binsB) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) mx&lt;-matrix(0,10,10) rownames(mx)&lt;-colnames(mx)&lt;-names(binsA) windowA &lt;- windowB &lt;- NULL iteration # iterative step for (i in 1:itera){ str&lt;-get_points(strA1) ai &lt;- str[1,1] bi &lt;- str[1,2] binA &lt;- which(ai &lt;= splits)[1] binsA &lt;-binsA * alpha binsA[binA] &lt;- binsA[binA] + 1 binB &lt;- which(bi &lt;= splits)[1] binsB &lt;-binsB * alpha binsB[binB] &lt;- binsB[binB] + 1 mx&lt;-mx * alpha #multiplying the matrix by alpha before updating it mx[binA,binB]&lt;-mx[binA,binB] + 1 # updating the matrix with a new count windowA &lt;- c(windowA, ai) windowB &lt;- c(windowB, bi) if (length(window) &gt; w) { oldbinA &lt;- which(windowA[1] &lt;= splits)[1] binsA[oldbinA] &lt;- binsA[oldbinA] - 1 windowA &lt;- windowA[-1] oldbinB &lt;- which(windowB[1] &lt;= splits)[1] binsB[oldbinB] &lt;- binsB[oldbinB] - 1 windowB &lt;- windowB[-1] mx[oldbinA,oldbinB]&lt;-mx[oldbinA,oldbinB] - alpha^w } if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(binsA) print(binsB) } } ## ## ### Iteration 500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 9.862771 12.716282 40.696190 79.798698 6.004151 1.962992 3.740664 1.514095 ## &lt;=464.6 &lt;=547 ## 1.058584 1.679942 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 ## 6.0146182 2.6308680 22.1702078 118.5148834 7.7376949 0.5858054 ## &lt;=299.8 &lt;=382.2 &lt;=464.6 &lt;=547 ## 1.3802920 0.0000000 0.0000000 0.0000000 ## ## ### Iteration 1000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 26.342501 19.498601 38.162601 57.694790 5.353712 4.551458 6.840510 2.790566 ## &lt;=464.6 &lt;=547 ## 3.056812 2.694537 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 ## 12.4932951 5.5643186 12.3378996 89.6498784 40.6335931 3.9734132 2.1006708 ## &lt;=382.2 &lt;=464.6 &lt;=547 ## 0.2330195 0.0000000 0.0000000 ## ## ### Iteration 1500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 8.272278 12.665316 48.309442 52.748796 11.978027 11.448122 9.207763 4.034632 ## &lt;=464.6 &lt;=547 ## 5.569572 3.149725 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 21.548245 7.717544 11.387921 99.318171 15.379753 5.650347 3.944561 2.437131 ## &lt;=464.6 &lt;=547 ## 0.000000 0.000000 ## ## ### Iteration 2000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 11.103121 12.089897 37.567685 66.923853 29.056388 2.525527 3.223038 1.606848 ## &lt;=464.6 &lt;=547 ## 1.437631 1.869565 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 ## 5.0810228 3.3434787 17.5203610 122.1273764 16.7587452 0.9993678 ## &lt;=299.8 &lt;=382.2 &lt;=464.6 &lt;=547 ## 1.0919750 0.4812265 0.0000000 0.0000000 heat map with the values of the matrix heatmap(mx, Colv = NA, Rowv = NA, scale=&quot;none&quot;, symm = F, revC = T, main = &quot;Weighted Distribution&quot;) 2.4.4 keep sample matrix over a alpha fading window definitions and initialization set.seed(1) reset_stream(strA1) w &lt;- 500 eps &lt;- 0.05 alpha &lt;- eps^(1/w) binsA &lt;- rep(0, length(splits)) names(binsA) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) binsB &lt;- rep(0, length(splits)) names(binsB) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) mx&lt;-matrix(0,10,10) rownames(mx)&lt;-colnames(mx)&lt;-names(binsA) iteration # iterative step for (i in 1:itera){ str&lt;-get_points(strA1) ai &lt;- str[1,1] bi &lt;- str[1,2] binA &lt;- which(ai &lt;= splits)[1] binsA[binA] &lt;- binsA[binA] + 1 binB &lt;- which(bi &lt;= splits)[1] binsB[binB] &lt;- binsB[binB] + 1 mx&lt;-mx * alpha # multiplying the matrix by alpha before updating it mx[binA,binB]&lt;-mx[binA,binB] + 1 # updating the matix if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(binsA) print(binsB) } } ## ## ### Iteration 500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 38 57 121 209 31 8 16 7 ## &lt;=464.6 &lt;=547 ## 5 8 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 26 11 85 310 58 3 7 0 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 1000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 110 105 237 372 50 25 44 19 ## &lt;=464.6 &lt;=547 ## 18 20 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 56 48 130 538 178 29 17 4 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 1500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 145 140 363 543 102 47 70 29 ## &lt;=464.6 &lt;=547 ## 33 28 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 109 64 165 840 232 52 30 8 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 2000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 204 189 459 720 177 56 83 36 ## &lt;=464.6 &lt;=547 ## 39 37 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 128 78 269 1113 311 56 35 10 ## &lt;=464.6 &lt;=547 ## 0 0 heat map with the values of the matrix heatmap(mx, Colv = NA, Rowv = NA, scale=&quot;none&quot;, symm = F, revC = T, main = &quot;Fading Distribution&quot;) 2.4.5 Bonus for a diferent solution https://rpubs.com/franzbischoff/hida_assign1 "],
["density-estimation.html", "2.5 Density Estimation", " 2.5 Density Estimation 2020-02-17 Classe Exercises (almost all code given by the professor) knitr::opts_chunk$set(echo = TRUE, warning = F, message = F) 2.5.1 Kernel density estimation data(faithful) hist(faithful$waiting, density = 10, main=&quot;wainting time to next eruption&quot;, xlab=&quot;minutes&quot;, freq=F) lines(density(faithful$waiting),lwd=2,col=4) lines(density(faithful$waiting, bw = 5),lwd=2,col=5) # defining a banwith = 5 in density function lines(density(faithful$waiting, bw = 10),lwd=2,col=6) # defining a banwith = 10 in density function 2.5.2 Conditional density With data from Coimbra Breast Cancer (Glucose | Classification) breast&lt;-read.csv(&quot;2.UploadedData/dataR2.csv&quot;) hist(breast$Glucose, density = 10, main=&quot;Glucose level&quot;, xlab=&quot;&quot;, freq=F, ylim = c(0,0.05)) lines(density(breast$Glucose), lwd=2,col=&quot;black&quot;) lines(density(subset(breast,Classification == 1)$Glucose), lwd=2,col=&quot;green&quot;) lines(density(subset(breast,Classification == 2)$Glucose), lwd=2,col=&quot;red&quot;) legend(130,0.03, legend=c(&quot;P(Glucose)&quot;,&quot;P(Glucose|Control)&quot;,&quot;P(Glucose|Patient)&quot;), col=c(&quot;black&quot;,&quot;green&quot;,&quot;red&quot;), lty = 1, cex = 0.8 ) 2.5.3 Expectation Maximization for the two latent variables Example with Age distribution in the breast cancer data set # read data breast&lt;-read.csv(&quot;2.UploadedData/dataR2.csv&quot;) # Expectation Maximization for the two latent variables (clusters) m&lt;-2 # the total number of clusters (m) set.seed(0) param.mean&lt;-sample(breast$Age,m) # initial parameters param.sd&lt;-rep(sd(breast$Age)/m,m) # initial parameters # E-step (Expectation): that estimates the probability of each point belonging to each cluster xx&lt;-breast$Age ## definition of M-step method alternative &lt;- 1 ## iteraction process itera &lt;-10 # maximum number of iterations eps&lt;-0.01 # minimum error to convergence for (k in 1:itera) { densities&lt;-sapply(1:m, function(i){dnorm(xx, mean=param.mean[i], sd=param.sd[i])}) relevance&lt;-t(apply(densities,1,function(l){l/sum(l)})) attrib&lt;-t(apply(relevance,1, function(l){replace(rep(0,m),which.max(l)[1],1)})) # Re-estimates the parameters of the probability distribution of each class if (alternative == 1) { ## M-step (alternative 1: crisp) param.mean.new&lt;-sapply(1:m, function(j){mean(breast$Age[attrib[,j]==1])}) param.sd.new&lt;-sapply(1:m, function(j){sqrt(sum(relevance[,j]*(breast$Age-param.mean[j])^2)/sum(relevance[,j]))}) } else { ## M-step (alternative 2: soft) param.mean.new&lt;-sapply(1:m, function(j){sum(breast$Age*relevance[,j])/sum(relevance[,j])}) param.sd.new&lt;-sapply(1:m, function(j){sqrt(sum(relevance[,j]*(breast$Age-param.mean[j])^2)/sum(relevance[,j]))}) } # convergence the maximum number of iterations (k). cat(&quot;iteration&quot;, itera, &quot;evolution:&quot;, dif&lt;-sum(abs(param.mean.new - param.mean)), &quot;\\n&quot;) if(dif &lt; eps) break # the accepted error to converge (e) param.mean&lt;-param.mean.new param.sd&lt;-param.sd.new } ## iteration 10 evolution: 13.44278 ## iteration 10 evolution: 9.497791 ## iteration 10 evolution: 4.21416 ## iteration 10 evolution: 0.4824254 ## iteration 10 evolution: 0.4839737 ## iteration 10 evolution: 0 Print the two new density lines with a large banwith # plot histogram and density function for Age hist(breast$Age, density = 20, main=&quot;Age distribution&quot;, xlab=&quot;&quot;, freq=F, ylim = c(0,0.03)) lines(density(breast$Age), lwd=2,col=&quot;black&quot;) set.seed(0) lines(density(rnorm(100, param.mean[1], param.sd[1]), bw = 12),lwd=2,col=5) lines(density(rnorm(100, param.mean[2], param.sd[2]), bw = 12),lwd=2,col=4) 2.5.4 Data Generation Comparing the distribution from data generated from 3 different ways ## length of data set generation n&lt;-100 # original sample data set sample&lt;-breast$Age # bootstrapped sample data set sample.obs&lt;-sample(breast$Age, size=n, replace = TRUE) # gaussian generated data set sample.simple&lt;-rnorm(n,mean=mean(breast$Age), sd=sd(breast$Age)) # EM model generated data set classes&lt;-apply(attrib, 1, which.max) # using attrib define previously, to calculate the number of observations in each class class.prop&lt;-prop.table(table(classes)) # calculate percentage of each class set.seed(523) sample.classes&lt;-sample(1:m, size=n, replace=TRUE, prob=class.prop) sample.prop&lt;-prop.table(table(sample.classes)) sample.values&lt;-round(unlist(sapply(1:m, function(j){ rnorm(n*sample.prop[j], # generating number with the proportions of the two classes we identified previously mean=param.mean[j], # parameters defined previously sd=param.sd[j]) # parameters defined previously })),0) Printing the distribution of the 4 data samples par(mfrow=c(2,2)) hist(breast$Age, freq = F, main = &quot;Observed data&quot;) lines(density(breast$Age)) hist(sample.obs, freq = F, main = &quot;data points sampled from observed data&quot;) lines(density(sample.obs)) hist(sample.simple, freq = F, main = &quot;data points generated from sample distribution&quot;) lines(density(sample.simple)) hist(sample.values, freq = F, main = &quot;data points generated from EM (m=2)&quot;) lines(density(sample.values)) "],
["assignment-glucose.html", "2.6 assignment Glucose", " 2.6 assignment Glucose Assignment from Lesson 3 2020-02-26 Using only the base and stats packages in R, define the iterative steps of ExpectationMaximization applied to the Glucose variable of the Breast Cancer Coimbra data set, searching for 2 or more latent classes. 2.6.1 Defining a function for Expectation-Maximization algorithm # function for E-M algorithm emf&lt;-function(xx = vv, # a vector with the data m = 2, # number of clustres method = &quot;crisp&quot;, # method used for parameters estimation in each class; options: &quot;crisp&quot; or &quot;soft&quot; itera = 10, # number of iterations eps = 0.01, # the accepted error to converge (e) params = &quot;random&quot;, # initial parameters, if &quot;random&quot; &#39;m&#39; values from &#39;xx&#39; are randomly select, or we can input a vector with &#39;m&#39; values main=&quot;Glucose level&quot;, # histogram title seed = 0 # value for set.seed() ){ medias=NA desvpadrao=NA if (params == &quot;random&quot;) { set.seed(seed) param.mean&lt;-sample(xx,m) param.sd&lt;-rep(sd(xx)/m,m) } else { param.mean&lt;-params param.sd&lt;-rep(sd(xx)/m,m) } for (k in 1:itera){ densities&lt;-sapply(1:m, function(i){dnorm(xx, mean=param.mean[i], sd=param.sd[i])}) relevance&lt;-t(apply(densities,1,function(l){l/sum(l)})) attrib&lt;-t(apply(relevance,1, function(l){replace(rep(0,m),which.max(l)[1],1)})) # Re-estimates the parameters of the probability distribution of each class if (method == &quot;crisp&quot;) { ## M-step (alternative 1: crisp) param.mean.new&lt;-sapply(1:m, function(j){mean(xx[attrib[,j]==1])}) param.sd.new&lt;-sapply(1:m, function(j){sqrt(sum(relevance[,j]*(xx-param.mean[j])^2)/sum(relevance[,j]))}) } else { ## M-step (alternative 2: soft) param.mean.new&lt;-sapply(1:m, function(j){sum(xx*relevance[,j])/sum(relevance[,j])}) param.sd.new&lt;-sapply(1:m, function(j){sqrt(sum(relevance[,j]*(xx-param.mean[j])^2)/sum(relevance[,j]))}) } # convergence the maximum number of iterations (k). cat(&quot;iteration&quot;, k, &quot;evolution:&quot;, dif&lt;-sum(abs(param.mean.new - param.mean)), &quot;\\n&quot;) if(dif &lt; eps) break # the accepted error to converge (e) param.mean&lt;-param.mean.new param.sd&lt;-param.sd.new } ## ploting the histogram and density curve for the vector of data hist(xx, density = 20, main=main, xlab=&quot;&quot;, freq=F) lines(density(xx), lwd=2,col=&quot;black&quot;) ## save means, std deviations and classes&#39; attributes on a list result&lt;-list(medias = param.mean, desvpadrao = param.sd, classe = as.data.frame(attrib)) } 2.6.2 2 clusters density curves # reading the data breast&lt;-read.csv(&quot;2.UploadedData/dataR2.csv&quot;) par(mfrow=c(1,2)) # run the EM function for Glucose with 2 clusters and max 20 iterations res&lt;-emf(xx = breast$Glucose, itera = 20, m=2, method = &quot;crisp&quot;, main = &quot;Glucose values&quot;, seed = 0) ## iteration 1 evolution: 17.79331 ## iteration 2 evolution: 9.991458 ## iteration 3 evolution: 9.361819 ## iteration 4 evolution: 1.118374 ## iteration 5 evolution: 2.419946 ## iteration 6 evolution: 5.454559 ## iteration 7 evolution: 0 # add density curves for each class lines(density(rnorm(100, res$medias[1], res$desvpadrao[1]), bw = 12),lwd=2,col=3) lines(density(rnorm(100, res$medias[2], res$desvpadrao[2]), bw = 12),lwd=2,col=4) ## column with 2 classes attach(res$classe) classe2&lt;-ifelse(V1 == 1, 3 , 4) # create &#39;m&#39; classes detach() # Plot the values in which each point has a color corresponding to the class to which it belongs plot(1:length(breast$Glucose), breast$Glucose, col=classe2, pch=20, xlab=&quot;index&quot;, ylab=&quot;Glucose values&quot;, main = &quot; Glucose colored by class (m=2)&quot;) 2.6.3 3 clusters density curves par(mfrow=c(1,2)) # run the EM function for Glucose with 3 clusters and max 20 iterations res&lt;-emf(xx = breast$Glucose, itera = 20, m=3, method = &quot;crisp&quot;, main = &quot;Glucose values&quot;, seed = 0) ## iteration 1 evolution: 22.28802 ## iteration 2 evolution: 16.6689 ## iteration 3 evolution: 3.445327 ## iteration 4 evolution: 0.3056396 ## iteration 5 evolution: 0 # add density curves for each class lines(density(rnorm(100, res$medias[1], res$desvpadrao[1]), bw = 12),lwd=2,col=3) lines(density(rnorm(100, res$medias[2], res$desvpadrao[2]), bw = 12),lwd=2,col=4) lines(density(rnorm(100, res$medias[3], res$desvpadrao[3]), bw = 12),lwd=2,col=5) ## column with 3 classes attach(res$classe) classe3&lt;-ifelse(V1 == 1, 3, ifelse(V2 == 1, 4, 5)) # create &#39;m&#39; classes detach() plot(1:length(breast$Glucose), breast$Glucose, col=classe3, pch=20, xlab=&quot;index&quot;, ylab=&quot;Glucose values&quot;, main = &quot; Glucose colored by class (m=3)&quot;) "],
["assignment-second-lead-from-ecg.html", "2.7 assignment Second Lead from ECG", " 2.7 assignment Second Lead from ECG Assignment from Lesson 3 2020-02-29 Consider the file A0001.mat from the PhysioNet Challenge https://physionetchallenges.github.io/2020/. Using R: Plot the histogram of all the 12 ECG leads with the respective density curve. Using the second lead inverted, apply the expectation-maximization algorithm with 2 and 3 latent classes. Plot the densities for each of the latent classes. Which point in the ECG belongs to each latent class? Plot the ECG in which each point has a color corresponding to the class to which it belongs. Use the difference between the averages of class distribution as the convergence criterion. Generate 1000 data points at random according to a single distribution fitted to the original data. Generate 1000 data points at random according to a mixture of distributions fitted using one of the previous EM computed in line b. 2.7.1 Defining a function for expectation-maximization (EM) algorithm # function for Expectation-Maximization algorithm emf&lt;-function(xx = vv, # a vector with the data m = 2, # number of clusters method = &quot;crisp&quot;, # method used for parameters estimation in each class; options: &quot;crisp&quot; or &quot;soft&quot; itera = 10, # number of iterations eps = 0.01, # the accepted error to converge (e) params = &quot;random&quot;, # initial parameters, if &quot;random&quot; &#39;m&#39; values from &#39;xx&#39; are randomly select, or we can input a vector with &#39;m&#39; values main=&quot;ECG&quot;, # histogram title seed = 0 # value for set.seed() ){ medias=NA desvpadrao=NA if (params == &quot;random&quot;) { set.seed(seed) param.mean&lt;-sample(xx,m) param.sd&lt;-rep(sd(xx)/m,m) } else { param.mean&lt;-params param.sd&lt;-rep(sd(xx)/m,m) } for (k in 1:itera){ densities&lt;-sapply(1:m, function(i){dnorm(xx, mean=param.mean[i], sd=param.sd[i])}) relevance&lt;-t(apply(densities,1,function(l){l/sum(l)})) attrib&lt;-t(apply(relevance,1, function(l){replace(rep(0,m),which.max(l)[1],1)})) # Re-estimates the parameters of the probability distribution of each class if (method == &quot;crisp&quot;) { ## M-step (alternative 1: crisp) param.mean.new&lt;-sapply(1:m, function(j){mean(xx[attrib[,j]==1])}) param.sd.new&lt;-sapply(1:m, function(j){sqrt(sum(relevance[,j]*(xx-param.mean[j])^2)/sum(relevance[,j]))}) } else { ## M-step (alternative 2: soft) param.mean.new&lt;-sapply(1:m, function(j){sum(xx*relevance[,j])/sum(relevance[,j])}) param.sd.new&lt;-sapply(1:m, function(j){sqrt(sum(relevance[,j]*(xx-param.mean[j])^2)/sum(relevance[,j]))}) } # convergence the maximum number of iterations (k). cat(&quot;iteration&quot;, k, &quot;evolution:&quot;, dif&lt;-sum(abs(param.mean.new - param.mean)), &quot;\\n&quot;) # using the difference between the averages of class distribution as the convergence criterion. if(dif &lt; eps) break # the accepted error to converge (e) param.mean&lt;-param.mean.new param.sd&lt;-param.sd.new } ## ploting the histogram and density curve for the vector of data hist(xx, density = 20, main=main, xlab=&quot;&quot;, freq=F, ylim = c(0,0.005)) lines(density(xx), lwd=2,col=&quot;black&quot;) # plot(xx, col=apply(-densities,1,which.max), # xlab = &quot;Index&quot;, # ylab = &quot;ECG Values&quot;, main = &quot;Colored by class&quot;) #plot colours according to vector densities ## save means, std deviations and classes&#39; attributes on a list result&lt;-list(medias = param.mean, desvpadrao = param.sd, classe = as.data.frame(attrib)) } 2.7.2 reading the data and ploting the leads distribution library(R.matlab) # after unzip the files, I took the first file fA1&lt;-readMat(&quot;2.UploadedData/A0001.mat&quot;)$val fA1&lt;-t(fA1) par(mfrow=c(3,4)) # Ploting the histogram of all the 12 ECG leads with the respective density curve. for(i in 1:12){ hist(fA1[,i], density = 10, main=paste(&quot;ECG lead&quot;,i, sep = &quot; &quot;), freq=F, ylim=c(0,max(density(fA1[,i]*(-1))[[&quot;y&quot;]])), xlab=&quot;ECG values&quot;) lines(density(fA1[,i]), lwd=2,col=&quot;black&quot;) } 2.7.3 EM algorithm for 2 latent classes par(mfrow=c(1,1)) # expectation-maximization algorithm with 2 latent classes and a maximum 50 iterations. res&lt;-emf(xx = fA1[,2]*(-1), itera = 50, m=2, method = &quot;crisp&quot;, main = &quot;second lead inverted&quot;, seed = 10) ## iteration 1 evolution: 63.64733 ## iteration 2 evolution: 46.32738 ## iteration 3 evolution: 52.78091 ## iteration 4 evolution: 23.65685 ## iteration 5 evolution: 13.68977 ## iteration 6 evolution: 10.54316 ## iteration 7 evolution: 9.96314 ## iteration 8 evolution: 7.105816 ## iteration 9 evolution: 5.806118 ## iteration 10 evolution: 4.504107 ## iteration 11 evolution: 4.9698 ## iteration 12 evolution: 5.118377 ## iteration 13 evolution: 5.773136 ## iteration 14 evolution: 6.764423 ## iteration 15 evolution: 5.433114 ## iteration 16 evolution: 7.445487 ## iteration 17 evolution: 8.256206 ## iteration 18 evolution: 7.612994 ## iteration 19 evolution: 12.67447 ## iteration 20 evolution: 9.597814 ## iteration 21 evolution: 5.658197 ## iteration 22 evolution: 2.98443 ## iteration 23 evolution: 2.079856 ## iteration 24 evolution: 2.490897 ## iteration 25 evolution: 0 # adding density curves from the 2 clusters lines(density(rnorm(1000, res$medias[1], res$desvpadrao[1]), bw = 40),lwd=2,col=3) lines(density(rnorm(1000, res$medias[2], res$desvpadrao[2]), bw = 40),lwd=2,col=4) ## column with 2 classes to classify data from each cluster attach(res$classe) classe2&lt;-ifelse(V1 == 1, 3 ,4) # create &#39;m&#39; classes detach() # ploting ECG inverted points according with the latent class it belongs to plot(1:length(fA1[,2]), fA1[,2]*(-1), col=classe2, pch=20, xlab=&quot;index&quot;, ylab=&quot;2nd lead ECG&quot;, main = &quot; ECG colored by class (m=2)&quot;) 2.7.4 EM algorithm for 3 latent classes par(mfrow=c(1,1)) # expectation-maximization algorithm with 3 latent classes and a maximum 50 iterations. res&lt;-emf(xx = fA1[,2]*(-1), itera = 50, m=3, method = &quot;crisp&quot;, main = &quot;second lead inverted&quot;, seed = 10) ## iteration 1 evolution: 175.0321 ## iteration 2 evolution: 84.11333 ## iteration 3 evolution: 41.73523 ## iteration 4 evolution: 43.49741 ## iteration 5 evolution: 28.14147 ## iteration 6 evolution: 11.77696 ## iteration 7 evolution: 9.002154 ## iteration 8 evolution: 6.878037 ## iteration 9 evolution: 3.751731 ## iteration 10 evolution: 4.932522 ## iteration 11 evolution: 0.6580674 ## iteration 12 evolution: 0 # adding density curves from the 3 clusters lines(density(rnorm(1000, res$medias[1], res$desvpadrao[1]), bw = 40),lwd=2,col=3) lines(density(rnorm(1000, res$medias[2], res$desvpadrao[2]), bw = 40),lwd=2,col=4) lines(density(rnorm(1000, res$medias[3], res$desvpadrao[3]), bw = 40),lwd=2,col=5) ## column with 3 classes to classify data from each cluster attach(res$classe) classe3&lt;-ifelse(V1 == 1, 3 , ifelse(V2 == 1, 4, 5)) # create &#39;m&#39; classes detach() # ploting ECG inverted points according with the latent class it belongs to plot(1:length(fA1[,2]), fA1[,2]*(-1), col=classe3, pch=20, xlab=&quot;index&quot;, ylab=&quot;2nd lead ECG&quot;, main = &quot; ECG colored by class (m=3)&quot;) 2.7.5 Generating 1000 data points at random with bootstrapped, with guassian distribution and EM with 3 clusters. ## length of data set generation n&lt;-1000 # original sample data set sample&lt;-fA1[,2]*(-1) # bootstrapped sample data set sample.obs&lt;-sample(sample, size=n, replace = TRUE) # gaussian generated data set sample.simple&lt;-rnorm(n,mean=mean(sample), sd=sd(sample)) # EM model generated data set #classes&lt;-apply(attrib, 1, which.max) # using attrib define previously, to calculate the number of observations in each class class.prop&lt;-prop.table(table(classe3)) # calculate percentage of each class m&lt;-3 set.seed(523) sample.classes&lt;-sample(1:m, size=n, replace=TRUE, prob=class.prop) sample.prop&lt;-prop.table(table(sample.classes)) sample.values&lt;-round(unlist(sapply(1:m, function(j){ rnorm(n*sample.prop[j], # generating number with the proportions of the two classes we identified previously mean=res$medias[j], # parameters defined previously sd=res$desvpadrao[j]) # parameters defined previously })),0) # Plotting results par(mfrow=c(2,2)) hist(sample, freq = F, main = &quot;Observed data&quot;, ylim = c(0,0.004)) lines(density(sample)) hist(sample.obs, freq = F, main = &quot;data points sampled from observed data&quot;, ylim = c(0,0.004)) lines(density(sample.obs)) hist(sample.simple, freq = F, main = &quot;data points generated from sample distribution&quot;, ylim = c(0,0.004)) lines(density(sample.simple)) hist(sample.values, freq = F, main = &quot;data points generated from EM (m=3)&quot;, ylim = c(0,0.004)) lines(density(sample.values)) 2.7.6 Bonus for a diferent solution: https://rpubs.com/franzbischoff/assignment2 "],
["random-numbers.html", "2.8 Random Numbers", " 2.8 Random Numbers 2020-03-02 Classe Exercises (code given by the professor) 2.8.1 Random Number Generation Linear Congruential Generator One of the most common pseudo-random number generator is the linear congruential generator, which uses the recurrence: \\(u_{k}\\) = (\\(a.u_{k-1}\\) + \\(b\\)) mod \\(m\\) # linear congruential generator .my.seed &lt;- as.numeric(Sys.time()) my.set.seed &lt;- function(seed=as.numeric(Sys.time())) { assign(&quot;.my.seed&quot;, .GlobalEnv, value=seed) } my.rlcg&lt;- function(n, min=0, max=1, a=2^23-1, b=2^53-1, m=2^43-1){ seq &lt;- NULL for (i in 1:n){ tmp &lt;- (a * get(&quot;.my.seed&quot;, env=.GlobalEnv) + b) %% m seq &lt;- c(seq, (tmp / m) * (max - min) + min) assign(&quot;.my.seed&quot;, env=.GlobalEnv, value=tmp) } seq } \\(m\\) = \\(2^{10}-1\\) par(mfrow=c(2,1)) m &lt;- 2^10 - 1 my.set.seed(523) gen &lt;- my.rlcg(10000, m=m) hist(gen, main=paste(&quot;10K pseudo-random points (a=2^23-1, b=2^53-1, m=2^10-1)&quot;), col=&quot;lightgrey&quot;,xlab=&quot;Value&quot;) plot(gen, c(0,diff(gen)), main=&quot;Lagged Differences in Generated Sequence&quot;, xlab=&quot;Generated Value&quot;, ylab=&quot;Lagged Difference&quot;, pch=&quot;.&quot;, cex=.8) \\(m\\) = \\(2^{43}-1\\) par(mfrow=c(2,1)) m &lt;- 2^43 - 1 my.set.seed(523) gen &lt;- my.rlcg(10000, m=m) hist(gen, main=paste(&quot;10K pseudo-random points (a=2^23-1, b=2^53-1, m=2^43-1)&quot;), col=&quot;lightgrey&quot;,xlab=&quot;Value&quot;) plot(gen, c(0,diff(gen)), main=&quot;Lagged Differences in Generated Sequence&quot;, xlab=&quot;Generated Value&quot;, ylab=&quot;Lagged Difference&quot;, pch=&quot;.&quot;, cex=.8) \\(m\\) = \\(2^{521}-1\\) par(mfrow=c(2,1)) m &lt;- 2^521 - 1 my.set.seed(523) gen &lt;- my.rlcg(10000, m=m) hist(gen, main=paste(&quot;10K pseudo-random points (a=2^23-1, b=2^53-1, m=2^521-1)&quot;), col=&quot;lightgrey&quot;,xlab=&quot;Value&quot;) plot(gen, c(0,diff(gen)), main=&quot;Lagged Differences in Generated Sequence&quot;, xlab=&quot;Generated Value&quot;, ylab=&quot;Lagged Difference&quot;, pch=&quot;.&quot;, cex=.8) \\(m\\) = \\(2^{937a}-1\\) par(mfrow=c(2,1)) m &lt;- 2^937 - 1 my.set.seed(523) gen &lt;- my.rlcg(10000, m=m) hist(gen, main=paste(&quot;10K pseudo-random points (a=2^23-1, b=2^53-1, m=2^937-1)&quot;), col=&quot;lightgrey&quot;,xlab=&quot;Value&quot;) plot(gen, c(0,diff(gen)), main=&quot;Lagged Differences in Generated Sequence&quot;, xlab=&quot;Generated Value&quot;, ylab=&quot;Lagged Difference&quot;, pch=&quot;.&quot;, cex=.8) \\(U[0, 1]\\) par(mfrow=c(2,1)) set.seed(523) gen &lt;- runif(10000, min=0, max=1) hist(gen, main=paste(&quot;10K pseudo-random points (using runif)&quot;), col=&quot;lightgrey&quot;,xlab=&quot;Value&quot;) plot(gen, c(0,diff(gen)), main=&quot;Lagged Differences in Generated Sequence&quot;, xlab=&quot;Generated Value&quot;, ylab=&quot;Lagged Difference&quot;, pch=&quot;.&quot;, cex=.8) 2.8.2 My simple solution for a random number generator: # Linear Congruential Generator lcg&lt;-function(a=7^11, b=3^11, m=5^23, n=100,u1 = 0){ u = NULL u[1]&lt;-u1 for (i in 2:n){ u[i] = (a*u[i-1] + b) %% m } return(u) } 2.8.3 Transition probabilities Rate = \\(\\frac{-ln(1-p)}{t}\\) Probability = \\(1 - e^{-rt}\\) Exercise Portuguese population is 11M people. 12% of the population is considered at risk for a certain disease, from which: • 60% are male, and • 45% are more than 65 years old. Risk factors are considered independent. For this sub-population, 20-year risk estimates indicate: a risk of disease of 10% • 15% in men | 3% in women • 5% if \\(&lt;\\) 65yo | 16.5% if \\(\\ge\\) 65yo a any-cause mortality of 10% • 0.5% if \\(&lt;\\) 65yo | 21.5% if \\(\\ge\\) 65yo Simulate the population at risk of disease. The resulting data frame should contain 4 columns: • Gender • Age group • Yearly risk of death - i.e. transition probabilty from alive to dead. • Yearly risk of disease - i.e. transition probability from healthy to ill. N &lt;- 11000000 # população n &lt;- 0.12*N # população em risco sex&lt;-sample(c(&quot;m&quot;,&quot;f&quot;), replace = TRUE, n, prob = c(0.6,0.4)) # sexo da população em risco age&lt;-sample(c(&quot;-65&quot;,&quot;+65&quot;), replace = TRUE, n, prob = c(0.55,0.45)) # grupo etátio da população em risco # criar data frame com colunas &#39;sex&#39; e &#39;age&#39; data&lt;-data.frame(sex,age) # definição dos riscos estimados a 20 anos p.m.65 &lt;-0.215 # probabilidade de morte a 20 anos de &lt;65yo p.m.64 &lt;-0.005 # probabilidade de morte a 20 anos de &gt;=65yo p.d.m &lt;-0.15 # probabilidade de doença a 20 anos do sexo masculino p.d.f &lt;-0.03 # probabilidade de doença a 20 anos do sexo feminino p.d.65 &lt;-0.165 # probabilidade de doença a 20 anos de &gt;=65yo p.d.64 &lt;-0.05 # probabilidade de doença a 20 anos de &lt;65yo # Rate = (-ln(1-p)) / t # Probability = 1 - exp(-rt) library(tidyverse) # acrescentar ao data frame as colunas com as probabilidades anuais de morte e de doença data&lt;-data %&gt;% mutate(taxa.mort.20y = case_when(age == &quot;+65&quot; ~ (-log(1-p.m.65))/20, TRUE ~ (-log(1-p.m.64))/20), # calculo da taxa de mortalidade a 20 anos prob.mort.1y = 1-exp(-taxa.mort.20y * 1), # calculo da probabilidade anual de morte taxa.dx.20y = case_when(sex == &quot;m&quot; &amp; age == &quot;+65&quot; ~ (-log(1-p.d.m*p.d.65))/20, sex == &quot;m&quot; &amp; age == &quot;-65&quot; ~ (-log(1-p.d.m*p.d.64))/20, sex == &quot;f&quot; &amp; age == &quot;+65&quot; ~ (-log(1-p.d.f*p.d.65))/20, sex == &quot;f&quot; &amp; age == &quot;-65&quot; ~ (-log(1-p.d.f*p.d.64))/20 ), # cálculo da taxa de doença a 20 anos prob.dx.1y = 1-exp(-taxa.dx.20y * 1) # calculo da probabilidade anual de doença ) %&gt;% select(-taxa.mort.20y, -taxa.dx.20y) require(printr) # ver as primeiras linhas dos dados head(data) sex age prob.mort.1y prob.dx.1y f +65 0.0120306 0.0002481 f -65 0.0002506 0.0000751 m +65 0.0120306 0.0012523 m +65 0.0120306 0.0012523 f +65 0.0120306 0.0002481 m -65 0.0002506 0.0003763 How many of the patients developed the disease in the first year? doentes1&lt;-data %&gt;% count(sex,age,prob.dx.1y) %&gt;% mutate(nr.doentes = n*prob.dx.1y) doentes1 sex age prob.dx.1y n nr.doentes f -65 0.0000751 289923 21.75973 f +65 0.0002481 237865 59.01046 m -65 0.0003763 435537 163.91108 m +65 0.0012523 356675 446.65938 options(scipen=999) total de doentes no primeiro ano: 691 How many of the patients died in the first year? mortos1&lt;-data %&gt;% count(age,prob.mort.1y) %&gt;% mutate(nr.mortos = n*prob.mort.1y) mortos1 age prob.mort.1y n nr.mortos -65 0.0002506 725460 181.7971 +65 0.0120306 594540 7152.6874 total de mortos no primeiro ano: 7334 "],
["estimate-pi.html", "2.9 Estimate \\(\\pi\\)", " 2.9 Estimate \\(\\pi\\) 2020-03-16 Estimate the value of \\(\\pi\\) through simulation (code given by the professor) https://rpubs.com/balima78/compstat_2050_lesson5 "],
["sir-model.html", "2.10 SIR model", " 2.10 SIR model 2020-03-30 The SIR MODEL (code given by the professor) load the packages library(deSolve) library(lubridate) library(Hmisc) library(FME) library(printr) read the data and set population for Portugal N = 10260906 data = read.csv(&quot;2.UploadedData/CasesP.csv&quot;) data = data.frame(time=0:(dim(data)[1]-1),I = data[,1],R=data[,2],S=N-data[,1]-data[,2]) str(data) ## &#39;data.frame&#39;: 25 obs. of 4 variables: ## $ time: int 0 1 2 3 4 5 6 7 8 9 ... ## $ I : int 2 4 6 9 13 21 30 39 41 59 ... ## $ R : int 0 0 0 0 0 0 0 0 0 0 ... ## $ S : num 10260904 10260902 10260900 10260897 10260893 ... using 1st day with a recover as initial values iR = min(which(data[,3]&gt;10)) data=data[-(1:iR),] data[,1]=data[,1]-data[1,1] S0 = data$S[1] I0 = data$I[1] R0 = data$R[1] S1 = data$S[2] I1 = data$I[2] R1 = data$R[2] S0_ = S1-S0 beta0 = -(S0_*N)/(I0*S0) I0_ = I1-I0 R0_ = R1-R0 gama0 = R0_/I0 the SIR model SIR &lt;- function(time, state, pars) { # returns rate of change with(as.list(c(state, pars)), { dS &lt;- -beta*S*I/N dI &lt;- beta *S*I/N - gama*I dR &lt;- gama*I return(list(c(dS, dI,dR))) }) } Initial Parameters state &lt;- c(S = S0, I = I0,R = R0) pars &lt;- list(beta = beta0, gama = gama0) time &lt;- seq(0,200,by=1) differential equations out &lt;- ode(y = state, times = time, func = SIR, parms = pars) plot the model matplot(out[,1], out[,-1], type = &quot;l&quot;, lty = 1:3, lwd = 2, col = 1:3, xlab = &quot;time, days&quot;, ylab = &quot;&quot;) legend(&quot;topright&quot;, c(&quot;S&quot;, &quot;I&quot;, &quot;R&quot;), lty = 1:3, lwd = 2,col=1:4) minor.tick(2,2,0.7) minor.tick(10,10,0.5) Fitting the SIR model to data modCost - Calculates the discrepancy Of a model solution with observations tout &lt;- seq(0,200,by=1) ## output times Objective &lt;- function(x, parset = names(x)) { pars[parset] &lt;- x tout &lt;- seq(0,200,by=1) ## output times out &lt;- ode(y = state, times = tout, func = SIR, parms = pars) ## Model cost return(modCost(obs = data, model = out)) } modFit - Constrained fitting Of a model to data Fit &lt;- modFit(p = c(beta = beta0, gama = gama0),f = Objective) fitted parameters and predicted data Fit$par ## beta gama ## 0.214469189 0.007391927 out2 &lt;- ode(y = state, times = tout, func = SIR, parms = Fit$par) plot fitted model matplot(out2[,1], out2[,-1], type = &quot;l&quot;, lty = 1:3, lwd = 2, col = 1:3, xlab = &quot;time, days&quot;, ylab = &quot;&quot;) legend(&quot;topright&quot;, c(&quot;S&quot;, &quot;I&quot;, &quot;R&quot;), lty = 1:3, lwd = 2,col=1:3) minor.tick(10,10,0.5) minor.tick(2,2,0.7) points(data$I,pch=16,col=&quot;red&quot;) points(data$R,pch=16,col=&quot;green&quot;) points(data$S,pch=16,col=&quot;black&quot;) zoom in real data matplot(out2[,1], out2[,-1], type = &quot;l&quot;, lty = 1:3, lwd = 2, col = 1:3, xlab = &quot;time, days&quot;, ylab = &quot;&quot;,ylim=c(0,10000),xlim=c(0,50)) legend(&quot;topright&quot;, c(&quot;S&quot;, &quot;I&quot;, &quot;R&quot;), lty = 1:3, lwd = 2,col=1:3) minor.tick(10,10,0.5) minor.tick(2,2,0.7) points(data$I,pch=16,col=&quot;red&quot;) points(data$R,pch=16,col=&quot;green&quot;) points(data$S,pch=16,col=&quot;black&quot;) 2.10.1 Markov chain Monte Carlo resume fitted model SF&lt;-summary(Fit) SF ## ## Parameters: ## Estimate Std. Error t value Pr(&gt;|t|) ## beta 0.214469 0.002528 84.830 &lt;0.0000000000000002 *** ## gama 0.007392 0.002891 2.557 0.0211 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 53.38 on 16 degrees of freedom ## ## Parameter correlation: ## beta gama ## beta 1.0000 0.7647 ## gama 0.7647 1.0000 see all the information on Fit SF[] ## $residuals ## I I I I I I ## 0.000000 -25.547946 -123.374023 20.091494 -65.009195 59.864614 ## R R R R R R ## 0.000000 8.512436 3.443102 1.348192 10.911638 -3.025298 ## S S S S S S ## 0.000000 17.035510 119.930921 -21.439687 54.097556 -56.839315 ## ## $residualVariance ## [1] 2849.524 ## ## $sigma ## [1] 53.38093 ## ## $modVariance ## [1] 2532.91 ## ## $df ## [1] 2 16 ## ## $cov.unscaled ## beta gama ## beta 0.000000002243130 0.000000001961327 ## gama 0.000000001961327 0.000000002932503 ## ## $cov.scaled ## beta gama ## beta 0.000006391851 0.000005588849 ## gama 0.000005588849 0.000008356237 ## ## $info ## [1] 1 ## ## $niter ## [1] 4 ## ## $stopmess ## [1] &quot;ok&quot; ## ## $par ## Estimate Std. Error t value Pr(&gt;|t|) ## beta 0.214469189 0.002528211 84.830412 0.0000000000000000000001153309 ## gama 0.007391927 0.002890716 2.557127 0.0210969571663339600098296955 save initials values and run MCMC Var0 &lt;- SF$modVariance covIni &lt;- SF$cov.scaled *2.4^2/2 MCMC &lt;- modMCMC(p = coef(Fit), f = Objective, jump = covIni,var0 = Var0, wvar0 = 1) ## number of accepted runs: 408 out of 1000 (40.8%) plot iterations’ results plot(MCMC, Full = TRUE) and correlation between parameters pairs(MCMC) plot of sample quantiles’ evolution as a function of the number of iterations MC &lt;- as.mcmc(MCMC$pars) cumuplot(MC) compare the covariances based on generated parameters with the ones from the fit: cov(MCMC$pars) beta gama beta 0.0000093 0.0000080 gama 0.0000080 0.0000115 covIni beta gama beta 0.0000184 0.0000161 gama 0.0000161 0.0000241 "],
["assignment-sir-model.html", "2.11 Assignment SIR model", " 2.11 Assignment SIR model 2020-03-27 Consider the SIR model discussed in class. Using and adapting the code provided, deliver a report on different scenarios simulated using parameters that could be estimated from : Portuguese data (all) Other countries’ data (all) Selected periods of data (e.g. using landmark windows or sliding windows in data used to fit beta and gamma) A possible solution to this exercise is presented as a shiny app: https://balima.shinyapps.io/sir_shiny/ All the code is available from here: https://github.com/balima78/covid19data/tree/master/sir_shiny "],
["stats.html", "3 STATS", " 3 STATS Modelação Estatística Conteúdos programáticos Regressão logística. Análise de sobrevivência. Análise de dados longitudinais. Causalidade. "],
["linear-and-logistic-regressions.html", "3.1 Linear and Logistic Regressions", " 3.1 Linear and Logistic Regressions 2020-02-03 exercise 1 The BMD_weight.sav database contains 190 densitometry records. library(foreign) data1 &lt;- read.spss(file=&quot;2.UploadedData/BMD_weight_new.sav&quot;, use.missing=TRUE) Calculate the body mass index. data1$IMC &lt;- ((data1$peso)/((data1$altura)/100)^2) Calculate the mean and standard deviation of total bone mineral density mean(data1$IMC) ## [1] 26.42143 sd(data1$IMC) ## [1] 4.299248 Analyze the relationship between bone mineral density and BMI using linear regression. plot(x = data1$IMC, y=data1$bmdtot) cor.test(data1$IMC, data1$bmdtot, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: data1$IMC and data1$bmdtot ## t = 4.4216, df = 186, p-value = 0.00001664 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1729248 0.4324277 ## sample estimates: ## cor ## 0.3084029 regressao &lt;- lm(formula = bmdtot ~ IMC, data=data1) summary(regressao) ## ## Call: ## lm(formula = bmdtot ~ IMC, data = data1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.28807 -0.07629 -0.00038 0.08273 0.37375 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.593934 0.052316 11.353 &lt; 0.0000000000000002 *** ## IMC 0.008639 0.001954 4.422 0.0000166 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1155 on 186 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.09511, Adjusted R-squared: 0.09025 ## F-statistic: 19.55 on 1 and 186 DF, p-value: 0.00001664 Compare the BMDTOT average according to the type of medication, adjusting for BMI and age. regressaoM &lt;- lm(formula = bmdtot ~ Medication + IMC + idade, data=data1) Check the assumptions for the model used in d). plot(regressaoM$fitted.values, regressaoM$residuals) hist(regressaoM$residuals) or we can plot everything plot(regressaoM) exercise 2 The data in the score2013.sav file refer to 1768 admissions to pediatric intensive care units in several Portuguese hospital units. data2 &lt;- read.spss(file=&quot;2.UploadedData/score2013.sav&quot;, use.missing=TRUE) The minimum systolic tension of the first 12 hours of hospitalization and mechanical ventilation at some point in the first hour of hospitalization are potential predictors of mortality in the ICU. summary(data2) Length Class Mode sexo 1768 factor numeric peso 1768 factor numeric outcome 1768 factor numeric apneia 1768 factor numeric ventil 1768 factor numeric hmedica 1768 factor numeric CPAP 1768 factor numeric Pupil 1768 factor numeric TAS12 1768 factor numeric temp12 1768 factor numeric entub 1768 factor numeric Study the individual association of each of them with mortality first. data2$TAS12 &lt;- as.numeric(factor(data2$TAS12)) logistic1 &lt;- glm(outcome ~ TAS12, data = data2, family = &quot;binomial&quot;) logistic2 &lt;- glm(outcome ~ ventil, data = data2, family = &quot;binomial&quot;) summary(logistic1) ## ## Call: ## glm(formula = outcome ~ TAS12, family = &quot;binomial&quot;, data = data2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.1116 -0.3953 -0.2934 -0.2225 3.0054 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.054551 0.215283 -0.253 0.8 ## TAS12 -0.051158 0.004748 -10.776 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 948.95 on 1767 degrees of freedom ## Residual deviance: 825.01 on 1766 degrees of freedom ## AIC: 829.01 ## ## Number of Fisher Scoring iterations: 6 summary(logistic2) ## ## Call: ## glm(formula = outcome ~ ventil, family = &quot;binomial&quot;, data = data2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.4776 -0.4776 -0.3022 -0.3022 2.4936 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.0634 0.1617 -18.940 &lt; 0.0000000000000002 *** ## ventilSim 0.9500 0.1952 4.868 0.00000113 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 948.95 on 1767 degrees of freedom ## Residual deviance: 923.16 on 1766 degrees of freedom ## AIC: 927.16 ## ## Number of Fisher Scoring iterations: 5 exp(cbind(OR = coef(logistic1), confint(logistic1))) OR 2.5 % 97.5 % (Intercept) 0.9469106 0.6185266 1.4405861 TAS12 0.9501285 0.9411981 0.9589037 exp(cbind(OR = coef(logistic2), confint(logistic2))) OR 2.5 % 97.5 % (Intercept) 0.046729 0.0334873 0.0632424 ventilSim 2.585604 1.7772140 3.8273766 Then test the interaction of these two variables and interpret the results. logistic3 &lt;- glm(outcome ~ ventil + TAS12 + ventil * TAS12, data = data2, family = &quot;binomial&quot;) summary(logistic3) ## ## Call: ## glm(formula = outcome ~ ventil + TAS12 + ventil * TAS12, family = &quot;binomial&quot;, ## data = data2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.2779 -0.3739 -0.2875 -0.2301 3.0171 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.323763 0.464494 -2.850 0.004373 ** ## ventilSim 1.669344 0.529036 3.155 0.001603 ** ## TAS12 -0.031758 0.008677 -3.660 0.000252 *** ## ventilSim:TAS12 -0.024409 0.010569 -2.310 0.020914 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 948.95 on 1767 degrees of freedom ## Residual deviance: 811.29 on 1764 degrees of freedom ## AIC: 819.29 ## ## Number of Fisher Scoring iterations: 6 exp(cbind(OR = coef(logistic3), confint(logistic3))) OR 2.5 % 97.5 % (Intercept) 0.2661319 0.1024458 0.6380664 ventilSim 5.3086860 1.9349881 15.5012115 TAS12 0.9687412 0.9524027 0.9854715 ventilSim:TAS12 0.9758865 0.9557194 0.9962234 Using all the variables in the database, build a model for mortality and study the model’s discrimination and calibration. accuracy pred&lt;-predict(logistic3, type = &quot;response&quot;) pred01&lt;-ifelse(pred&lt;0.5, 0, 1) table(pred01,data2$outcome) pred01/ Vivo Falecido 0 1624 121 1 10 13 acc&lt;-(1624+13)/(1624+121+10+13) acc ## [1] 0.925905 AUC library(pROC) g &lt;- roc(outcome ~ pred, data = data2) plot(g) Goodness of fit test library(ResourceSelection) hoslem.test(data2$outcome, pred01) ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: data2$outcome, pred01 ## X-squared = 1768, df = 8, p-value &lt; 0.00000000000000022 "],
["variable-transformation.html", "3.2 Variable transformation", " 3.2 Variable transformation 2020-02-10 Length of stay is an important indicator used in health care assessment. The UCIP_PRISM.sav database contains the record of 812 pediatric admissions in 3 intensive care units (Variable P1: Coimbra, Lisboa and Porto). We want to compare the hospitalization time (Variable tempoint) of the three units. However, we must take into account that the units may receive patients with different levels of severity. The PRISM variable is an indicator of the patient’s severity at the date of admission. library(foreign) aula2 &lt;- read.spss(file=&quot;2.UploadedData/UCIP_PRISM HEADS.sav&quot;, use.missing=TRUE) summary(aula2) Length Class Mode P1 812 factor numeric PRISM 812 -none- numeric tempoint 812 -none- numeric Make a histogram of the length of stay hist(aula2$tempoint) 2.Make a linear regression model with the dependent variable length of stay (tempoint), and the intensive care unit (P1) and PRISM variables as independent variables. regressao &lt;- lm(formula = tempoint ~ P1 + PRISM, data=aula2) summary(regressao) ## ## Call: ## lm(formula = tempoint ~ P1 + PRISM, data = aula2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.987 -6.568 -3.944 -0.047 255.598 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.84611 1.36895 2.810 0.00508 ** ## P1Lisboa 4.27584 1.54619 2.765 0.00581 ** ## P1Porto 2.70428 1.54758 1.747 0.08094 . ## PRISM 0.18291 0.07643 2.393 0.01693 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.04 on 808 degrees of freedom ## Multiple R-squared: 0.01558, Adjusted R-squared: 0.01192 ## F-statistic: 4.261 on 3 and 808 DF, p-value: 0.005353 3.Check that the model assumptions in question b) are not satisfied. plot(regressao$fitted.values, regressao$residuals) hist(regressao$residuals) or you can plot everything plot(regressao) 4.Transform the length of stay variable into the Neperian logarithm of length of stay (tempoint -&gt; logtempoint). aula2$logtempoint &lt;- log(aula2$tempoint) 5.Analyze the differences in the logarithm of length of stay between the 3 units, adjusting to the severity of the patients, ie, remake the model in question b) but with the logarithm of length of stay as dependent variable. logregressao &lt;- lm(formula = logtempoint ~ P1 + PRISM, data=aula2) summary(logregressao) ## ## Call: ## lm(formula = logtempoint ~ P1 + PRISM, data = aula2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.4230 -0.9225 -0.1198 0.7848 4.2611 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.734535 0.085985 8.543 &lt; 0.0000000000000002 *** ## P1Lisboa 0.396173 0.097117 4.079 0.0000496450 *** ## P1Porto 0.319145 0.097205 3.283 0.00107 ** ## PRISM 0.026849 0.004801 5.593 0.0000000306 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.133 on 808 degrees of freedom ## Multiple R-squared: 0.05785, Adjusted R-squared: 0.05435 ## F-statistic: 16.54 on 3 and 808 DF, p-value: 0.0000000001954 Geometric mean ratio (percentage value) (exp(logregressao$coefficients)-1)*100 ## (Intercept) P1Lisboa P1Porto PRISM ## 108.451271 48.612650 37.595032 2.721276 6.Check that the model assumptions in question e) are satisfied. plot(logregressao$fitted.values, logregressao$residuals) hist(logregressao$residuals) or you can plot everything plot(logregressao) "],
["poisson-regression.html", "3.3 Poisson Regression", " 3.3 Poisson Regression 2020-03-02 3.3.1 GLM - Poisson regression library(foreign) # ler os dados datapoisson &lt;- read.spss(file=&quot;2.UploadedData/epilepsy.sav&quot;, use.missing=TRUE, to.data.frame = T) sumário dos dados summary(datapoisson) ID group age seizures Min. : 1.00 old drug:25 Min. :20.0 Min. :0.00 1st Qu.:13.25 new drug:25 1st Qu.:27.0 1st Qu.:1.00 Median :25.50 NA Median :31.0 Median :2.00 Mean :25.50 NA Mean :30.3 Mean :2.14 3rd Qu.:37.75 NA 3rd Qu.:33.0 3rd Qu.:3.00 Max. :50.00 NA Max. :40.0 Max. :7.00 modelo de regressão de poisson poisson &lt;- glm(datapoisson$seizures ~ datapoisson$age + datapoisson$group, data = datapoisson, family = &quot;poisson&quot;) summary(poisson) ## ## Call: ## glm(formula = datapoisson$seizures ~ datapoisson$age + datapoisson$group, ## family = &quot;poisson&quot;, data = datapoisson) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.3308 -0.4922 -0.4563 0.3505 2.2198 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.347514 0.678376 0.512 0.60846 ## datapoisson$age 0.003112 0.021099 0.148 0.88273 ## datapoisson$groupnew drug 0.561495 0.204292 2.748 0.00599 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 61.808 on 49 degrees of freedom ## Residual deviance: 53.827 on 47 degrees of freedom ## AIC: 177.48 ## ## Number of Fisher Scoring iterations: 5 Risco Relativo e intervalos de confiança library(printr) exp(cbind(RR = coef(poisson), confint(poisson))) RR 2.5 % 97.5 % (Intercept) 1.415545 0.3667683 5.258693 datapoisson$age 1.003117 0.9626480 1.045764 datapoisson$groupnew drug 1.753292 1.1809467 2.637163 "],
["kaplan-meier-curves.html", "3.4 Kaplan-Meier curves", " 3.4 Kaplan-Meier curves 2020-03-16 3.4.1 Example from classe (code from the professor) #Load the library required for a survival analysis library(splines) library(survival) #Read data data&lt;-read.csv(&quot;2.UploadedData/data2.csv&quot;,sep=&quot;;&quot;,dec=&quot;,&quot;) Variables included in the dataset: str(data) ## &#39;data.frame&#39;: 274 obs. of 11 variables: ## $ id : int 1 3 7 9 11 13 15 17 19 21 ... ## $ fu : num 7 22 13 42 5 13 70 51 25 5 ... ## $ event : int 1 0 0 1 0 1 0 1 0 0 ... ## $ event_HD : int 0 0 0 0 1 0 0 0 1 1 ... ## $ event_RT : int 0 1 1 0 0 0 1 0 0 0 ... ## $ event_CR : int 1 3 3 1 2 1 3 1 2 2 ... ## $ peritonitis: int 0 1 1 1 1 0 1 0 1 1 ... ## $ sex : int 1 1 0 0 0 0 1 0 1 1 ... ## $ age : int 60 41 38 46 51 36 43 71 31 50 ... ## $ diab : int 1 1 0 0 0 0 0 1 0 0 ... ## $ first : int 0 0 1 0 0 1 1 0 0 1 ... Kaplan-Meier survival curve KM_1&lt;-survfit(Surv(data$fu,data$event)~1) KM_1 ## Call: survfit(formula = Surv(data$fu, data$event) ~ 1) ## ## n events median 0.95LCL 0.95UCL ## 274 62 NA 51 NA summary(KM_1) ## Call: survfit(formula = Surv(data$fu, data$event) ~ 1) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 274 2 0.993 0.00514 0.983 1.000 ## 2 266 6 0.970 0.01034 0.950 0.991 ## 5 242 2 0.962 0.01171 0.940 0.986 ## 6 232 2 0.954 0.01299 0.929 0.980 ## 7 220 2 0.945 0.01425 0.918 0.974 ## 8 210 2 0.936 0.01547 0.906 0.967 ## 9 198 4 0.917 0.01782 0.883 0.953 ## 10 190 2 0.908 0.01889 0.871 0.946 ## 12 176 2 0.897 0.02004 0.859 0.938 ## 13 174 6 0.866 0.02299 0.823 0.913 ## 14 166 2 0.856 0.02386 0.811 0.904 ## 17 150 2 0.845 0.02487 0.797 0.895 ## 18 144 2 0.833 0.02587 0.784 0.885 ## 22 128 2 0.820 0.02706 0.769 0.875 ## 23 114 2 0.806 0.02843 0.752 0.863 ## 24 110 2 0.791 0.02974 0.735 0.851 ## 27 94 2 0.774 0.03140 0.715 0.838 ## 35 62 4 0.724 0.03802 0.653 0.803 ## 36 58 2 0.699 0.04061 0.624 0.783 ## 37 54 4 0.647 0.04510 0.565 0.742 ## 38 50 2 0.621 0.04687 0.536 0.720 ## 42 46 2 0.594 0.04857 0.506 0.698 ## 51 36 2 0.561 0.05118 0.470 0.671 ## 60 28 2 0.521 0.05482 0.424 0.641 Plot survival curve with confidence intervals plot(KM_1, mark.time = F) abline(h = 0.5) Kaplan-Meier survival according to diabetes KM.diab&lt;-survfit(Surv(data$fu,data$event) ~ data$diab) KM.diab ## Call: survfit(formula = Surv(data$fu, data$event) ~ data$diab) ## ## n events median 0.95LCL 0.95UCL ## data$diab=0 206 38 NA NA NA ## data$diab=1 68 24 37 36 NA summary(KM.diab) ## Call: survfit(formula = Surv(data$fu, data$event) ~ data$diab) ## ## data$diab=0 ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 206 2 0.990 0.00683 0.977 1.000 ## 2 202 4 0.971 0.01179 0.948 0.994 ## 6 178 2 0.960 0.01396 0.933 0.988 ## 8 160 2 0.948 0.01616 0.917 0.980 ## 9 148 2 0.935 0.01830 0.900 0.972 ## 10 146 2 0.922 0.02017 0.883 0.963 ## 12 132 2 0.908 0.02215 0.866 0.953 ## 13 130 6 0.866 0.02694 0.815 0.921 ## 17 110 2 0.851 0.02866 0.796 0.909 ## 23 88 2 0.831 0.03110 0.772 0.894 ## 27 72 2 0.808 0.03425 0.744 0.878 ## 35 48 4 0.741 0.04500 0.658 0.834 ## 37 42 2 0.705 0.04929 0.615 0.809 ## 38 40 2 0.670 0.05276 0.574 0.782 ## 42 36 2 0.633 0.05601 0.532 0.753 ## ## data$diab=1 ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 2 64 2 0.969 0.0217 0.9270 1.000 ## 5 56 2 0.934 0.0319 0.8737 0.999 ## 7 52 2 0.898 0.0395 0.8240 0.979 ## 9 50 2 0.862 0.0454 0.7778 0.956 ## 14 44 2 0.823 0.0511 0.7288 0.930 ## 18 38 2 0.780 0.0568 0.6760 0.900 ## 22 34 2 0.734 0.0621 0.6218 0.866 ## 24 24 2 0.673 0.0704 0.5481 0.826 ## 36 14 2 0.577 0.0872 0.4288 0.775 ## 37 12 2 0.481 0.0955 0.3255 0.709 ## 51 6 2 0.320 0.1123 0.1612 0.637 ## 60 4 2 0.160 0.0978 0.0484 0.530 Plot survival curve according to diabetes plot(KM.diab, col = c(&quot;red&quot;, &quot;blue&quot;), mark.time = F, ylim=c(0,1), xlab=&quot;time&quot;, ylab = &quot;S&quot;) legend(&quot;topright&quot;, title=&quot;Diabetes&quot;, legend=c(&quot;No&quot;,&quot;Yes&quot;), col=c(&quot;red&quot;,&quot;blue&quot;), lty=1:1, cex=0.8) Log-rank test H0: No difference between the groups. survdiff(Surv(data$fu, data$event) ~ data$diab, rho = 0) ## Call: ## survdiff(formula = Surv(data$fu, data$event) ~ data$diab, rho = 0) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## data$diab=0 206 38 47.5 1.89 8.27 ## data$diab=1 68 24 14.5 6.19 8.27 ## ## Chisq= 8.3 on 1 degrees of freedom, p= 0.004 "],
["assignment-cancer.html", "3.5 Assignment cancer", " 3.5 Assignment cancer 2020-03-20 The file cancer.csv contains data of 1207 women who were diagnosed with breast cancer [event: death -&gt; variable: status]. #Load the library required for a survival analysis library(splines) library(survival) #Read data data&lt;-read.csv(&quot;2.UploadedData/cancer.csv&quot;,sep=&quot;;&quot;,dec=&quot;,&quot;) Variables included in the dataset: str(data) ## &#39;data.frame&#39;: 1207 obs. of 11 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ age : int 60 79 82 66 52 58 50 83 46 54 ... ## $ pathsize: num 99 99 99 99 99 99 99 99 99 99 ... ## $ lnpos : int 0 0 0 0 0 0 0 0 17 6 ... ## $ histgrad: int 3 4 2 2 3 4 2 3 4 2 ... ## $ er : int 0 2 2 1 2 2 1 0 2 1 ... ## $ pr : int 0 2 2 1 2 2 0 0 2 1 ... ## $ status : int 0 0 0 0 0 0 0 0 0 0 ... ## $ pathscat: int 99 99 99 99 99 99 99 99 99 99 ... ## $ ln_yesno: int 0 0 0 0 0 0 0 0 1 1 ... ## $ time : num 9.47 8.6 19.33 16.33 8.5 ... Produce a Kaplan-Meier curve for the data presented in the file. Kaplan-Meier survival curve km&lt;-survfit(Surv(data$time,data$status)~1) km ## Call: survfit(formula = Surv(data$time, data$status) ~ 1) ## ## n events median 0.95LCL 0.95UCL ## 1207 72 NA NA NA Plot survival curve with confidence intervals plot(km, mark.time = F) abline(h = 0.5) Obtain a Kaplan-Meier curve, separating women according to the variable ln_yesno (Lymph nodes) and test whether the survival of the two groups is significantly different (Log-rank test). Kaplan-Meier survival according to Lymph nodes: km.ln&lt;-survfit(Surv(data$time,data$status) ~ data$ln_yesno) km.ln ## Call: survfit(formula = Surv(data$time, data$status) ~ data$ln_yesno) ## ## n events median 0.95LCL 0.95UCL ## data$ln_yesno=0 929 42 NA NA NA ## data$ln_yesno=1 278 30 NA NA NA Plot survival curve according to Lymph nodes: plot(km.ln, col = c(&quot;red&quot;, &quot;blue&quot;), mark.time = F, ylim=c(0,1), xlab=&quot;time&quot;, ylab = &quot;Survival&quot;) legend(&quot;topright&quot;, title=&quot;Lymph nodes&quot;, legend=c(&quot;No&quot;,&quot;Yes&quot;), col=c(&quot;red&quot;,&quot;blue&quot;), lty=1:1, cex=0.8) Log-rank test H0: No difference between the groups. survdiff(Surv(data$time, data$status) ~ data$ln_yesno, rho = 0) ## Call: ## survdiff(formula = Surv(data$time, data$status) ~ data$ln_yesno, ## rho = 0) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## data$ln_yesno=0 929 42 56.1 3.53 16 ## data$ln_yesno=1 278 30 15.9 12.45 16 ## ## Chisq= 16 on 1 degrees of freedom, p= 0.00006 "],
["cox-regression.html", "3.6 Cox Regression", " 3.6 Cox Regression 2020-03-23 3.6.1 Example from classe (code from the professor) #Load the library required for a survival analysis library(survival) #Read data data&lt;-read.csv(&quot;2.UploadedData/survival.csv&quot;,sep=&quot;;&quot;,dec=&quot;,&quot;) Variables included in the dataset: str(data) ## &#39;data.frame&#39;: 16 obs. of 5 variables: ## $ ID : num 1 2 3 4 5 6 7 8 9 10 ... ## $ Event: num 1 0 1 1 0 1 1 0 1 1 ... ## $ Time : num 9 12 14 14 16 18 24 30 3 7 ... ## $ Group: num 1 1 1 1 1 1 1 1 2 2 ... ## $ Age : num 65 61 57 55 50 52 51 50 70 64 ... Unadjusted Cox regression for Group cox1 &lt;- coxph(Surv(data$Time, data$Event) ~ as.factor(data$Group)) summary(cox1) ## Call: ## coxph(formula = Surv(data$Time, data$Event) ~ as.factor(data$Group)) ## ## n= 16, number of events= 12 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## as.factor(data$Group)2 0.9224 2.5154 0.6307 1.462 0.144 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## as.factor(data$Group)2 2.515 0.3976 0.7307 8.66 ## ## Concordance= 0.629 (se = 0.08 ) ## Likelihood ratio test= 2.25 on 1 df, p=0.1 ## Wald test = 2.14 on 1 df, p=0.1 ## Score (logrank) test = 2.29 on 1 df, p=0.1 Unadjusted Cox regression for Age cox2 &lt;- coxph(Surv(data$Time, data$Event) ~ data$Age) summary(cox2) ## Call: ## coxph(formula = Surv(data$Time, data$Event) ~ data$Age) ## ## n= 16, number of events= 12 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## data$Age 0.4229 1.5264 0.1388 3.047 0.00231 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## data$Age 1.526 0.6551 1.163 2.004 ## ## Concordance= 0.892 (se = 0.034 ) ## Likelihood ratio test= 17.57 on 1 df, p=0.00003 ## Wald test = 9.28 on 1 df, p=0.002 ## Score (logrank) test = 17.03 on 1 df, p=0.00004 Adjusted Cox regression for Group and Age cox3 &lt;- coxph(Surv(data$Time, data$Event) ~ as.factor(data$Group) + data$Age) summary(cox3) ## Call: ## coxph(formula = Surv(data$Time, data$Event) ~ as.factor(data$Group) + ## data$Age) ## ## n= 16, number of events= 12 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## as.factor(data$Group)2 2.1612 8.6812 0.9583 2.255 0.02412 * ## data$Age 0.5999 1.8220 0.2002 2.997 0.00273 ** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## as.factor(data$Group)2 8.681 0.1152 1.327 56.793 ## data$Age 1.822 0.5489 1.231 2.697 ## ## Concordance= 0.952 (se = 0.03 ) ## Likelihood ratio test= 24.32 on 2 df, p=0.000005 ## Wald test = 9.42 on 2 df, p=0.009 ## Score (logrank) test = 21.8 on 2 df, p=0.00002 Proportional hazards assumptions H0: independence between scaled Schoenfeld residuals and time. For each variable and for the global model. test.res &lt;- cox.zph(cox3) test.res ## chisq df p ## as.factor(data$Group) 0.6809 1 0.41 ## data$Age 0.0437 1 0.83 ## GLOBAL 1.2163 2 0.54 Graphical diagnostic library(survminer) ggcoxzph(test.res) These plots do not indicate obvious trends and are generally centered at zero. "],
["assignment-cancer-1.html", "3.7 Assignment cancer", " 3.7 Assignment cancer 2020-03-23 The file cancer.csv contains data of 1207 women who were diagnosed with breast cancer [event: death -&gt; variable: status]. #Load the library required for a survival analysis library(survival) #Read data data&lt;-read.csv(&quot;2.UploadedData/cancer.csv&quot;,sep=&quot;;&quot;,dec=&quot;,&quot;) Variables included in the dataset: str(data) ## &#39;data.frame&#39;: 1207 obs. of 11 variables: ## $ id : int 1 2 3 4 5 6 7 8 9 10 ... ## $ age : int 60 79 82 66 52 58 50 83 46 54 ... ## $ pathsize: num 99 99 99 99 99 99 99 99 99 99 ... ## $ lnpos : int 0 0 0 0 0 0 0 0 17 6 ... ## $ histgrad: int 3 4 2 2 3 4 2 3 4 2 ... ## $ er : int 0 2 2 1 2 2 1 0 2 1 ... ## $ pr : int 0 2 2 1 2 2 0 0 2 1 ... ## $ status : int 0 0 0 0 0 0 0 0 0 0 ... ## $ pathscat: int 99 99 99 99 99 99 99 99 99 99 ... ## $ ln_yesno: int 0 0 0 0 0 0 0 0 1 1 ... ## $ time : num 9.47 8.6 19.33 16.33 8.5 ... Produce unadjusted Cox regressions for the covariates: age, ln_yesno and pathsize Unadjusted Cox regression for age cox1 &lt;- coxph(Surv(data$time, data$status) ~ data$age) summary(cox1) ## Call: ## coxph(formula = Surv(data$time, data$status) ~ data$age) ## ## n= 1207, number of events= 72 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## data$age -0.020905 0.979312 0.009369 -2.231 0.0257 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## data$age 0.9793 1.021 0.9615 0.9975 ## ## Concordance= 0.587 (se = 0.042 ) ## Likelihood ratio test= 5.09 on 1 df, p=0.02 ## Wald test = 4.98 on 1 df, p=0.03 ## Score (logrank) test = 5.02 on 1 df, p=0.03 Unadjusted Cox regression for ln_yesno cox2 &lt;- coxph(Surv(data$time, data$status) ~ as.factor(data$ln_yesno)) summary(cox2) ## Call: ## coxph(formula = Surv(data$time, data$status) ~ as.factor(data$ln_yesno)) ## ## n= 1207, number of events= 72 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## as.factor(data$ln_yesno)1 0.9228 2.5164 0.2391 3.86 0.000114 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## as.factor(data$ln_yesno)1 2.516 0.3974 1.575 4.021 ## ## Concordance= 0.614 (se = 0.032 ) ## Likelihood ratio test= 13.73 on 1 df, p=0.0002 ## Wald test = 14.9 on 1 df, p=0.0001 ## Score (logrank) test = 15.98 on 1 df, p=0.00006 Unadjusted Cox regression for pathsize cox3 &lt;- coxph(Surv(data$time, data$status) ~ data$pathsize) summary(cox3) ## Call: ## coxph(formula = Surv(data$time, data$status) ~ data$pathsize) ## ## n= 1207, number of events= 72 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## data$pathsize 0.0008173 1.0008176 0.0042145 0.194 0.846 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## data$pathsize 1.001 0.9992 0.9926 1.009 ## ## Concordance= 0.69 (se = 0.029 ) ## Likelihood ratio test= 0.04 on 1 df, p=0.8 ## Wald test = 0.04 on 1 df, p=0.8 ## Score (logrank) test = 0.04 on 1 df, p=0.8 Produce adjusted Cox regression with the variables age,ln_yesno and pathsize as covariates. Adjusted Cox regression for age,ln_yesno and pathsize cox4 &lt;- coxph(Surv(data$time, data$status) ~ data$age + as.factor(data$ln_yesno) + data$pathsize) summary(cox4) ## Call: ## coxph(formula = Surv(data$time, data$status) ~ data$age + as.factor(data$ln_yesno) + ## data$pathsize) ## ## n= 1207, number of events= 72 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## data$age -0.0151156 0.9849980 0.0094274 -1.603 0.108851 ## as.factor(data$ln_yesno)1 0.8477195 2.3343174 0.2437639 3.478 0.000506 *** ## data$pathsize 0.0004944 1.0004945 0.0042497 0.116 0.907394 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## data$age 0.985 1.0152 0.9670 1.003 ## as.factor(data$ln_yesno)1 2.334 0.4284 1.4477 3.764 ## data$pathsize 1.000 0.9995 0.9922 1.009 ## ## Concordance= 0.629 (se = 0.043 ) ## Likelihood ratio test= 16.4 on 3 df, p=0.0009 ## Wald test = 17.54 on 3 df, p=0.0005 ## Score (logrank) test = 18.65 on 3 df, p=0.0003 Proportional hazards assumptions H0: independence between scaled Schoenfeld residuals and time. For each variable and for the global model. test.res &lt;- cox.zph(cox4) test.res ## chisq df p ## data$age 0.443 1 0.51 ## as.factor(data$ln_yesno) 1.753 1 0.19 ## data$pathsize 1.267 1 0.26 ## GLOBAL 3.211 3 0.36 Graphical diagnostic library(survminer) ggcoxzph(test.res) These plots do not indicate obvious trends and are generally centered at zero. "],
["assignment-alcohol.html", "3.8 Assignment alcohol", " 3.8 Assignment alcohol 2020-04-13 Exercise The “alcohol.sav” database contains information about alcoholic patients who have finished their hospital treatment. Explore the predictors of the time elapsed from the release of alcohol treatment to relapse. Variables: Weeks: the number of weeks until relapse or censoring Event: 0-no relapse; 1-relapse Group: group of treatment (0-detox only; 1-detox plus therapy) Symptoms: level of psychological distress (ranging from 1-lowest to 5-highest) AA: attending of Alcoholics Anonymous (AA) meetings (0-no; 1-yes) all code from the professor Open the file in spss. library(foreign) data &lt;- read.spss(file=&quot;2.UploadedData/Alcohol.sav&quot;, use.missing=TRUE) data ## $ID ## [1] 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 ## [26] 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 ## ## $weeks ## [1] 1 1 2 3 4 4 27 7 9 11 13 25 17 3 22 4 28 30 30 30 2 5 6 6 8 ## [26] 9 10 14 15 17 19 24 26 28 30 30 30 30 30 30 ## ## $Event ## [1] 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 0 0 0 1 1 1 1 1 1 1 1 1 1 0 1 1 1 0 0 0 0 ## [39] 0 0 ## ## $Group ## [1] detox detox detox detox detox detox detox ## [8] detox detox detox detox detox detox detox ## [15] detox detox detox detox detox detox treatment ## [22] treatment treatment treatment treatment treatment treatment treatment ## [29] treatment treatment treatment treatment treatment treatment treatment ## [36] treatment treatment treatment treatment treatment ## Levels: detox treatment ## ## $symptoms ## [1] 4.1 3.2 3.0 3.2 4.0 2.5 1.5 3.8 4.5 1.8 3.2 2.5 3.3 3.0 2.5 3.0 1.2 1.8 1.0 ## [20] 1.8 4.8 2.5 4.5 4.0 2.0 3.0 1.2 2.8 3.0 2.5 1.5 1.5 2.2 3.3 1.0 1.5 3.5 1.4 ## [39] 2.0 1.5 ## ## $AA ## [1] 0 0 0 0 0 0 1 0 1 0 1 0 0 0 1 0 1 1 1 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 1 0 0 1 ## [39] 1 1 ## ## attr(,&quot;label.table&quot;) ## attr(,&quot;label.table&quot;)$ID ## NULL ## ## attr(,&quot;label.table&quot;)$weeks ## NULL ## ## attr(,&quot;label.table&quot;)$Event ## NULL ## ## attr(,&quot;label.table&quot;)$Group ## treatment detox ## &quot;1&quot; &quot;0&quot; ## ## attr(,&quot;label.table&quot;)$symptoms ## NULL ## ## attr(,&quot;label.table&quot;)$AA ## NULL ## ## attr(,&quot;codepage&quot;) ## [1] 65001 ## attr(,&quot;variable.labels&quot;) ## named character(0) Create the survival object # load packages library(splines) library(survival) Surv(data$weeks, data$Event) ## [1] 1 1 2 3 4 4 27 7 9 11 13 25 17 3 22 4 28 30+ 30+ ## [20] 30+ 2 5 6 6 8 9 10 14 15 17 19+ 24 26 28 30+ 30+ 30+ 30+ ## [39] 30+ 30+ obtain the median of survival. # produce the Kaplan-Meier curves and estimates. KM1 &lt;- survfit(Surv(data$weeks, data$Event)~1) KM1 ## Call: survfit(formula = Surv(data$weeks, data$Event) ~ 1) ## ## n events median 0.95LCL 0.95UCL ## 40.0 30.0 14.5 9.0 27.0 obtain the survival in several points of time. summary(KM1) ## Call: survfit(formula = Surv(data$weeks, data$Event) ~ 1) ## ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 40 2 0.950 0.0345 0.885 1.000 ## 2 38 2 0.900 0.0474 0.812 0.998 ## 3 36 2 0.850 0.0565 0.746 0.968 ## 4 34 3 0.775 0.0660 0.656 0.916 ## 5 31 1 0.750 0.0685 0.627 0.897 ## 6 30 2 0.700 0.0725 0.571 0.857 ## 7 28 1 0.675 0.0741 0.544 0.837 ## 8 27 1 0.650 0.0754 0.518 0.816 ## 9 26 2 0.600 0.0775 0.466 0.773 ## 10 24 1 0.575 0.0782 0.441 0.751 ## 11 23 1 0.550 0.0787 0.416 0.728 ## 13 22 1 0.525 0.0790 0.391 0.705 ## 14 21 1 0.500 0.0791 0.367 0.682 ## 15 20 1 0.475 0.0790 0.343 0.658 ## 17 19 2 0.425 0.0782 0.296 0.609 ## 22 16 1 0.398 0.0777 0.272 0.584 ## 24 15 1 0.372 0.0769 0.248 0.558 ## 25 14 1 0.345 0.0758 0.225 0.531 ## 26 13 1 0.319 0.0745 0.202 0.504 ## 27 12 1 0.292 0.0729 0.179 0.476 ## 28 11 2 0.239 0.0686 0.136 0.420 Plot survival curve with confidence intervals plot(KM1, mark.time=F) produce the Kaplan-Meier curves and estimates according one factor (group of treatment) KM_group &lt;- survfit(Surv(data$weeks, data$Event)~1+data$Group) # To obtain the median of survival according one factor (group of treatment). KM_group ## Call: survfit(formula = Surv(data$weeks, data$Event) ~ 1 + data$Group) ## ## n events median 0.95LCL 0.95UCL ## data$Group=detox 20 17 10.0 4 27 ## data$Group=treatment 20 13 20.5 10 NA obtain the survival in several points of time according one factor (group of treatment). summary(KM_group) ## Call: survfit(formula = Surv(data$weeks, data$Event) ~ 1 + data$Group) ## ## data$Group=detox ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 1 20 2 0.90 0.0671 0.7777 1.000 ## 2 18 1 0.85 0.0798 0.7071 1.000 ## 3 17 2 0.75 0.0968 0.5823 0.966 ## 4 15 3 0.60 0.1095 0.4195 0.858 ## 7 12 1 0.55 0.1112 0.3700 0.818 ## 9 11 1 0.50 0.1118 0.3226 0.775 ## 11 10 1 0.45 0.1112 0.2772 0.731 ## 13 9 1 0.40 0.1095 0.2339 0.684 ## 17 8 1 0.35 0.1067 0.1926 0.636 ## 22 7 1 0.30 0.1025 0.1536 0.586 ## 25 6 1 0.25 0.0968 0.1170 0.534 ## 27 5 1 0.20 0.0894 0.0832 0.481 ## 28 4 1 0.15 0.0798 0.0528 0.426 ## ## data$Group=treatment ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 2 20 1 0.950 0.0487 0.859 1.000 ## 5 19 1 0.900 0.0671 0.778 1.000 ## 6 18 2 0.800 0.0894 0.643 0.996 ## 8 16 1 0.750 0.0968 0.582 0.966 ## 9 15 1 0.700 0.1025 0.525 0.933 ## 10 14 1 0.650 0.1067 0.471 0.897 ## 14 13 1 0.600 0.1095 0.420 0.858 ## 15 12 1 0.550 0.1112 0.370 0.818 ## 17 11 1 0.500 0.1118 0.323 0.775 ## 24 9 1 0.444 0.1123 0.271 0.729 ## 26 8 1 0.389 0.1112 0.222 0.681 ## 28 7 1 0.333 0.1083 0.176 0.630 Plot survival curve with confidence intervals according one factor (group of treatment) plot(KM_group,col=c(&quot;red&quot;, &quot;blue&quot;),mark.time=F,ylim=c(0,1),xlab=&quot;time&quot;,ylab=&quot;S&quot;) legend(&quot;topright&quot;, title=&quot;Group&quot;, legend=c( &quot;Detox&quot; , &quot;Treatment&quot;), col=c(&quot;red&quot; , &quot;blue&quot;), lty=1:1, cex=0.8) Log-rank test survdiff(Surv(data$weeks, data$Event)~1+data$Group, rho=0) ## Call: ## survdiff(formula = Surv(data$weeks, data$Event) ~ 1 + data$Group, ## rho = 0) ## ## N Observed Expected (O-E)^2/E (O-E)^2/V ## data$Group=detox 20 17 12.8 1.41 2.53 ## data$Group=treatment 20 13 17.2 1.04 2.53 ## ## Chisq= 2.5 on 1 degrees of freedom, p= 0.1 CONCLUSION FROM LOG-RANK TEST: there is not a significant difference in the time until relapse between groups (p=0.1) Unadjusted cox regression. coxuni &lt;- coxph(Surv(data$weeks, data$Event)~1+as.factor(data$Group)) summary(coxuni) ## Call: ## coxph(formula = Surv(data$weeks, data$Event) ~ 1 + as.factor(data$Group)) ## ## n= 40, number of events= 30 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## as.factor(data$Group)treatment -0.5836 0.5579 0.3693 -1.58 0.114 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## as.factor(data$Group)treatment 0.5579 1.793 0.2705 1.151 ## ## Concordance= 0.59 (se = 0.048 ) ## Likelihood ratio test= 2.53 on 1 df, p=0.1 ## Wald test = 2.5 on 1 df, p=0.1 ## Score (logrank) test = 2.57 on 1 df, p=0.1 CONCLUSION FROM Unadjusted cox regression: The conclusion from log-rank test is very similar to the conclusion from Log-Rank test. However, with this aproach (Cox regression), we can adjust for other variables. Multivariated cox regression. coxadjust &lt;- coxph(Surv(data$weeks, data$Event)~1+as.factor(data$Group)+data$symptoms+as.factor(data$AA)) summary(coxadjust) ## Call: ## coxph(formula = Surv(data$weeks, data$Event) ~ 1 + as.factor(data$Group) + ## data$symptoms + as.factor(data$AA)) ## ## n= 40, number of events= 30 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## as.factor(data$Group)treatment -0.9258 0.3962 0.3917 -2.364 0.018097 * ## data$symptoms 0.7779 2.1770 0.2227 3.493 0.000477 *** ## as.factor(data$AA)1 -1.0518 0.3493 0.4832 -2.177 0.029506 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## as.factor(data$Group)treatment 0.3962 2.5238 0.1839 0.8538 ## data$symptoms 2.1770 0.4594 1.4070 3.3683 ## as.factor(data$AA)1 0.3493 2.8627 0.1355 0.9006 ## ## Concordance= 0.805 (se = 0.037 ) ## Likelihood ratio test= 27.96 on 3 df, p=0.000004 ## Wald test = 22.1 on 3 df, p=0.00006 ## Score (logrank) test = 26.59 on 3 df, p=0.000007 When adjusted form level of psychological distress (symptoms) and attending AA there is a significant difference in the time until relapse between groups (p=0.018) "],
["repeated-measures-anova.html", "3.9 Repeated-measures ANOVA", " 3.9 Repeated-measures ANOVA 2020-04-20 3.9.1 Rats example (code from the professor) 5 rats are weighed 4 times with intervals of 4 weeks (week 8 to 20). ratwght&lt;-c(164,164,158,159,155,220,230,226,227,222,261,275,264,280,272,306,326,320,330,312) time &lt;- as.factor(c(rep(&quot;week08&quot;,5), rep(&quot;week12&quot;,5), rep(&quot;week16&quot;,5), rep(&quot;week20&quot;,5))) rat.ID &lt;- as.factor(rep(c(&quot;rat1&quot;,&quot;rat2&quot;,&quot;rat3&quot;, &quot;rat4&quot;, &quot;rat5&quot;),4)) datarat&lt;- data.frame(rat.ID,time,ratwght) Variables included in the dataset: library(printr) datarat rat.ID time ratwght rat1 week08 164 rat2 week08 164 rat3 week08 158 rat4 week08 159 rat5 week08 155 rat1 week12 220 rat2 week12 230 rat3 week12 226 rat4 week12 227 rat5 week12 222 rat1 week16 261 rat2 week16 275 rat3 week16 264 rat4 week16 280 rat5 week16 272 rat1 week20 306 rat2 week20 326 rat3 week20 320 rat4 week20 330 rat5 week20 312 plot the data ggplot(datarat, aes(x=time, y = ratwght, group=rat.ID)) + geom_line(aes(color=rat.ID)) + geom_point() + theme_bw() repeated-measures ANOVA library(nlme) model &lt;- lme(ratwght ~ time, random=~1|rat.ID, data = datarat) anova(model) numDF denDF F-value p-value (Intercept) 1 12 11442.4654 0 time 3 12 793.9949 0 The significant difference is due to the factor time, meaning that the rats gained in weight with time. Post-hoc tests library(emmeans) emmeans(model, pairwise ~ time) ## $emmeans ## time emmean SE df lower.CL upper.CL ## week08 160 3.08 4 151 169 ## week12 225 3.08 4 216 234 ## week16 270 3.08 4 262 279 ## week20 319 3.08 4 310 327 ## ## Degrees-of-freedom method: containment ## Confidence level used: 0.95 ## ## $contrasts ## contrast estimate SE df t.ratio p.value ## week08 - week12 -65.0 3.39 12 -19.159 &lt;.0001 ## week08 - week16 -110.4 3.39 12 -32.541 &lt;.0001 ## week08 - week20 -158.8 3.39 12 -46.807 &lt;.0001 ## week12 - week16 -45.4 3.39 12 -13.382 &lt;.0001 ## week12 - week20 -93.8 3.39 12 -27.648 &lt;.0001 ## week16 - week20 -48.4 3.39 12 -14.266 &lt;.0001 ## ## Degrees-of-freedom method: containment ## P value adjustment: tukey method for comparing a family of 4 estimates "],
["linear-mixed-models.html", "3.10 Linear Mixed Models", " 3.10 Linear Mixed Models 2020-04-28 Repeated measures designs: measurements are taken several times in each participant. Example: measure both arm lengths in each participant Model assumptions: Random intercept and slope are assumed independent Class exercise: Effect of sleep deprivation on cognitive performances. # Load the lme4 package library(lme4) # load the sleepstudy data set data(sleepstudy) data dimensions: dim(sleepstudy) ## [1] 180 3 variables: colnames(sleepstudy) ## [1] &quot;Reaction&quot; &quot;Days&quot; &quot;Subject&quot; data visualisation plot(sleepstudy$Days,sleepstudy$Reaction) plot(sleepstudy$Days,sleepstudy$Reaction) lines(sleepstudy$Days,sleepstudy$Reaction) Define the model: model &lt;- lmer(Reaction ~ Days + (Days | Subject), sleepstudy) summary(model) ## Linear mixed model fit by REML [&#39;lmerMod&#39;] ## Formula: Reaction ~ Days + (Days | Subject) ## Data: sleepstudy ## ## REML criterion at convergence: 1743.6 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -3.9536 -0.4634 0.0231 0.4634 5.1793 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## Subject (Intercept) 612.10 24.741 ## Days 35.07 5.922 0.07 ## Residual 654.94 25.592 ## Number of obs: 180, groups: Subject, 18 ## ## Fixed effects: ## Estimate Std. Error t value ## (Intercept) 251.405 6.825 36.838 ## Days 10.467 1.546 6.771 ## ## Correlation of Fixed Effects: ## (Intr) ## Days -0.138 Model interpretation: an average reaction time at enrollment of 251.4 ms; increase of 10.5 ms/day of sleep deprivation. Also: low correlation between random slope and intercept (7%) "],
["exercises.html", "3.11 Exercises", " 3.11 Exercises 2020-05-05 3.11.1 Diet (diet.sav) To evaluate weight loss diets, a sample of 60 overweight individuals was collected. The subjects were weighed before the diet (time = 0), three months after the begining (time = 1) and six months after the begining (time = 2) The subjects were randomized in two group (diet=1 and diet=2). The two groups had diferente diet plans. Read data library(foreign) data1 &lt;- read.spss(file=&quot;2.UploadedData/diet.sav&quot;, use.missing=TRUE) data1&lt;-as.data.frame(data1) # factorize variables data1$subject&lt;-as.factor(data1$subject) data1$diet&lt;-as.factor(data1$diet) str(data1) ## &#39;data.frame&#39;: 180 obs. of 4 variables: ## $ subject: Factor w/ 60 levels &quot;1&quot;,&quot;2&quot;,&quot;3&quot;,&quot;4&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ diet : Factor w/ 2 levels &quot;1&quot;,&quot;2&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ time : num 1 1 1 1 1 1 1 1 1 1 ... ## $ weigth : num 83.1 80.2 83.7 86.6 83 ... Was there a significant weight loss in this sample of 60 subjects? Fit a Linear Mixed Model library(afex) modeld&lt;-lmer(weigth ~ time + (time|subject) , data1) summary(modeld) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: weigth ~ time + (time | subject) ## Data: data1 ## ## REML criterion at convergence: 854.1 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.2421 -0.3782 0.0330 0.3630 3.4208 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 5.193 2.279 ## time 2.070 1.439 -0.08 ## Residual 2.402 1.550 ## Number of obs: 180, groups: subject, 60 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 88.4339 0.3463 58.9986 255.38 &lt;0.0000000000000002 *** ## time -5.6009 0.2335 59.0005 -23.99 &lt;0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) ## time -0.302 Was there a significant difference between diets? What is the best diet? modeldt&lt;-lmer(weigth ~ diet + time + (time|subject) , data1) summary(modeldt) ## Linear mixed model fit by REML. t-tests use Satterthwaite&#39;s method [ ## lmerModLmerTest] ## Formula: weigth ~ diet + time + (time | subject) ## Data: data1 ## ## REML criterion at convergence: 848.4 ## ## Scaled residuals: ## Min 1Q Median 3Q Max ## -2.1280 -0.3976 -0.0357 0.3700 3.4353 ## ## Random effects: ## Groups Name Variance Std.Dev. Corr ## subject (Intercept) 6.042 2.458 ## time 2.070 1.439 -0.37 ## Residual 2.402 1.550 ## Number of obs: 180, groups: subject, 60 ## ## Fixed effects: ## Estimate Std. Error df t value Pr(&gt;|t|) ## (Intercept) 89.3411 0.4861 60.7368 183.787 &lt; 0.0000000000000002 *** ## diet2 -1.8145 0.6395 58.0000 -2.837 0.00626 ** ## time -5.6009 0.2335 59.0001 -23.988 &lt; 0.0000000000000002 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Correlation of Fixed Effects: ## (Intr) diet2 ## diet2 -0.658 ## time -0.367 0.000 3.11.2 Child development (dev3yrheads.sav) In a study on risk factors that affect child development, 1145 children were followed up to the third year of life. At the age of three, a test was performed to assess the child’s psycho-motor development, resulting in a global development score (DEVSCORE). The dev3yr.sav database contains part of the information collected in the study. Read data data2 &lt;- read.spss(file=&quot;2.UploadedData/Dev3yrheads.sav&quot;, use.missing=TRUE) data2&lt;-as.data.frame(data2) str(data2) ## &#39;data.frame&#39;: 1145 obs. of 5 variables: ## $ numero : num 1 2 3 7 9 10 16 17 20 20 ... ## $ devscore: num 98 100 98 100 96 100 100 93 100 100 ... ## $ mage : num 29 20 23 25 31 29 25 36 27 27 ... ## $ resid : Factor w/ 3 levels &quot;urban&quot;,&quot;rural&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ score : num 31 29 34 30 30 34 33 32 34 33 ... Is the type of residence (urban, rural or degraded) a risk factor for psycho-motor development at the age of three when adjusted for the mother’s age (mage) and for neurological neonatal Score (score)? Fit a linear model model1&lt;-lm(devscore ~ mage + resid + score, data = data2) library(broom) tidy(model1) term estimate std.error statistic p.value (Intercept) 74.4788331 5.6290671 13.2311149 0.0000000 mage 0.0207467 0.0458686 0.4523066 0.6511341 residrural -0.8667158 0.7446393 -1.1639404 0.2446917 residdegradada 0.4965857 1.1105569 0.4471502 0.6548516 score 0.9132600 0.1656827 5.5121010 0.0000000 glance(model1) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual 0.0274182 0.0240056 8.063187 8.034472 0.0000022 5 -4012.148 8036.296 8066.554 74117.08 1140 "],
["conditional-logistic-regression.html", "3.12 Conditional Logistic Regression", " 3.12 Conditional Logistic Regression 2020-05-11 Internet (internet.sav) To evaluate if Sense Age is associated with internet usage in the elderly adjusting for SHLT Read data library(foreign) data &lt;- read.spss(file=&quot;2.UploadedData/internet.sav&quot;, use.missing=TRUE) data&lt;-as.data.frame(data) str(data) ## &#39;data.frame&#39;: 42 obs. of 4 variables: ## $ ID : num 1 1 2 2 3 3 4 4 5 5 ... ## $ SAGE : num 60 82 50 75 40 50 77 80 60 74 ... ## $ INTERNET: num 1 0 1 0 1 0 1 0 1 0 ... ## $ SHLT : num 1 1 4 4 3 3 3 3 3 4 ... Conditional Logistic model library(survival) model&lt;-clogit(INTERNET~SAGE+SHLT+strata(ID), data = data) summary(model) ## Call: ## coxph(formula = Surv(rep(1, 42L), INTERNET) ~ SAGE + SHLT + strata(ID), ## data = data, method = &quot;exact&quot;) ## ## n= 42, number of events= 20 ## ## coef exp(coef) se(coef) z Pr(&gt;|z|) ## SAGE -0.09066 0.91333 0.05044 -1.797 0.0723 . ## SHLT -1.21111 0.29787 0.81520 -1.486 0.1374 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## exp(coef) exp(-coef) lower .95 upper .95 ## SAGE 0.9133 1.095 0.82735 1.008 ## SHLT 0.2979 3.357 0.06027 1.472 ## ## Concordance= 0.775 (se = 0.127 ) ## Likelihood ratio test= 11.49 on 2 df, p=0.003 ## Wald test = 5.28 on 2 df, p=0.07 ## Score (logrank) test = 8.89 on 2 df, p=0.01 "],
["assignment-feature-selection.html", "3.13 Assignment feature selection", " 3.13 Assignment feature selection 2020-05-11 3.13.1 Boruta Package in R 113 patients at the Los Angeles County Hospital Shock Unit (afifi.sav) Build a model to predict the Hemoglobin at time 2 (HGB2) using the patients characteristics (AGE and SEX) and measurements taken at admission (SBP1, MAP1, HEART1, DBP1, CVP1, BSA1, CARDIAC1, APPTIME1, CIRCTIME1, URINE1, PLASVOL, REDCELL1, HCT1) Read data library(haven) data2 &lt;- read_sav(&quot;2.UploadedData/afifi.sav&quot;) data2&lt;-as.data.frame(data2) str(data2) ## &#39;data.frame&#39;: 113 obs. of 17 variables: ## $ IDNUM : num 340 412 426 444 515 517 518 522 526 527 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F4.0&quot; ## ..- attr(*, &quot;display_width&quot;)= int 7 ## $ AGE : num 70 56 47 75 61 68 71 30 78 40 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F4.0&quot; ## ..- attr(*, &quot;display_width&quot;)= int 6 ## $ SEX : dbl+lbl [1:113] 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 1, 0, 1, 0, 1, 1, 0, 0, ... ## ..@ format.spss : chr &quot;F3.0&quot; ## ..@ display_width: int 5 ## ..@ labels : Named num 0 1 ## .. ..- attr(*, &quot;names&quot;)= chr &quot;female&quot; &quot;male&quot; ## $ SBP1 : num 62 83 80 62 128 114 102 97 90 112 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F4.0&quot; ## ..- attr(*, &quot;display_width&quot;)= int 6 ## $ MAP1 : num 38 66 64 51 91 88 74 71 60 61 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F4.0&quot; ## ..- attr(*, &quot;display_width&quot;)= int 6 ## $ HEART1 : num 53 110 84 97 107 95 112 93 113 136 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F4.0&quot; ## $ DBP1 : num 29 60 55 43 71 73 65 55 46 48 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F4.0&quot; ## ..- attr(*, &quot;display_width&quot;)= int 6 ## $ CVP1 : num 10 1 1 13 11.5 1.7 1.9 15.1 8.6 25.7 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F5.1&quot; ## ..- attr(*, &quot;display_width&quot;)= int 7 ## $ BSA1 : num 1.87 1.82 1.8 1.3 NA 1.41 1.69 1.58 1.63 NA ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F5.2&quot; ## ..- attr(*, &quot;display_width&quot;)= int 7 ## $ CARDIAC1 : num 0.9 1.26 1.1 0.6 2.3 0.66 1.33 2.86 3.3 1.2 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F5.2&quot; ## ..- attr(*, &quot;display_width&quot;)= int 10 ## $ APPTIME1 : num 19 22.1 12 15 7.4 11.5 15.3 6 10 10.7 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F5.1&quot; ## ..- attr(*, &quot;display_width&quot;)= int 10 ## $ CIRCTIME1: num 39 40.7 28 59 19.3 22.5 31.3 14.4 19.4 34.5 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F5.1&quot; ## ..- attr(*, &quot;display_width&quot;)= int 11 ## $ URINE1 : num 0 110 80 5 140 110 80 10 21 1 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F4.0&quot; ## $ PLASVOL1 : num 39.4 36.2 37.3 33.5 74.7 56.2 32.1 55.7 65.3 70.9 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F5.1&quot; ## ..- attr(*, &quot;display_width&quot;)= int 10 ## $ REDCELL1 : num 24.1 24 27.2 20.8 18.6 20.6 14.1 15.1 16.8 16.2 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F5.1&quot; ## ..- attr(*, &quot;display_width&quot;)= int 10 ## $ HCT1 : num 40 50 49 43 24 34 40.3 30 27 42 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F5.1&quot; ## ..- attr(*, &quot;display_width&quot;)= int 7 ## $ HGB2 : num 11.2 15.4 9.9 10.9 9.8 10 9.4 10 10 9.1 ... ## ..- attr(*, &quot;format.spss&quot;)= chr &quot;F5.1&quot; ## ..- attr(*, &quot;display_width&quot;)= int 7 Implement Boruta results from boruta library(printr) print(boruta) ## Boruta performed 99 iterations in 16.2145 secs. ## 5 attributes confirmed important: AGE, CARDIAC1, CIRCTIME1, HCT1, ## REDCELL1; ## 7 attributes confirmed unimportant: CVP1, HEART1, IDNUM, PLASVOL1, ## SBP1 and 2 more; ## 4 tentative attributes left: APPTIME1, BSA1, DBP1, MAP1; selected features plot(boruta, xlab = &quot;&quot;, xaxt = &quot;n&quot;) lz&lt;-lapply(1:ncol(boruta$ImpHistory),function(i) boruta$ImpHistory[is.finite(boruta$ImpHistory[,i]),i]) names(lz) &lt;- colnames(boruta$ImpHistory) Labels &lt;- sort(sapply(lz,median)) axis(side = 1,las=2,labels = names(Labels), at = 1:ncol(boruta$ImpHistory), cex.axis = 0.7) print(getSelectedAttributes(boruta)) ## [1] &quot;AGE&quot; &quot;CARDIAC1&quot; &quot;CIRCTIME1&quot; &quot;REDCELL1&quot; &quot;HCT1&quot; Extract attribute statistics attStats(boruta) meanImp medianImp minImp maxImp normHits decision IDNUM 0.1473461 0.4062690 -2.2900612 1.9708158 0.0000000 Rejected AGE 4.1293472 4.1108677 0.9760621 7.5978631 0.7171717 Confirmed SEX -0.4363506 -0.7385775 -1.5190680 1.0372338 0.0000000 Rejected SBP1 1.2675332 1.3136094 -0.9877035 2.8263270 0.0000000 Rejected MAP1 3.2799824 3.2208108 1.2710892 8.1144665 0.6060606 Tentative HEART1 0.4802892 0.2703868 -0.8675089 2.9051318 0.0000000 Rejected DBP1 2.2732396 2.1263872 0.0189792 5.9360089 0.3434343 Tentative CVP1 -0.2915782 -0.5958642 -1.9801162 1.8558854 0.0000000 Rejected BSA1 3.3493170 3.2751972 1.5107522 7.1322729 0.6161616 Tentative CARDIAC1 4.4206597 4.3992274 2.2478339 6.4482453 0.7878788 Confirmed APPTIME1 2.6313731 2.5778063 0.4829264 5.2601524 0.4141414 Tentative CIRCTIME1 7.7565666 7.7979221 5.6473791 10.0225069 0.9797980 Confirmed URINE1 -0.3816787 -0.2914073 -1.2744776 0.3743351 0.0000000 Rejected PLASVOL1 1.6590866 1.6356248 -0.6676299 3.3260795 0.0000000 Rejected REDCELL1 4.2309630 4.1903148 1.3766086 6.8995463 0.7373737 Confirmed HCT1 8.7953939 8.6546255 6.6957330 10.8857185 1.0000000 Confirmed with the selected features, fit a linear model #form&lt;-getConfirmedFormula(boruta) form&lt;-getNonRejectedFormula(boruta) model&lt;-lm(form, data = data2) library(broom) tidy(model) term estimate std.error statistic p.value (Intercept) 1.0223582 1.8418376 0.5550751 0.5801480 AGE 0.0117378 0.0118097 0.9939077 0.3227926 MAP1 -0.0065953 0.0342614 -0.1925005 0.8477611 DBP1 0.0261556 0.0429193 0.6094124 0.5437059 BSA1 2.2066938 0.8543697 2.5828324 0.0113252 CARDIAC1 0.0361544 0.1477869 0.2446385 0.8072640 APPTIME1 0.0142515 0.0624277 0.2282883 0.8199128 CIRCTIME1 0.0391994 0.0334208 1.1729034 0.2437671 REDCELL1 0.0370067 0.0185559 1.9943391 0.0489812 HCT1 0.0604300 0.0275308 2.1950001 0.0306000 glance(model) r.squared adj.r.squared sigma statistic p.value df logLik AIC BIC deviance df.residual 0.3512673 0.2898084 1.649498 5.715485 0.0000026 10 -196.2836 414.5672 443.7608 258.4801 95 print(form) ## HGB2 ~ AGE + MAP1 + DBP1 + BSA1 + CARDIAC1 + APPTIME1 + CIRCTIME1 + ## REDCELL1 + HCT1 ## &lt;environment: 0x0000000019483460&gt; plot model assumptions par(mfrow=c(2,2)) plot(model) references https://www.analyticsvidhya.com/blog/2016/03/select-important-variables-boruta-package/ https://www.andreaperlato.com/mlpost/feature-selection-using-boruta-algorithm/ https://earlglynn.github.io/RNotes/package/Boruta/index.html "],
["glm-poisson-logistic.html", "3.14 GLM (Poisson - Logistic)", " 3.14 GLM (Poisson - Logistic) 2020-05-18 3.14.1 exercises 3.14.1.1 We want to know if the number of awards earned by students at one high school is associated to the type of program in which the student was enrolled (prog: 1=vocational, 2=general, 3=academic) and the score on their final exam in math (math). Read the data library(tidyverse) sim &lt;- read_csv(&quot;2.UploadedData/sim.csv&quot;) summary(sim) id num_awards prog math Min. : 1.00 Min. :0.00 Min. :1.000 Min. :33.00 1st Qu.: 50.75 1st Qu.:0.00 1st Qu.:2.000 1st Qu.:45.00 Median :100.50 Median :0.00 Median :2.000 Median :52.00 Mean :100.50 Mean :0.63 Mean :2.025 Mean :52.65 3rd Qu.:150.25 3rd Qu.:1.00 3rd Qu.:2.250 3rd Qu.:59.00 Max. :200.00 Max. :6.00 Max. :3.000 Max. :75.00 table outcome table(sim$num_awards) 0 1 2 3 4 5 6 124 49 13 9 2 2 1 sim&lt;-sim %&gt;% mutate(prog = factor(prog)) fit a poisson regression poisson &lt;- glm(num_awards ~ prog + math, data = sim, family = &quot;poisson&quot;) summary(poisson) ## ## Call: ## glm(formula = num_awards ~ prog + math, family = &quot;poisson&quot;, data = sim) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.2043 -0.8436 -0.5106 0.2558 2.6796 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -5.24712 0.65845 -7.969 0.0000000000000016 *** ## prog2 1.08386 0.35825 3.025 0.00248 ** ## prog3 0.36981 0.44107 0.838 0.40179 ## math 0.07015 0.01060 6.619 0.0000000000362501 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 287.67 on 199 degrees of freedom ## Residual deviance: 189.45 on 196 degrees of freedom ## AIC: 373.5 ## ## Number of Fisher Scoring iterations: 6 compute relative risks (RR) exp(cbind(RR = coef(poisson), confint(poisson))) RR 2.5 % 97.5 % (Intercept) 0.0052626 0.0014003 0.0185869 prog2 2.9560655 1.5450309 6.3972554 prog3 1.4474585 0.6125014 3.5467192 math 1.0726716 1.0507422 1.0953533 plot residuals against predicted pred&lt;-predict(poisson, data=sim) res&lt;-residuals(poisson, data=sim) plot(pred,res) histogram for the outcome hist(sim$num_awards) we can chek models zero inflation library(performance) check_zeroinflation(poisson, tolerance= 0.05) # tolerance we are willing to acept ## # Check for zero-inflation ## ## Observed zeros: 124 ## Predicted zeros: 122 ## Ratio: 0.98 and also dispersion library(AER) dispersiontest(poisson) ## ## Overdispersion test ## ## data: poisson ## z = 0.53224, p-value = 0.2973 ## alternative hypothesis: true dispersion is greater than 1 ## sample estimates: ## dispersion ## 1.047254 3.14.1.2 A researcher is interested in how variables, such as GRE (Graduate Record Exam scores), GPA (grade point average) and prestige of the undergraduate institution, effect admission into graduate school. school &lt;- read_csv(&quot;2.UploadedData/school.csv&quot;) summary(school) admit gre gpa rank Min. :0.0000 Min. :220.0 Min. :2.260 Min. :1.000 1st Qu.:0.0000 1st Qu.:520.0 1st Qu.:3.130 1st Qu.:2.000 Median :0.0000 Median :580.0 Median :3.395 Median :2.000 Mean :0.3175 Mean :587.7 Mean :3.390 Mean :2.485 3rd Qu.:1.0000 3rd Qu.:660.0 3rd Qu.:3.670 3rd Qu.:3.000 Max. :1.0000 Max. :800.0 Max. :4.000 Max. :4.000 fit a logistic regression model # factor variable rank school$rank&lt;-factor(school$rank) logistic &lt;- glm(admit ~ gre + gpa + rank, data = school, family = &quot;binomial&quot;) summary(logistic) ## ## Call: ## glm(formula = admit ~ gre + gpa + rank, family = &quot;binomial&quot;, ## data = school) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6268 -0.8662 -0.6388 1.1490 2.0790 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.989979 1.139951 -3.500 0.000465 *** ## gre 0.002264 0.001094 2.070 0.038465 * ## gpa 0.804038 0.331819 2.423 0.015388 * ## rank2 -0.675443 0.316490 -2.134 0.032829 * ## rank3 -1.340204 0.345306 -3.881 0.000104 *** ## rank4 -1.551464 0.417832 -3.713 0.000205 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 499.98 on 399 degrees of freedom ## Residual deviance: 458.52 on 394 degrees of freedom ## AIC: 470.52 ## ## Number of Fisher Scoring iterations: 4 compute OR and 95%CI ors&lt;-exp(cbind(OR = coef(logistic), confint(logistic))) ors OR 2.5 % 97.5 % (Intercept) 0.0185001 0.0018892 0.1665354 gre 1.0022670 1.0001376 1.0044457 gpa 2.2345448 1.1738582 4.3238349 rank2 0.5089310 0.2722897 0.9448343 rank3 0.2617923 0.1316417 0.5115181 rank4 0.2119375 0.0907155 0.4706961 plot ORs and CIs library(forestmodel) forest_model(logistic) Predict the probability (p) of admission probabilities &lt;- predict(logistic, type = &quot;response&quot;) add a column with the logit values school &lt;- school %&gt;% mutate(logit = log(probabilities/(1-probabilities))) plot linearity between logit and predictive variables ggplot(school, aes(logit, gpa)) + geom_point() + geom_smooth() ggplot(school, aes(logit, gre)) + geom_point() + geom_smooth() ggplot(school, aes(logit, rank)) + geom_point() + geom_smooth() test models’ goodness of fit library(ResourceSelection) hoslem.test(school$admit, fitted(logistic)) ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: school$admit, fitted(logistic) ## X-squared = 11.085, df = 8, p-value = 0.1969 as the p-value is higher than 0.05 we do not reject H0: good fit of the model to the data. ROC - AUC library(pROC) plot.roc(school$admit, fitted(logistic), print.auc=T, ci=TRUE) determine best cutoff test.roc&lt;-roc(school$admit, fitted(logistic)) coords(test.roc,&quot;best&quot;, res=&quot;threshold&quot;, transpose = F) threshold specificity sensitivity 0.3536984 0.7435897 0.5748031 Confusion matrix library(caret) confusionMatrix(factor(school$admit), factor(ifelse(probabilities&lt;0.5,0,1))) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 0 1 ## 0 254 19 ## 1 97 30 ## ## Accuracy : 0.71 ## 95% CI : (0.6628, 0.754) ## No Information Rate : 0.8775 ## P-Value [Acc &gt; NIR] : 1 ## ## Kappa : 0.1994 ## ## Mcnemar&#39;s Test P-Value : 0.0000000000008724 ## ## Sensitivity : 0.7236 ## Specificity : 0.6122 ## Pos Pred Value : 0.9304 ## Neg Pred Value : 0.2362 ## Prevalence : 0.8775 ## Detection Rate : 0.6350 ## Detection Prevalence : 0.6825 ## Balanced Accuracy : 0.6679 ## ## &#39;Positive&#39; Class : 0 ## "],
["survival-analysis.html", "3.15 Survival Analysis", " 3.15 Survival Analysis 2020-05-25 3.15.1 exercises code and comments from the professor read data and have a look at it # Load required packages library(survival) library(survminer) library(dplyr) data(ovarian) glimpse(ovarian) ## Rows: 26 ## Columns: 6 ## $ futime &lt;dbl&gt; 59, 115, 156, 421, 431, 448, 464, 475, 477, 563, 638, 744,... ## $ fustat &lt;dbl&gt; 1, 1, 1, 0, 1, 0, 1, 1, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0... ## $ age &lt;dbl&gt; 72.3315, 74.4932, 66.4658, 53.3644, 50.3397, 56.4301, 56.9... ## $ resid.ds &lt;dbl&gt; 2, 2, 2, 2, 2, 1, 2, 2, 2, 1, 1, 1, 2, 2, 1, 1, 2, 1, 1, 2... ## $ rx &lt;dbl&gt; 1, 1, 1, 2, 1, 1, 2, 2, 1, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 2... ## $ ecog.ps &lt;dbl&gt; 1, 1, 2, 1, 1, 2, 2, 2, 1, 2, 2, 1, 2, 1, 1, 2, 2, 1, 1, 1... help(ovarian) ovarian R Documentation Ovarian Cancer Survival Data Description Survival in a randomised trial comparing two treatments for ovarian cancer Usage ovarian Format futime: survival or censoring time fustat: censoring status age: in years resid.ds: residual disease present (1=no,2=yes) rx: treatment group ecog.ps: ECOG performance status (1 is better, see reference) Source Terry Therneau References Edmunson, J.H., Fleming, T.R., Decker, D.G., Malkasian, G.D., Jefferies, J.A., Webb, M.J., and Kvols, L.K., Different Chemotherapeutic Sensitivities and Host Factors Affecting Prognosis in Advanced Ovarian Carcinoma vs. Minimal Residual Disease. Cancer Treatment Reports, 63:241-47, 1979. # Change data labels ovarian$rx &lt;- factor(ovarian$rx, levels = c(&quot;1&quot;, &quot;2&quot;), labels = c(&quot;A&quot;, &quot;B&quot;)) ovarian$resid.ds &lt;- factor(ovarian$resid.ds, levels = c(&quot;1&quot;, &quot;2&quot;), labels = c(&quot;no&quot;, &quot;yes&quot;)) ovarian$ecog.ps &lt;- factor(ovarian$ecog.ps, levels = c(&quot;1&quot;, &quot;2&quot;), labels = c(&quot;good&quot;, &quot;bad&quot;)) Apply one of the techniques given in this course to answer the following questions: 3.15.1.1 Do patients benefit from therapy regimen A as opposed to regimen B? Fit survival data using the Kaplan-Meier method. A + behind survival times indicates censored data points. surv_object &lt;- Surv(time = ovarian$futime, event = ovarian$fustat) surv_object ## [1] 59 115 156 421+ 431 448+ 464 475 477+ 563 638 744+ ## [13] 769+ 770+ 803+ 855+ 1040+ 1106+ 1129+ 1206+ 1227+ 268 329 353 ## [25] 365 377+ The next step is to fit the Kaplan-Meier curves. You can easily do that by passing the surv_object to the survfit function. You can also stratify the curve depending on the treatment regimen rx that patients were assigned to. A summary() of the resulting fit1 object shows, among other things, survival times, the proportion of surviving patients at every time point, namely your p.1, p.2, … from above, and treatment groups. fit1 &lt;- survfit(surv_object ~ rx, data = ovarian) summary(fit1) ## Call: survfit(formula = surv_object ~ rx, data = ovarian) ## ## rx=A ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 59 13 1 0.923 0.0739 0.789 1.000 ## 115 12 1 0.846 0.1001 0.671 1.000 ## 156 11 1 0.769 0.1169 0.571 1.000 ## 268 10 1 0.692 0.1280 0.482 0.995 ## 329 9 1 0.615 0.1349 0.400 0.946 ## 431 8 1 0.538 0.1383 0.326 0.891 ## 638 5 1 0.431 0.1467 0.221 0.840 ## ## rx=B ## time n.risk n.event survival std.err lower 95% CI upper 95% CI ## 353 13 1 0.923 0.0739 0.789 1.000 ## 365 12 1 0.846 0.1001 0.671 1.000 ## 464 9 1 0.752 0.1256 0.542 1.000 ## 475 8 1 0.658 0.1407 0.433 1.000 ## 563 7 1 0.564 0.1488 0.336 0.946 You can examine the corresponding survival curve by passing the survival object to the ggsurvplot function. The pval = TRUE argument is very useful, because it plots the p-value of a log rank test as well! ggsurvplot(fit1, data = ovarian, pval = TRUE) By convention, vertical lines indicate censored data, their corresponding x values the time at which censoring occurred.The log-rank p-value of 0.3 indicates a non-significant result if you consider p &lt; 0.05 to indicate statistical significance. In this study, none of the treatments examined were significantly superior, although patients receiving treatment B are doing better in the follow-up. What about the other variables? Examine prdictive value of residual disease status fit2 &lt;- survfit(surv_object ~ resid.ds, data = ovarian) ggsurvplot(fit2, data = ovarian, pval = TRUE) The Kaplan-Meier plots stratified according to residual disease status look a bit different: The curves diverge early and the log-rank test is almost significant. You might want to argue that a follow-up study with an increased sample size could validate these results, that is, that patients with positive residual disease status have a significantly worse prognosis compared to patients without residual disease. 3.15.1.2 Do patients’ age and fitness significantly influence the outcome? Is residual disease a prognostic biomarker in terms of survival? Cox proportional hazards models allow you to include covariates. You can build Cox proportional hazards models using the coxph function and visualize them using the ggforest. These type of plot is called a forest plot. It shows so-called hazard ratios (HR) which are derived from the model for all covariates that we included in the formula in coxph. Briefly, an HR &gt; 1 indicates an increased risk of death (according to the definition of h(t)) if a specific condition is met by a patient. An HR &lt; 1, on the other hand, indicates a decreased risk. Fit a Cox proportional hazards model fit.coxph &lt;- coxph(surv_object ~ rx + resid.ds + age + ecog.ps, data = ovarian) ggforest(fit.coxph, data = ovarian) Every HR represents a relative risk of death that compares one instance of a binary feature to the other instance. For example, a hazard ratio of 0.4 for treatment groups tells you that patients who received treatment B have a reduced risk of dying compared to patients who received treatment A (which served as a reference to calculate the hazard ratio). As shown by the forest plot, the respective 95% confidence interval is 0.11 - 1.4 and this result is no significant. Using this model, you can see that the age variable significantly influence the patients risk of death in this study. This could be quite different from what you saw with the Kaplan-Meier estimator and the log-rank test. Whereas the former estimates the survival probability, the latter calculates the risk of death and respective hazard ratios. The results of these methods yield can differ in terms of significance. "],
["exam-a.html", "3.16 Exam A", " 3.16 Exam A 2020-06-01 Considere a base de dados “Estudo.sav” que contém informações sobre 113 pacientes. As informações baseiam-se em 3 variáveis: - State: estado relativo à cura (0: não curado; 1: curado) - Group: grupo de tratamento (0: sem tratamento; 1: com tratamento) - Duration: nº de dias de internamento Apresente um modelo para prever as chances de ser curado tendo como variáveis explicativas se fez ou não o tratamento estudado e a duração do internamento. Interprete os coeficientes e avalie o modelo encontrado. Vou começar por ler os dados e ver um sumário das varáveis library(foreign) library(tidyverse) estudo&lt;-read.spss(file=&quot;2.UploadedData/Estudo.sav&quot;, use.missing=TRUE) estudo&lt;-data.frame(estudo) # criar variavel State 0/1, apenas para ter a certeza que estou a modelar o que pretendo estudo&lt;-estudo %&gt;% mutate(State01 = ifelse(State == &quot;Curado/Cured&quot;,1,0)) summary(estudo) State Group Duration State01 Não curado/Not cured:48 Sem tratamento/ No treatment:56 Min. : 4.00 Min. :0.0000 Curado/Cured :65 Com tratamento/Treatment :57 1st Qu.: 6.00 1st Qu.:0.0000 NA NA Median : 7.00 Median :1.0000 NA NA Mean : 7.08 Mean :0.5752 NA NA 3rd Qu.: 8.00 3rd Qu.:1.0000 NA NA Max. :10.00 Max. :1.0000 Agora vou definir um modelo de regressão logística para prever a variável dependente ‘State’ através das variáveis independentes ‘Group’ e ‘Duration’ logistic &lt;- glm(State01 ~ Group + Duration, data = estudo, family = &quot;binomial&quot;) summary(logistic) ## ## Call: ## glm(formula = State01 ~ Group + Duration, family = &quot;binomial&quot;, ## data = estudo) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.6025 -1.0572 0.8107 0.8161 1.3095 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.234660 1.220563 -0.192 0.84754 ## GroupCom tratamento/Treatment 1.233532 0.414565 2.975 0.00293 ** ## Duration -0.007835 0.175913 -0.045 0.96447 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 154.08 on 112 degrees of freedom ## Residual deviance: 144.16 on 110 degrees of freedom ## AIC: 150.16 ## ## Number of Fisher Scoring iterations: 4 Verificamos que a variável Group é estatisticamente significativa (para um nível de con fiança de 5%) mas a variável Duration não. Calculando os valores de OR (e respectivos intervalos de confiança) para as variáveis no modelo: exp(cbind(OR = coef(logistic), confint(logistic))) OR 2.5 % 97.5 % (Intercept) 0.7908401 0.0694732 8.699169 GroupCom tratamento/Treatment 3.4333349 1.5465116 7.911563 Duration 0.9921954 0.7014431 1.406691 pondo-os num forest plot library(forestmodel) forest_model(logistic, format_options = list(text_size = 3)) Ou seja, o grupo com tratamento tem um risco para a cura 3.4 vezes maior quando comparado com o grupo sem tratamento e ajustando para a duração do internamento. Para sabermos da qualidade do modelo para explicar os dados, aplicamos o teste de Hosmer-Lemeshow library(ResourceSelection) hoslem.test(estudo$State, fitted(logistic)) ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: estudo$State, fitted(logistic) ## X-squared = 113, df = 8, p-value &lt; 0.00000000000000022 Como o valor de p é menor que 0.05, rejeitamos a hipótese de goodness of fit deste modelo. Ou seja o modelo não é um bom fit para os dados Com as probabilidades obtidas do modelo podemos também calcular a qualidade do modelo para descriminar entre a cura e a não cura, com uma curva ROC e respectivo valor de AUC library(pROC) plot.roc(estudo$State, fitted(logistic), print.auc=T, ci=TRUE, thresholds=&quot;best&quot;, print.thres=&quot;best&quot;) O valor da AUC (0.66) corrobora que este modelo não é muito bom a descriminar entre a cura e a não cura. "],
["exam-b.html", "3.17 Exam B", " 3.17 Exam B 2020-06-01 Considere a base de dados “headache.sav” que contém informações sobre 80 pacientes com cefaleias frequentes, alguns dos doentes fizeram acupuntura. Gostaríamos de saber se a acupuntura reduz o número de cefaleias dos doentes quando ajustado para a idade. - headache: número de cefaleias num mês - acupuntura: grupo (0=no, 1=yes) - age: idade em anos Vou começar por ler os dados e fazer um resumo das variáveis: library(foreign) heada&lt;-read.spss(file=&quot;2.UploadedData/headache.sav&quot;, use.missing=TRUE) heada&lt;-data.frame(heada) summary(heada) acupuncture age headache não/no :55 Min. :20.00 Min. :0 sim/yes:25 1st Qu.:27.00 1st Qu.:1 NA Median :31.00 Median :1 NA Mean :30.43 Mean :2 NA 3rd Qu.:33.00 3rd Qu.:2 NA Max. :40.00 Max. :7 Fazen do um histogram a para a variave l ‘headache’ hist(heada$headache) Como neste problema a variável dependente é uma contagem e a sua distribuição é skewed para a direita, podemos aplicar uma regressão de poisson. poisson &lt;- glm(headache ~ acupuncture + age, data = heada, family = &quot;poisson&quot;) summary(poisson) ## ## Call: ## glm(formula = headache ~ acupuncture + age, family = &quot;poisson&quot;, ## data = heada) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -2.4000 -0.5103 -0.5097 0.3043 3.1405 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) 0.47210196 0.57917272 0.815 0.414997 ## acupuncturesim/yes 0.58768536 0.16127088 3.644 0.000268 *** ## age -0.00006793 0.01842918 -0.004 0.997059 ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for poisson family taken to be 1) ## ## Null deviance: 94.389 on 79 degrees of freedom ## Residual deviance: 81.153 on 77 degrees of freedom ## AIC: 274 ## ## Number of Fisher Scoring iterations: 5 Obtemos uma valor estatisticamente significativo para o coeficiente de ‘acupuncture’ mas não para ‘age’ Calculando os RR (e respectivos intervalos de confiança) exp(cbind(RR = coef(poisson), confint(poisson))) RR 2.5 % 97.5 % (Intercept) 1.6033609 0.5086059 4.935990 acupuncturesim/yes 1.7998177 1.3088986 2.465976 age 0.9999321 0.9645351 1.036866 Dos resultados de RR, podemos concluir que 1.8 é a taxa estimada de dores de cabeça para quem faz acupuntura relativamente à taxa estimada de dores de cabeça para quem não faz acupuntura, ajustando para a idade.Ou seja, quem faz acupuntura tem um risco aumentado de ter mais dores de cabeça quando comparado com com não faz acupuntura. Podemos verificar também dispersão do modelo library(AER) dispersiontest(poisson) ## ## Overdispersion test ## ## data: poisson ## z = 0.57572, p-value = 0.2824 ## alternative hypothesis: true dispersion is greater than 1 ## sample estimates: ## dispersion ## 1.137103 neste caso (p=0.28), não podemos afirmar que a dispersão seja estatisticamente diferente de 1. "],
["learn.html", "4 LEARN", " 4 LEARN Macine Learning Conteúdos programáticos O processo de extração de conhecimento de dados Compreensão do negócio Compreensão dos dados Pré-processamento de dados Modelação de dados Avaliação de modelos de extração de conhecimento de dados Implantação prática dos resultados de modelação Aprendizagem automática Aprender conceitos a partir de dados Processos indutivos vs processos dedutivos Viés indutivo Validação de modelos Medidas de erro e processos de estimação Aprendizagem supervisionada Árvores de decisão Redes Bayesianas Redes neuronais Deep learning e análise de grandes bases de dados Aprendizagem não supervisionada Análise de clusters Deteção de casos extremos e anomalias Associação e análise de padrões frequentes Interpretação de grandes bases de dados "],
["classification.html", "4.1 Classification", " 4.1 Classification 2020-02-04 This exercise was done as described by: https://machinelearningmastery.com/machine-learning-in-r-step-by-step/ but using a data set from: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra# Read the data dataset &lt;- read.csv(&quot;2.UploadedData/dataR2.csv&quot;) Factor variable Classification dataset$Classification&lt;-as.factor(dataset$Classification) NOTE: in order to use the caret packages in all its’ capabilities you must do install.packages(\"caret\", dependencies = T) create a validation dataset and use the remaing for training library(caret) # create a list of 80% of the rows in the original dataset we can use for training validation_index &lt;- createDataPartition(dataset$Classification, p=0.80, list=FALSE) # select 20% of the data for validation validation &lt;- dataset[-validation_index,] # use the remaining 80% of data to training and testing the models dataset &lt;- dataset[validation_index,] dimensions of the new dataset dim(dataset) ## [1] 94 10 list types for each attribute sapply(dataset, class) ## Age BMI Glucose Insulin HOMA ## &quot;integer&quot; &quot;numeric&quot; &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; ## Leptin Adiponectin Resistin MCP.1 Classification ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;factor&quot; take a peek at the first 5 rows of the data head(dataset) Age BMI Glucose Insulin HOMA Leptin Adiponectin Resistin MCP.1 Classification 2 83 20.69049 92 3.115 0.7068973 8.8438 5.429285 4.06405 468.786 1 3 82 23.12467 91 4.498 1.0096511 17.9393 22.432040 9.27715 554.697 1 4 68 21.36752 77 3.226 0.6127249 9.8827 7.169560 12.76600 928.220 1 5 86 21.11111 92 3.549 0.8053864 6.6994 4.819240 10.57635 773.920 1 6 49 22.85446 92 3.226 0.7320869 6.8317 13.679750 10.31760 530.410 1 7 89 22.70000 77 4.690 0.8907873 6.9640 5.589865 12.93610 1256.083 1 list the levels for the class levels(dataset$Classification) ## [1] &quot;1&quot; &quot;2&quot; summarize the class distribution percentage &lt;- prop.table(table(dataset$Classification)) * 100 cbind(freq=table(dataset$Classification), percentage=percentage) freq percentage 42 44.68085 52 55.31915 summarize attribute distributions summary(dataset) Age BMI Glucose Insulin HOMA Leptin Adiponectin Resistin MCP.1 Classification Min. :25.00 Min. :18.37 Min. : 60.00 Min. : 2.432 Min. : 0.5079 Min. : 6.334 Min. : 1.656 Min. : 3.210 Min. : 45.84 1:42 1st Qu.:45.00 1st Qu.:22.86 1st Qu.: 86.00 1st Qu.: 4.387 1st Qu.: 0.9314 1st Qu.:12.279 1st Qu.: 5.375 1st Qu.: 6.963 1st Qu.: 295.14 2:52 Median :54.00 Median :27.69 Median : 92.50 Median : 5.796 Median : 1.3744 Median :19.587 Median : 8.294 Median :10.636 Median : 501.24 NA Mean :56.87 Mean :27.59 Mean : 98.49 Mean : 9.821 Mean : 2.7169 Mean :26.618 Mean : 9.829 Mean :14.840 Mean : 557.96 NA 3rd Qu.:69.00 3rd Qu.:31.25 3rd Qu.:102.00 3rd Qu.:11.670 3rd Qu.: 2.7982 3rd Qu.:37.688 3rd Qu.:11.493 3rd Qu.:17.510 3rd Qu.: 728.66 NA Max. :89.00 Max. :38.58 Max. :201.00 Max. :51.814 Max. :25.0503 Max. :90.280 Max. :38.040 Max. :82.100 Max. :1698.44 NA split input and output x &lt;- dataset[,1:9] y &lt;- dataset[,10] boxplot for each attribute on one image par(mfrow=c(2,5)) for(i in 1:9) { boxplot(x[,i], main=names(dataset)[i]) } barplot for class breakdown par(mfrow=c(1,1)) plot(y) scatterplot matrix scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) featurePlot(x=x, y=y, plot=&quot;ellipse&quot;) box and whisker plots for each attribute scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) featurePlot(x=x, y=y, plot=&quot;box&quot;, scales=scales) density plots for each attribute by class value scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) featurePlot(x=x, y=y, plot=&quot;density&quot;, scales=scales) Run algorithms using 10-fold cross validation control &lt;- trainControl(method=&quot;cv&quot;, number=10) metric &lt;- &quot;Accuracy&quot; linear algorithms set.seed(7) fit.lda &lt;- train(Classification~., data=dataset, method=&quot;lda&quot;, metric=metric, trControl=control) nonlinear algorithms CART set.seed(7) fit.cart &lt;- train(Classification~., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) kNN set.seed(7) fit.knn &lt;- train(Classification~., data=dataset, method=&quot;knn&quot;, metric=metric, trControl=control) advanced algorithms SVM set.seed(7) fit.svm &lt;- train(Classification~., data=dataset, method=&quot;svmRadial&quot;, metric=metric, trControl=control) Random Forest set.seed(7) fit.rf &lt;- train(Classification~., data=dataset, method=&quot;rf&quot;, metric=metric, trControl=control) summarize accuracy of models results &lt;- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf)) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: lda, cart, knn, svm, rf ## Number of resamples: 10 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## lda 0.5555556 0.6000000 0.7777778 0.7553535 0.8888889 1.0000000 0 ## cart 0.4545455 0.6750000 0.7888889 0.7843434 0.9722222 1.0000000 0 ## knn 0.1000000 0.5555556 0.5555556 0.5285859 0.5888889 0.7777778 0 ## svm 0.6000000 0.6666667 0.7388889 0.7825253 0.8888889 1.0000000 0 ## rf 0.5454545 0.6750000 0.7777778 0.7812121 0.8694444 1.0000000 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## lda 0.05263158 0.20769231 0.55000000 0.51069057 0.7804878 1.0000000 0 ## cart -0.13793103 0.34305408 0.57500000 0.55705892 0.9423077 1.0000000 0 ## knn -0.66666667 0.05263158 0.05263158 0.04082771 0.1631579 0.5263158 0 ## svm 0.23076923 0.31613508 0.47500000 0.56575672 0.7804878 1.0000000 0 ## rf 0.06779661 0.33076923 0.55000000 0.55331812 0.7375000 1.0000000 0 compare accuracy of models dotplot(results) summarize Best Model print(fit.rf) ## Random Forest ## ## 94 samples ## 9 predictor ## 2 classes: &#39;1&#39;, &#39;2&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 85, 85, 83, 84, 84, 85, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.7734343 0.5397959 ## 5 0.7812121 0.5533181 ## 9 0.7589899 0.5139748 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 5. estimate skill of LDA on the validation dataset predictions &lt;- predict(fit.rf, validation) confusionMatrix(predictions, validation$Classification) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 2 ## 1 5 2 ## 2 5 10 ## ## Accuracy : 0.6818 ## 95% CI : (0.4513, 0.8614) ## No Information Rate : 0.5455 ## P-Value [Acc &gt; NIR] : 0.1419 ## ## Kappa : 0.3419 ## ## Mcnemar&#39;s Test P-Value : 0.4497 ## ## Sensitivity : 0.5000 ## Specificity : 0.8333 ## Pos Pred Value : 0.7143 ## Neg Pred Value : 0.6667 ## Prevalence : 0.4545 ## Detection Rate : 0.2273 ## Detection Prevalence : 0.3182 ## Balanced Accuracy : 0.6667 ## ## &#39;Positive&#39; Class : 1 ## In this post you discovered step-by-step how to complete your first machine learning project in R "],
["crossvalidation.html", "4.2 Crossvalidation", " 4.2 Crossvalidation 2020-02-11 # Load package &#39;caret&#39; library(caret) # Load package &#39;pROC&#39; library(pROC) continuation of the previous classe. All the code was provided by the professor. # loading and preparing the data dataset &lt;- read.csv(&quot;2.UploadedData/dataR2.csv&quot;) # Define Classification as factor dataset$Classification &lt;- factor(dataset$Classification, levels=1:2, labels=c(&quot;Control&quot;, &quot;Patient&quot;)) # Holdout a validation set, by defining the indices of the training set training.index &lt;- createDataPartition(dataset$Classification, p=0.8, list=FALSE) validation &lt;- dataset[-training.index,] dataset &lt;- dataset[training.index,] Dimensions (should be 116 observations and 10 variables) dim(dataset) ## [1] 94 10 # Split input and output input &lt;- dataset[,-10] output &lt;- dataset[,10] 4.2.1 Test harness Define the metric metric &lt;- \"ROC\" Define the estimation method control &lt;- trainControl(...) Train the model with chosen metric and method (output is factor with “Yes” or “No”) fit.model &lt;- train(output ~ ., data=dataset, method=\"nnet\", metric=metric, trControl=control) Plot ROC curve plot.roc(fit.model$pred$obs,fit.model$pred$Yes, main=fit.model$method, print.auc=T) # Run algorithm using different validation approaches metric &lt;- &quot;ROC&quot; 4.2.1.1 Run algorithm using 20% hold-out validation control &lt;- trainControl(method=&quot;LGOCV&quot;, p=0.8, number=1, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE) set.seed(7) fit.cart.hold &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.nb.hold &lt;- train(Classification ~ ., data=dataset, method=&quot;naive_bayes&quot;, metric=metric, trControl=control) Summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.hold, nb=fit.nb.hold) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, nb ## Number of resamples: 1 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.59375 0.59375 0.59375 0.59375 0.59375 0.59375 0 ## nb 0.66250 0.66250 0.66250 0.66250 0.66250 0.66250 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.625 0.625 0.625 0.625 0.625 0.625 0 ## nb 0.750 0.750 0.750 0.750 0.750 0.750 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.6 0.6 0.6 0.6 0.6 0.6 0 ## nb 0.6 0.6 0.6 0.6 0.6 0.6 0 ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;20% Hold-out -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) 4.2.1.2 Run algorithm using multiple 20% hold-out validation control &lt;- trainControl(method=&quot;LGOCV&quot;, p=0.8, number=25, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE) set.seed(7) fit.cart.mhold &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.nb.mhold &lt;- train(Classification ~ ., data=dataset, method=&quot;naive_bayes&quot;, metric=metric, trControl=control) Summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.mhold, nb=fit.nb.mhold) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, nb ## Number of resamples: 25 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.4625 0.59375 0.6875 0.67575 0.775 0.9250 0 ## nb 0.4875 0.73750 0.7875 0.77000 0.825 0.9375 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.25 0.500 0.625 0.605 0.75 1 0 ## nb 0.50 0.625 0.750 0.705 0.75 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.4 0.6 0.8 0.708 0.8 0.9 0 ## nb 0.4 0.6 0.7 0.664 0.8 0.9 0 ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;25 x 20% Hold-out -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) 4.2.1.3 Run algorithm using 10-fold cross validation control &lt;- trainControl(method=&quot;cv&quot;, number=10, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE, repeats = 1) set.seed(7) fit.cart.cv &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.nb.cv &lt;- train(Classification ~ ., data=dataset, method=&quot;naive_bayes&quot;, metric=metric, trControl=control) Summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.cv, nb=fit.nb.cv) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, nb ## Number of resamples: 10 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.52 0.7260417 0.775 0.7524167 0.825 0.9 0 ## nb 0.56 0.6875000 0.825 0.8010000 0.900 1.0 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.25 0.50 0.50 0.52 0.60 0.75 0 ## nb 0.40 0.75 0.75 0.77 0.95 1.00 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.2 0.80 0.8 0.7433333 0.825 1 0 ## nb 0.2 0.45 0.7 0.6600000 0.800 1 0 ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;10-fold CV -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) 4.2.1.4 Run algorithm using 25 times 10-fold cross validation control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE, repeats = 25) set.seed(7) fit.cart.rcv &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.nb.rcv &lt;- train(Classification ~ ., data=dataset, method=&quot;naive_bayes&quot;, metric=metric, trControl=control) Summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.rcv, nb=fit.nb.rcv) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, nb ## Number of resamples: 250 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.25 0.65 0.7345833 0.7315767 0.8333333 1 0 ## nb 0.15 0.70 0.8000000 0.7823867 0.9000000 1 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0 0.5 0.60 0.6010 0.75 1 0 ## nb 0 0.5 0.75 0.7188 1.00 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.0 0.6 0.8 0.7442667 0.8 1 0 ## nb 0.2 0.6 0.6 0.6762667 0.8 1 0 ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;25 x 10-fold CV -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) 4.2.1.5 Run algorithm using leave-one-out validation control &lt;- trainControl(method=&quot;LOOCV&quot;, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE) set.seed(7) fit.cart.loo &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.nb.loo &lt;- train(Classification ~ ., data=dataset, method=&quot;naive_bayes&quot;, metric=metric, trControl=control) Summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.loo, nb=fit.nb.loo) #results &lt;- resamples(fit.models) #summary(results) summary(fit.models) Length Class Mode rpart 23 train list nb 23 train list ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;Leave-One-Out -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) 4.2.1.6 Run algorithm using bootstrap validation control &lt;- trainControl(method=&quot;boot_all&quot;, number=25, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE) set.seed(7) fit.cart.boot &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.nb.boot &lt;- train(Classification ~ ., data=dataset, method=&quot;naive_bayes&quot;, metric=metric, trControl=control) Summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.boot, nb=fit.nb.boot) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, nb ## Number of resamples: 25 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.3777778 0.5798319 0.6729323 0.6546427 0.7182540 0.7924837 0 ## nb 0.5079365 0.7074074 0.7625418 0.7416113 0.8134921 0.8791209 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.3333333 0.5000000 0.5833333 0.5986555 0.7058824 1.0000000 0 ## nb 0.2666667 0.6153846 0.7142857 0.6817272 0.7500000 0.9230769 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.2666667 0.5882353 0.6521739 0.6419404 0.7222222 1.0000000 0 ## nb 0.4736842 0.5714286 0.7058824 0.6707410 0.7391304 0.9444444 0 ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;25 x Bootstrap -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) 4.2.2 Make predictions Estimate skill of GLM Step AIC on the validation dataset par(mfrow=c(1,1)) predictions.prob &lt;- predict(fit.nb.boot, validation, type=&quot;prob&quot;) predictions &lt;- predict(fit.nb.boot, validation, type=&quot;raw&quot;) confusionMatrix(predictions, validation$Classification) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Control Patient ## Control 8 6 ## Patient 2 6 ## ## Accuracy : 0.6364 ## 95% CI : (0.4066, 0.828) ## No Information Rate : 0.5455 ## P-Value [Acc &gt; NIR] : 0.2622 ## ## Kappa : 0.2903 ## ## Mcnemar&#39;s Test P-Value : 0.2888 ## ## Sensitivity : 0.8000 ## Specificity : 0.5000 ## Pos Pred Value : 0.5714 ## Neg Pred Value : 0.7500 ## Prevalence : 0.4545 ## Detection Rate : 0.3636 ## Detection Prevalence : 0.6364 ## Balanced Accuracy : 0.6500 ## ## &#39;Positive&#39; Class : Control ## plot.roc(validation$Classification, predictions.prob$Patient, print.auc=T) "],
["regression-trees.html", "4.3 Regression Trees", " 4.3 Regression Trees 2020-02-18 4.3.1 Using ‘rpart’ and the Breast Cancer Coimbra data set # Define the filename filename &lt;- &quot;2.UploadedData/dataR2.csv&quot; # Load the CSV file from the local directory dataset &lt;- read.csv(filename, header=T) dataset$Classification&lt;-factor(dataset$Classification, levels=1:2, labels=c(&quot;Controls&quot;,&quot;Patients&quot;)) library(rpart) # learn tree with Gini impurity tree.gini&lt;-rpart(Classification~.,data = dataset) # learn tree with information gain tree.information&lt;-rpart(Classification~., data=dataset, parms=list(split=&quot;information&quot;)) Summary Information summary(tree.information) ## Call: ## rpart(formula = Classification ~ ., data = dataset, parms = list(split = &quot;information&quot;)) ## n= 116 ## ## CP nsplit rel error xerror xstd ## 1 0.38461538 0 1.0000000 1.0000000 0.10300524 ## 2 0.08653846 1 0.6153846 0.8653846 0.10092601 ## 3 0.03846154 3 0.4423077 0.5769231 0.09069378 ## 4 0.01000000 7 0.2307692 0.5769231 0.09069378 ## ## Variable importance ## Age Glucose BMI HOMA Resistin Insulin ## 25 21 12 11 9 7 ## MCP.1 Leptin Adiponectin ## 6 6 3 ## ## Node number 1: 116 observations, complexity param=0.3846154 ## predicted class=Patients expected loss=0.4482759 P(node) =1 ## class counts: 52 64 ## probabilities: 0.448 0.552 ## left son=2 (50 obs) right son=3 (66 obs) ## Primary splits: ## Glucose &lt; 91.5 to the left, improve=11.586660, (0 missing) ## HOMA &lt; 2.169985 to the left, improve= 8.039333, (0 missing) ## Age &lt; 37 to the left, improve= 7.453390, (0 missing) ## Resistin &lt; 13.71318 to the left, improve= 6.993164, (0 missing) ## Insulin &lt; 10.285 to the left, improve= 6.048777, (0 missing) ## Surrogate splits: ## HOMA &lt; 1.231021 to the left, agree=0.698, adj=0.30, (0 split) ## Age &lt; 37 to the left, agree=0.664, adj=0.22, (0 split) ## Insulin &lt; 4.8075 to the left, agree=0.629, adj=0.14, (0 split) ## Adiponectin &lt; 22.12789 to the right, agree=0.612, adj=0.10, (0 split) ## MCP.1 &lt; 532.317 to the right, agree=0.612, adj=0.10, (0 split) ## ## Node number 2: 50 observations, complexity param=0.08653846 ## predicted class=Controls expected loss=0.3 P(node) =0.4310345 ## class counts: 35 15 ## probabilities: 0.700 0.300 ## left son=4 (15 obs) right son=5 (35 obs) ## Primary splits: ## Age &lt; 44.5 to the left, improve=6.641431, (0 missing) ## Resistin &lt; 13.24805 to the left, improve=6.556913, (0 missing) ## BMI &lt; 31.01992 to the right, improve=5.052061, (0 missing) ## Insulin &lt; 3.4445 to the right, improve=3.236703, (0 missing) ## MCP.1 &lt; 228.949 to the left, improve=2.734089, (0 missing) ## Surrogate splits: ## BMI &lt; 31.71078 to the right, agree=0.78, adj=0.267, (0 split) ## Leptin &lt; 31.16805 to the right, agree=0.76, adj=0.200, (0 split) ## MCP.1 &lt; 183.26 to the left, agree=0.76, adj=0.200, (0 split) ## Adiponectin &lt; 12.34449 to the right, agree=0.72, adj=0.067, (0 split) ## ## Node number 3: 66 observations, complexity param=0.03846154 ## predicted class=Patients expected loss=0.2575758 P(node) =0.5689655 ## class counts: 17 49 ## probabilities: 0.258 0.742 ## left son=6 (47 obs) right son=7 (19 obs) ## Primary splits: ## Age &lt; 48.5 to the right, improve=6.897003, (0 missing) ## Glucose &lt; 118.5 to the left, improve=4.016843, (0 missing) ## Leptin &lt; 7.93315 to the left, improve=3.666612, (0 missing) ## Resistin &lt; 11.92665 to the left, improve=3.111218, (0 missing) ## HOMA &lt; 2.173469 to the left, improve=2.979801, (0 missing) ## Surrogate splits: ## HOMA &lt; 0.6920566 to the right, agree=0.742, adj=0.105, (0 split) ## Adiponectin &lt; 15.105 to the left, agree=0.742, adj=0.105, (0 split) ## Insulin &lt; 3.057 to the right, agree=0.727, adj=0.053, (0 split) ## ## Node number 4: 15 observations ## predicted class=Controls expected loss=0 P(node) =0.1293103 ## class counts: 15 0 ## probabilities: 1.000 0.000 ## ## Node number 5: 35 observations, complexity param=0.08653846 ## predicted class=Controls expected loss=0.4285714 P(node) =0.3017241 ## class counts: 20 15 ## probabilities: 0.571 0.429 ## left son=10 (20 obs) right son=11 (15 obs) ## Primary splits: ## Resistin &lt; 13.24805 to the left, improve=7.941566, (0 missing) ## Age &lt; 74.5 to the right, improve=5.353821, (0 missing) ## BMI &lt; 30.13047 to the right, improve=1.622848, (0 missing) ## Glucose &lt; 85.5 to the left, improve=1.479354, (0 missing) ## Insulin &lt; 3.207 to the right, improve=1.464713, (0 missing) ## Surrogate splits: ## BMI &lt; 27.9125 to the left, agree=0.714, adj=0.333, (0 split) ## HOMA &lt; 1.408116 to the left, agree=0.714, adj=0.333, (0 split) ## Insulin &lt; 6.283 to the left, agree=0.686, adj=0.267, (0 split) ## Age &lt; 65.5 to the right, agree=0.657, adj=0.200, (0 split) ## Glucose &lt; 86.5 to the left, agree=0.657, adj=0.200, (0 split) ## ## Node number 6: 47 observations, complexity param=0.03846154 ## predicted class=Patients expected loss=0.3617021 P(node) =0.4051724 ## class counts: 17 30 ## probabilities: 0.362 0.638 ## left son=12 (37 obs) right son=13 (10 obs) ## Primary splits: ## Glucose &lt; 118.5 to the left, improve=5.231699, (0 missing) ## Leptin &lt; 9.2319 to the left, improve=4.358824, (0 missing) ## HOMA &lt; 2.222398 to the left, improve=3.016650, (0 missing) ## Insulin &lt; 5.673 to the left, improve=2.666489, (0 missing) ## MCP.1 &lt; 297.372 to the left, improve=2.089658, (0 missing) ## Surrogate splits: ## HOMA &lt; 5.177161 to the left, agree=0.894, adj=0.5, (0 split) ## Insulin &lt; 19.055 to the left, agree=0.872, adj=0.4, (0 split) ## Leptin &lt; 86.37605 to the left, agree=0.830, adj=0.2, (0 split) ## MCP.1 &lt; 905.523 to the left, agree=0.830, adj=0.2, (0 split) ## Age &lt; 84 to the left, agree=0.809, adj=0.1, (0 split) ## ## Node number 7: 19 observations ## predicted class=Patients expected loss=0 P(node) =0.1637931 ## class counts: 0 19 ## probabilities: 0.000 1.000 ## ## Node number 10: 20 observations ## predicted class=Controls expected loss=0.15 P(node) =0.1724138 ## class counts: 17 3 ## probabilities: 0.850 0.150 ## ## Node number 11: 15 observations ## predicted class=Patients expected loss=0.2 P(node) =0.1293103 ## class counts: 3 12 ## probabilities: 0.200 0.800 ## ## Node number 12: 37 observations, complexity param=0.03846154 ## predicted class=Patients expected loss=0.4594595 P(node) =0.3189655 ## class counts: 17 20 ## probabilities: 0.459 0.541 ## left son=24 (10 obs) right son=25 (27 obs) ## Primary splits: ## Age &lt; 72.5 to the right, improve=3.334784, (0 missing) ## MCP.1 &lt; 297.372 to the left, improve=3.104932, (0 missing) ## BMI &lt; 32.275 to the right, improve=2.939143, (0 missing) ## Leptin &lt; 9.2319 to the left, improve=2.939143, (0 missing) ## Resistin &lt; 12.7652 to the left, improve=2.201398, (0 missing) ## Surrogate splits: ## Leptin &lt; 6.76555 to the left, agree=0.811, adj=0.3, (0 split) ## Insulin &lt; 17.356 to the right, agree=0.757, adj=0.1, (0 split) ## HOMA &lt; 4.164391 to the right, agree=0.757, adj=0.1, (0 split) ## MCP.1 &lt; 166.3975 to the left, agree=0.757, adj=0.1, (0 split) ## ## Node number 13: 10 observations ## predicted class=Patients expected loss=0 P(node) =0.0862069 ## class counts: 0 10 ## probabilities: 0.000 1.000 ## ## Node number 24: 10 observations ## predicted class=Controls expected loss=0.2 P(node) =0.0862069 ## class counts: 8 2 ## probabilities: 0.800 0.200 ## ## Node number 25: 27 observations, complexity param=0.03846154 ## predicted class=Patients expected loss=0.3333333 P(node) =0.2327586 ## class counts: 9 18 ## probabilities: 0.333 0.667 ## left son=50 (7 obs) right son=51 (20 obs) ## Primary splits: ## BMI &lt; 32.275 to the right, improve=5.860887, (0 missing) ## Leptin &lt; 15.02435 to the right, improve=2.744531, (0 missing) ## MCP.1 &lt; 292.096 to the left, improve=1.158823, (0 missing) ## Insulin &lt; 5.673 to the left, improve=0.979303, (0 missing) ## Glucose &lt; 103.5 to the left, improve=0.854835, (0 missing) ## Surrogate splits: ## Leptin &lt; 68.09345 to the right, agree=0.815, adj=0.286, (0 split) ## MCP.1 &lt; 832.433 to the right, agree=0.815, adj=0.286, (0 split) ## ## Node number 50: 7 observations ## predicted class=Controls expected loss=0.1428571 P(node) =0.06034483 ## class counts: 6 1 ## probabilities: 0.857 0.143 ## ## Node number 51: 20 observations ## predicted class=Patients expected loss=0.15 P(node) =0.1724138 ## class counts: 3 17 ## probabilities: 0.150 0.850 Summary Gini summary(tree.gini) ## Call: ## rpart(formula = Classification ~ ., data = dataset) ## n= 116 ## ## CP nsplit rel error xerror xstd ## 1 0.38461538 0 1.0000000 1.0000000 0.10300524 ## 2 0.10576923 1 0.6153846 0.7307692 0.09720906 ## 3 0.06730769 3 0.4038462 0.7307692 0.09720906 ## 4 0.01000000 5 0.2692308 0.7500000 0.09784651 ## ## Variable importance ## Glucose HOMA BMI Age Leptin Insulin ## 21 14 14 13 12 12 ## Resistin Adiponectin MCP.1 ## 7 4 4 ## ## Node number 1: 116 observations, complexity param=0.3846154 ## predicted class=Patients expected loss=0.4482759 P(node) =1 ## class counts: 52 64 ## probabilities: 0.448 0.552 ## left son=2 (50 obs) right son=3 (66 obs) ## Primary splits: ## Glucose &lt; 91.5 to the left, improve=11.136890, (0 missing) ## HOMA &lt; 2.169985 to the left, improve= 7.526679, (0 missing) ## Resistin &lt; 13.71318 to the left, improve= 6.668751, (0 missing) ## Age &lt; 37 to the left, improve= 6.601118, (0 missing) ## Insulin &lt; 10.285 to the left, improve= 5.651907, (0 missing) ## Surrogate splits: ## HOMA &lt; 1.231021 to the left, agree=0.698, adj=0.30, (0 split) ## Age &lt; 37 to the left, agree=0.664, adj=0.22, (0 split) ## Insulin &lt; 4.8075 to the left, agree=0.629, adj=0.14, (0 split) ## Adiponectin &lt; 22.12789 to the right, agree=0.612, adj=0.10, (0 split) ## MCP.1 &lt; 532.317 to the right, agree=0.612, adj=0.10, (0 split) ## ## Node number 2: 50 observations, complexity param=0.1057692 ## predicted class=Controls expected loss=0.3 P(node) =0.4310345 ## class counts: 35 15 ## probabilities: 0.700 0.300 ## left son=4 (29 obs) right son=5 (21 obs) ## Primary splits: ## Resistin &lt; 13.24805 to the left, improve=5.334975, (0 missing) ## Age &lt; 44.5 to the left, improve=3.857143, (0 missing) ## Insulin &lt; 3.4445 to the right, improve=2.951220, (0 missing) ## BMI &lt; 31.01992 to the right, improve=2.842105, (0 missing) ## HOMA &lt; 0.8390145 to the right, improve=1.882353, (0 missing) ## Surrogate splits: ## BMI &lt; 27.9125 to the left, agree=0.74, adj=0.381, (0 split) ## Insulin &lt; 6.3155 to the left, agree=0.70, adj=0.286, (0 split) ## HOMA &lt; 1.408116 to the left, agree=0.70, adj=0.286, (0 split) ## Leptin &lt; 29.01205 to the left, agree=0.70, adj=0.286, (0 split) ## MCP.1 &lt; 587.3165 to the left, agree=0.68, adj=0.238, (0 split) ## ## Node number 3: 66 observations, complexity param=0.06730769 ## predicted class=Patients expected loss=0.2575758 P(node) =0.5689655 ## class counts: 17 49 ## probabilities: 0.258 0.742 ## left son=6 (26 obs) right son=7 (40 obs) ## Primary splits: ## Age &lt; 65.5 to the right, improve=3.569347, (0 missing) ## Leptin &lt; 7.93315 to the left, improve=3.266637, (0 missing) ## Resistin &lt; 11.92665 to the left, improve=2.201071, (0 missing) ## BMI &lt; 32.48096 to the right, improve=2.187879, (0 missing) ## HOMA &lt; 2.173469 to the left, improve=2.183601, (0 missing) ## Surrogate splits: ## Leptin &lt; 47.14355 to the right, agree=0.697, adj=0.231, (0 split) ## MCP.1 &lt; 207.996 to the left, agree=0.682, adj=0.192, (0 split) ## Resistin &lt; 4.558425 to the left, agree=0.667, adj=0.154, (0 split) ## Glucose &lt; 107 to the right, agree=0.652, adj=0.115, (0 split) ## HOMA &lt; 17.95804 to the right, agree=0.636, adj=0.077, (0 split) ## ## Node number 4: 29 observations ## predicted class=Controls expected loss=0.1034483 P(node) =0.25 ## class counts: 26 3 ## probabilities: 0.897 0.103 ## ## Node number 5: 21 observations, complexity param=0.1057692 ## predicted class=Patients expected loss=0.4285714 P(node) =0.1810345 ## class counts: 9 12 ## probabilities: 0.429 0.571 ## left son=10 (10 obs) right son=11 (11 obs) ## Primary splits: ## BMI &lt; 30.0273 to the right, improve=8.485714, (0 missing) ## Leptin &lt; 35.16835 to the right, improve=3.857143, (0 missing) ## Age &lt; 45.5 to the left, improve=2.670330, (0 missing) ## HOMA &lt; 0.8695296 to the right, improve=1.714286, (0 missing) ## Insulin &lt; 5.2615 to the right, improve=1.341270, (0 missing) ## Surrogate splits: ## Age &lt; 40.5 to the left, agree=0.810, adj=0.6, (0 split) ## Leptin &lt; 28.041 to the right, agree=0.810, adj=0.6, (0 split) ## Insulin &lt; 3.6505 to the right, agree=0.714, adj=0.4, (0 split) ## HOMA &lt; 0.8695296 to the right, agree=0.714, adj=0.4, (0 split) ## Adiponectin &lt; 6.456392 to the left, agree=0.667, adj=0.3, (0 split) ## ## Node number 6: 26 observations, complexity param=0.06730769 ## predicted class=Patients expected loss=0.4615385 P(node) =0.2241379 ## class counts: 12 14 ## probabilities: 0.462 0.538 ## left son=12 (7 obs) right son=13 (19 obs) ## Primary splits: ## Glucose &lt; 98.5 to the left, improve=5.554656, (0 missing) ## HOMA &lt; 1.483263 to the left, improve=2.998265, (0 missing) ## Resistin &lt; 12.7652 to the left, improve=2.983683, (0 missing) ## Insulin &lt; 5.923 to the left, improve=1.923077, (0 missing) ## Leptin &lt; 16.5075 to the left, improve=1.923077, (0 missing) ## Surrogate splits: ## Insulin &lt; 5.533 to the left, agree=0.885, adj=0.571, (0 split) ## HOMA &lt; 1.100691 to the left, agree=0.885, adj=0.571, (0 split) ## Leptin &lt; 9.2716 to the left, agree=0.846, adj=0.429, (0 split) ## BMI &lt; 21.31248 to the left, agree=0.769, adj=0.143, (0 split) ## ## Node number 7: 40 observations ## predicted class=Patients expected loss=0.125 P(node) =0.3448276 ## class counts: 5 35 ## probabilities: 0.125 0.875 ## ## Node number 10: 10 observations ## predicted class=Controls expected loss=0.1 P(node) =0.0862069 ## class counts: 9 1 ## probabilities: 0.900 0.100 ## ## Node number 11: 11 observations ## predicted class=Patients expected loss=0 P(node) =0.09482759 ## class counts: 0 11 ## probabilities: 0.000 1.000 ## ## Node number 12: 7 observations ## predicted class=Controls expected loss=0 P(node) =0.06034483 ## class counts: 7 0 ## probabilities: 1.000 0.000 ## ## Node number 13: 19 observations ## predicted class=Patients expected loss=0.2631579 P(node) =0.1637931 ## class counts: 5 14 ## probabilities: 0.263 0.737 Plot par(mfrow=c(1,2)) plot(tree.information,main=&quot;Information&quot;) text(tree.information) plot(tree.gini, main=&quot;Gini&quot;) text(tree.gini) 4.3.2 Random Forest library(caret) library(pROC) Run algorithms using 3 times 10-fold cross validation # set definitions metric &lt;- &quot;ROC&quot; control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE, repeats=3) train Tree and RF set.seed(7) fit.cart.rcv &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.rf.rcv &lt;- train(Classification ~ ., data=dataset, method=&quot;rf&quot;, metric=metric, trControl=control) summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.rcv, rf=fit.rf.rcv) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, rf ## Number of resamples: 30 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.4428571 0.6297619 0.7166667 0.6827513 0.7482143 0.8571429 0 ## rf 0.6000000 0.7333333 0.8000000 0.8175132 0.9000000 1.0000000 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.4 0.5 0.6 0.6344444 0.8 1 0 ## rf 0.4 0.6 0.6 0.6577778 0.8 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.2857143 0.6666667 0.7142857 0.7341270 0.8333333 1 0 ## rf 0.3333333 0.6666667 0.8333333 0.7642857 0.8511905 1 0 ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$P, main=paste(&quot;3x10-fold-CV&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) Inspect models print(fit.cart.rcv) ## CART ## ## 116 samples ## 9 predictor ## 2 classes: &#39;Controls&#39;, &#39;Patients&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 105, 105, 104, 104, 103, 104, ... ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.06730769 0.6730556 0.5400000 0.7563492 ## 0.10576923 0.6827513 0.6344444 0.7341270 ## 0.38461538 0.5290476 0.1922222 0.8658730 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.1057692. #getModelInfo(fit.cart.rcv) #getModelInfo(fit.cart.rcv)$rpart getModelInfo(fit.cart.rcv)$rpart$parameters parameter class label cp numeric Complexity Parameter Inspect models print(fit.rf.rcv) ## Random Forest ## ## 116 samples ## 9 predictor ## 2 classes: &#39;Controls&#39;, &#39;Patients&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 105, 105, 104, 104, 103, 104, ... ## Resampling results across tuning parameters: ## ## mtry ROC Sens Spec ## 2 0.8012037 0.6455556 0.7603175 ## 5 0.8175132 0.6577778 0.7642857 ## 9 0.8128439 0.6955556 0.7714286 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 5. #getModelInfo(fit.rf.rcv) #getModelInfo(fit.rf.rcv)$rf getModelInfo(fit.rf.rcv)$rf$parameters parameter class label mtry numeric #Randomly Selected Predictors ROC complexity for models plot(fit.cart.rcv) plot(fit.rf.rcv) 4.3.3 Improve Random Forest myGrid&lt;-expand.grid(mtry=1:9) # numero de variaveis/colunas a considerar na RF set.seed(7) fit.rf.rcv.tune &lt;- train(Classification ~ ., data=dataset, method=&quot;rf&quot;, metric=metric, trControl=control, tuneGrid = myGrid) summarize accuracy of models fit.models&lt;-list(rpart=fit.cart.rcv,rf = fit.rf.rcv, rf.tune=fit.rf.rcv.tune) results&lt;-resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, rf, rf.tune ## Number of resamples: 30 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.4428571 0.6297619 0.7166667 0.6827513 0.7482143 0.8571429 0 ## rf 0.6000000 0.7333333 0.8000000 0.8175132 0.9000000 1.0000000 0 ## rf.tune 0.6285714 0.7333333 0.8452381 0.8229101 0.9226190 0.9857143 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.4 0.5 0.6 0.6344444 0.8 1 0 ## rf 0.4 0.6 0.6 0.6577778 0.8 1 0 ## rf.tune 0.4 0.6 0.6 0.6777778 0.8 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.2857143 0.6666667 0.7142857 0.7341270 0.8333333 1 0 ## rf 0.3333333 0.6666667 0.8333333 0.7642857 0.8511905 1 0 ## rf.tune 0.3333333 0.6666667 0.8333333 0.7706349 0.8571429 1 0 Roc curves for models par(mfrow=c(1,3)) rocs&lt;-lapply(fit.models, function(fit){plot.roc(fit$pred$obs, fit$pred$P, main=paste(&quot;3x10-fold CV-&quot;,fit$method),debug=F,print.auc=T)}) Compare accuracy of models dotplot(results) Inspect models getModelInfo(fit.rf.rcv.tune)$rf$parameters parameter class label mtry numeric #Randomly Selected Predictors "],
["assignment-trees.html", "4.4 Assignment Trees", " 4.4 Assignment Trees 2020-02-21 Compare a decision tree with a random forest in the Cervical Cancer (Risk Factors) data set (available from UCI repository), trying to accurately classify Dx.Cancer A possible solution using the data as it is: https://rpubs.com/balima78/learn_4031_assign1 some references: http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/141-cart-model-decision-tree-essentials https://bradleyboehmke.github.io/HOML/DT.html https://www.machinelearningplus.com/machine-learning/logistic-regression-tutorial-examples-r/ "],
["assignment-trees-up-sampling.html", "4.5 Assignment Trees - Up Sampling", " 4.5 Assignment Trees - Up Sampling Taking in consideration the fact that the data is unbalanced regarding the outcome variable, we try another solution: https://rpubs.com/balima78/learn_4032_assign1 some references: https://shiring.github.io/machine_learning/2017/04/02/unbalanced http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/141-cart-model-decision-tree-essentials https://bradleyboehmke.github.io/HOML/DT.html https://topepo.github.io/caret/subsampling-for-class-imbalances.html#subsampling-techniques https://www.machinelearningplus.com/machine-learning/logistic-regression-tutorial-examples-r/ "],
["bayes-network.html", "4.6 Bayes Network", " 4.6 Bayes Network 2020-03-03 4.6.1 Build network from data using naive bayes # get the library require(bnlearn) # read data from CSV file, telling R that the first line contains headers osa.data &lt;- read.csv(&quot;2.UploadedData/osa.csv&quot;, header=T) # check the data print(str(osa.data)) ## &#39;data.frame&#39;: 273 obs. of 7 variables: ## $ bmi : Factor w/ 2 levels &quot;NotObese&quot;,&quot;Obese&quot;: 2 2 2 2 2 2 2 1 1 2 ... ## $ ac : Factor w/ 2 levels &quot;Abnormal&quot;,&quot;Normal&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ nc : Factor w/ 2 levels &quot;Abnormal&quot;,&quot;Normal&quot;: 1 1 1 1 1 1 1 1 1 1 ... ## $ gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ wa : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 2 2 2 2 2 2 2 ... ## $ alcohol: Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 1 2 1 2 2 2 2 1 ... ## $ osa : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 2 2 2 1 2 1 1 2 1 2 ... ## NULL Learn a Bayes Network from the data using two naive bayes # naive Bayes osa.nb.net &lt;- naive.bayes(osa.data, training=&quot;osa&quot;) # osa is the outcome of interest # TAN osa.tan.net &lt;- tree.bayes(osa.data, training=&quot;osa&quot;) # osa is the outcome of interest # plot network par(mfrow=c(1,2)) plot(osa.nb.net, main = &quot;naive Bayes&quot;) plot(osa.tan.net, main = &quot;Tree-Augmented naive Bayes&quot;) split training and testing data sets # define training set set.seed(523) training &lt;- sample(1:nrow(osa.data), size=200) # define training set testing &lt;- (1:nrow(osa.data))[-training] fit the defined networks with the train data itself osa.nb.fit &lt;- bn.fit(osa.nb.net, osa.data[training,]) osa.tan.fit &lt;- bn.fit(osa.tan.net, osa.data[training,]) predict outcome for test data pred.nb &lt;- predict(osa.nb.fit, osa.data[testing,]) pred.tan &lt;- predict(osa.tan.fit, osa.data[testing,]) compute confusion matrix for each network table(pred.nb, osa.data$osa[testing]) pred.nb/ No Yes No 26 2 Yes 18 27 table(pred.tan, osa.data$osa[testing]) pred.tan/ No Yes No 29 3 Yes 15 26 4.6.2 Learn the structure of a Bayesian network using a hill-climbing (HC) or a Tabu search (TABU) greedy search # learn the BNs from the data using hill-climbing osa.hc.net &lt;- hc(osa.data, debug=F) # hill-climbing osa.tabu.net &lt;- tabu(osa.data, debug=F) # Tabu greedy search # plot network par(mfrow=c(1,2)) plot(osa.hc.net) plot(osa.tabu.net) describe the HC model print(osa.hc.net) ## ## Bayesian network learned via Score-based methods ## ## model: ## [bmi][ac|bmi][nc|bmi][gender|nc][wa|gender][alcohol|wa][osa|nc:wa] ## nodes: 7 ## arcs: 7 ## undirected arcs: 0 ## directed arcs: 7 ## average markov blanket size: 2.29 ## average neighbourhood size: 2.00 ## average branching factor: 1.00 ## ## learning algorithm: Hill-Climbing ## score: BIC (disc.) ## penalization coefficient: 2.804736 ## tests used in the learning procedure: 63 ## optimized: TRUE describe the TABU model print(osa.tabu.net) ## ## Bayesian network learned via Score-based methods ## ## model: ## [bmi][ac|bmi][nc|bmi][gender|nc][wa|gender][alcohol|wa][osa|nc:wa] ## nodes: 7 ## arcs: 7 ## undirected arcs: 0 ## directed arcs: 7 ## average markov blanket size: 2.29 ## average neighbourhood size: 2.00 ## average branching factor: 1.00 ## ## learning algorithm: Tabu Search ## score: BIC (disc.) ## penalization coefficient: 2.804736 ## tests used in the learning procedure: 177 ## optimized: TRUE after you have the grapfs, fit the networks with the data itself osa.hc.fit &lt;- bn.fit(osa.hc.net, osa.data) osa.tabu.fit &lt;- bn.fit(osa.tabu.net, osa.data) print the conditional probability tables for HC print(osa.hc.fit) ## ## Bayesian network parameters ## ## Parameters of node bmi (multinomial distribution) ## ## Conditional probability table: ## NotObese Obese ## 0.6153846 0.3846154 ## ## Parameters of node ac (multinomial distribution) ## ## Conditional probability table: ## ## bmi ## ac NotObese Obese ## Abnormal 0.8333333 1.0000000 ## Normal 0.1666667 0.0000000 ## ## Parameters of node nc (multinomial distribution) ## ## Conditional probability table: ## ## bmi ## nc NotObese Obese ## Abnormal 0.1964286 0.7904762 ## Normal 0.8035714 0.2095238 ## ## Parameters of node gender (multinomial distribution) ## ## Conditional probability table: ## ## nc ## gender Abnormal Normal ## Female 0.2931034 0.7452229 ## Male 0.7068966 0.2547771 ## ## Parameters of node wa (multinomial distribution) ## ## Conditional probability table: ## ## gender ## wa Female Male ## No 0.34437086 0.07377049 ## Yes 0.65562914 0.92622951 ## ## Parameters of node alcohol (multinomial distribution) ## ## Conditional probability table: ## ## wa ## alcohol No Yes ## No 0.7213115 0.2405660 ## Yes 0.2786885 0.7594340 ## ## Parameters of node osa (multinomial distribution) ## ## Conditional probability table: ## ## , , wa = No ## ## nc ## osa Abnormal Normal ## No 1.00000000 0.95555556 ## Yes 0.00000000 0.04444444 ## ## , , wa = Yes ## ## nc ## osa Abnormal Normal ## No 0.31000000 0.59821429 ## Yes 0.69000000 0.40178571 print the conditional probability tables for TABU print(osa.tabu.fit) ## ## Bayesian network parameters ## ## Parameters of node bmi (multinomial distribution) ## ## Conditional probability table: ## NotObese Obese ## 0.6153846 0.3846154 ## ## Parameters of node ac (multinomial distribution) ## ## Conditional probability table: ## ## bmi ## ac NotObese Obese ## Abnormal 0.8333333 1.0000000 ## Normal 0.1666667 0.0000000 ## ## Parameters of node nc (multinomial distribution) ## ## Conditional probability table: ## ## bmi ## nc NotObese Obese ## Abnormal 0.1964286 0.7904762 ## Normal 0.8035714 0.2095238 ## ## Parameters of node gender (multinomial distribution) ## ## Conditional probability table: ## ## nc ## gender Abnormal Normal ## Female 0.2931034 0.7452229 ## Male 0.7068966 0.2547771 ## ## Parameters of node wa (multinomial distribution) ## ## Conditional probability table: ## ## gender ## wa Female Male ## No 0.34437086 0.07377049 ## Yes 0.65562914 0.92622951 ## ## Parameters of node alcohol (multinomial distribution) ## ## Conditional probability table: ## ## wa ## alcohol No Yes ## No 0.7213115 0.2405660 ## Yes 0.2786885 0.7594340 ## ## Parameters of node osa (multinomial distribution) ## ## Conditional probability table: ## ## , , wa = No ## ## nc ## osa Abnormal Normal ## No 1.00000000 0.95555556 ## Yes 0.00000000 0.04444444 ## ## , , wa = Yes ## ## nc ## osa Abnormal Normal ## No 0.31000000 0.59821429 ## Yes 0.69000000 0.40178571 4.6.3 Fit existing network using data read network from file osa.manual.fit &lt;- read.net(&quot;2.UploadedData/osa_hc.net&quot;) transform the object (fit) into a unfitted network and plot it # transform the object (fit) into a unfitted network osa.manual.net &lt;- bn.net(osa.manual.fit) # plot network plot(osa.manual.net) fit the network with the data itself and save it as a .net file # fit the network with the data itself osa.manual.fit &lt;- bn.fit(osa.manual.net, osa.data) # save the network in a file to open later in SamIam write.net(osa.manual.fit, file=&quot;osa.manual.fitted.net&quot;) "],
["naive-bayes-network.html", "4.7 Naive Bayes Network", " 4.7 Naive Bayes Network 2020-03-10 All the code was given by the professor and is available from here: https://rpubs.com/balima78/learn_4050_lesson5 "],
["ensemble-classifiers.html", "4.8 Ensemble classifiers", " 4.8 Ensemble classifiers 2020-03-24 Build three ensemble classifiers for the Breast Cancer Coimbra data set (available from UCI repository) using the ‘caret’ package, using: Bagging (treebag and rf) Boosting (C5.0 and gdm) Stacking (at least three models) https://rpubs.com/balima78/learn_4060_exercise "],
["machine-learning-from-data-streams.html", "4.9 Machine Learning from Data Streams", " 4.9 Machine Learning from Data Streams 2020-04-21 4.9.1 Repositories R packages for machine learning in data streams Python/R connectable software for machine learning in data streams 4.9.2 Prequential evaluation method presented in Gama, Sebastiao &amp; Rodrigues (2013) Evaluation of Online Learning Algorithms and Concept Drift Detection pseudocodes on the article to R: 4.9.2.1 Update prequential Error estimator itera &lt;- 2000 Pe&lt;-NULL plot(NULL,xlab = &quot;examples&quot;, ylab = &quot;Prequential error&quot;, xlim = c(0,itera), ylim = c(-1,1), main = &quot;Algorithm 1: Prequential Error Estimator&quot;) for (i in 2:itera){ ## Loss at example i ## ei &lt;- ifelse(i&lt;=1000, rbinom(1, 1,.3), rbinom(1, 1,.8)) Pe[1]&lt;-0 Pe[i]&lt;-(ei + (i-1)*Pe[i-1]) / i points(x=i,y=Pe[i],pch=20,cex=.1,col=&quot;black&quot;) } 4.9.2.2 Update Prequential Error Estimator in sliding window w&lt;-50 itera &lt;- 2000 Pw&lt;-NULL E&lt;-NULL plot(NULL,xlab = &quot;examples&quot;, ylab = &quot;Prequential error&quot;, xlim = c(0,itera), ylim = c(-1,1), main = &quot;Algorithm 2: sliding window&quot;) for (i in 1:itera){ ## Loss at example i ## ei &lt;- ifelse(i&lt;=1000, rbinom(1, 1,.3), rbinom(1, 1,.8)) S&lt;-0 E[1:w]&lt;-0 p&lt;-((i-1) %% w) + 1 S&lt;-S - E[p] + ei Pw[i]=S/min(w,i) points(x=i,y=Pw[i],pch=20,cex=.1,col=&quot;black&quot;) } 4.9.2.3 Update rule for Prequential error estimator using fading factors a &lt;- 0.999 itera &lt;- 2000 Pa&lt;-NULL Sa&lt;-NULL Na&lt;-NULL plot(NULL,xlab = &quot;examples&quot;, ylab = &quot;Prequential error&quot;, xlim = c(0,itera), ylim = c(-1,1), main = &quot;Algorithm 3: using fading factors&quot;) for (i in 2:itera){ ## Loss at example i ## ei &lt;- ifelse(i&lt;=1000, rbinom(1, 1,.3), rbinom(1, 1,.8)) Sa[1]&lt;-0 Na[1]&lt;-0 Pa[1]&lt;-0 Sa[i]&lt;-ei + a*Sa[i-1] Na[i]&lt;-1 + a*Na[i-1] Pa[i]&lt;-Sa[i]/Na[i] points(x=i,y=Pa[i],pch=20,cex=.1,col=&quot;black&quot;) } 4.9.2.4 Drift detector based on the ratio of two fading factors # required values itera&lt;- 2000 a1&lt;-0.999 a2&lt;-0.9 g&lt;-0.1 l&lt;-10 drift&lt;-logical() # Initialize the error estimators Sa1&lt;-NULL Na1&lt;-NULL Sa2&lt;-NULL Na2&lt;-NULL SR&lt;-NULL mT&lt;-NULL MT&lt;-NULL R&lt;-NULL plot(NULL,xlab = &quot;examples&quot;, ylab = &quot;Drift&quot;, xlim = c(0,itera), ylim = c(-1,1), main = &quot;Algorithm 4: Drift detector&quot;) for (i in 2:itera){ ## Loss at example i ## ei &lt;- ifelse(i&lt;=1000, rbinom(1, 1,.3), rbinom(1, 1,.8)) R[1]&lt;-0 Sa1[1]&lt;-1 Sa1[i]&lt;-ei + a1*Sa1[i-1] Na1[1]&lt;-0 Na1[i]&lt;-1+a1*Na1[i-1] Ma1&lt;-Sa1[i]/Na1[i] Sa2[1]&lt;-0 Sa2[i]&lt;-ei + a2*Sa2[i-1] Na2[1]&lt;-0 Na2[i]&lt;-1+a2*Na2[i-1] Ma2&lt;-Sa2[i]/Na2[i] R[i]&lt;-Ma2/Ma1 ## PH test ## SR[1]&lt;-0 SR[i]&lt;-SR[i-1] + R[i] mT[1]&lt;-0 mT[i]&lt;-mT[i-1] + R[i] - SR[i]/i - g MT&lt;-min(c(MT,mT[i])) if(mT[i]-MT &gt;= l) {drift = TRUE} else {drift = FALSE} points(x=i,y=drift,pch=20,cex=.1,col=&quot;black&quot;) } "],
["unsupervised-machine-learning.html", "4.10 Unsupervised Machine Learning", " 4.10 Unsupervised Machine Learning 2020-05-05 4.10.1 Clustering and Association Rules Consider the the Cervical Cancer (Risk Factors) data set (available from UCI repository) and exploit the data using association rules (for binary variables) and clustering (for continuous variables). Read data 4.10.1.1 Association rules str(datacat_complete) ## &#39;data.frame&#39;: 726 obs. of 21 variables: ## $ Smokes : logi FALSE FALSE FALSE TRUE FALSE FALSE ... ## $ Hormonal.Contraceptives : logi FALSE FALSE FALSE TRUE TRUE FALSE ... ## $ IUD : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ STDs.condylomatosis : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ STDs.cervical.condylomatosis : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ STDs.vulvo.perineal.condylomatosis: logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ STDs.syphilis : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ STDs.pelvic.inflammatory.disease : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ STDs.genital.herpes : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ STDs.molluscum.contagiosum : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ STDs.AIDS : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ STDs.HIV : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ STDs.Hepatitis.B : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ STDs.HPV : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ Dx.Cancer : logi FALSE FALSE FALSE TRUE FALSE FALSE ... ## $ Dx.CIN : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ Dx.HPV : logi FALSE FALSE FALSE TRUE FALSE FALSE ... ## $ Hinselmann : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ Schiller : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ Citology : logi FALSE FALSE FALSE FALSE FALSE FALSE ... ## $ Biopsy : logi FALSE FALSE FALSE FALSE FALSE FALSE ... Coerce Item List to the Transactions class ## [1] &quot;transactions&quot; ## attr(,&quot;package&quot;) ## [1] &quot;arules&quot; ## transactions as itemMatrix in sparse format with ## 726 rows (elements/itemsets/transactions) and ## 19 columns (items) and a density of 0.07206032 ## ## most frequent items: ## Hormonal.Contraceptives Smokes IUD ## 466 104 81 ## Schiller Biopsy (Other) ## 69 50 224 ## ## element (itemset/transaction) length distribution: ## sizes ## 0 1 2 3 4 5 6 7 ## 164 342 113 47 32 15 9 4 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 0.000 1.000 1.000 1.369 2.000 7.000 ## ## includes extended item information - examples: ## labels variables levels ## 1 Smokes Smokes TRUE ## 2 Hormonal.Contraceptives Hormonal.Contraceptives TRUE ## 3 IUD IUD TRUE ## ## includes extended transaction information - examples: ## transactionID ## 1 1 ## 2 2 ## 3 3 ## items transactionID ## [1] {} 1 ## [2] {} 2 ## [3] {} 3 ## [4] {Smokes,Hormonal.Contraceptives,Dx.Cancer,Dx.HPV} 4 ## [5] {Hormonal.Contraceptives} 5 ## [6] {} 6 ## [7] {Smokes,IUD,Hinselmann,Schiller,Biopsy} 7 ## [8] {Hormonal.Contraceptives,IUD} 8 ## [9] {Dx.Cancer,Dx.HPV} 9 ## [10] {Hormonal.Contraceptives} 11 plot more frequent events applying Apriori ## Apriori ## ## Parameter specification: ## confidence minval smax arem aval originalSupport maxtime support minlen ## 0.1 0.1 1 none FALSE TRUE 5 0.01 1 ## maxlen target ext ## 10 rules TRUE ## ## Algorithmic control: ## filter tree heap memopt load sort verbose ## 0.1 TRUE TRUE FALSE TRUE 2 TRUE ## ## Absolute minimum support count: 7 ## ## set item appearances ...[0 item(s)] done [0.00s]. ## set transactions ...[19 item(s), 726 transaction(s)] done [0.00s]. ## sorting and recoding items ... [13 item(s)] done [0.00s]. ## creating transaction tree ... done [0.00s]. ## checking subsets of size 1 2 3 4 done [0.00s]. ## writing ... [110 rule(s)] done [0.00s]. ## creating S4 object ... done [0.00s]. ## set of 110 rules ## ## rule length distribution (lhs + rhs):sizes ## 1 2 3 4 ## 3 45 54 8 ## ## Min. 1st Qu. Median Mean 3rd Qu. Max. ## 1.000 2.000 3.000 2.609 3.000 4.000 ## ## summary of quality measures: ## support confidence coverage lift ## Min. :0.01102 Min. :0.1111 Min. :0.01102 Min. : 0.8309 ## 1st Qu.:0.01377 1st Qu.:0.2564 1st Qu.:0.02514 1st Qu.: 1.2595 ## Median :0.01791 Median :0.6283 Median :0.05234 Median : 3.6341 ## Mean :0.03168 Mean :0.5591 Mean :0.08620 Mean : 7.1996 ## 3rd Qu.:0.03168 3rd Qu.:0.7681 3rd Qu.:0.06680 3rd Qu.:10.1200 ## Max. :0.64187 Max. :1.0000 Max. :1.00000 Max. :41.5938 ## count ## Min. : 8 ## 1st Qu.: 10 ## Median : 13 ## Mean : 23 ## 3rd Qu.: 23 ## Max. :466 ## ## mining info: ## data ntransactions support confidence ## datacat_complete 726 0.01 0.1 see the first 10 resulted rules ordered by lift and support ## lhs rhs support confidence coverage lift count ## [1] {Hormonal.Contraceptives, ## Dx.Cancer} =&gt; {Dx.HPV} 0.01515152 0.9166667 0.01652893 41.59375 11 ## [2] {Dx.HPV} =&gt; {Dx.Cancer} 0.02066116 0.9375000 0.02203857 40.03676 15 ## [3] {Dx.Cancer} =&gt; {Dx.HPV} 0.02066116 0.8823529 0.02341598 40.03676 15 ## [4] {Hormonal.Contraceptives, ## Dx.HPV} =&gt; {Dx.Cancer} 0.01515152 0.9166667 0.01652893 39.14706 11 ## [5] {Smokes, ## STDs.condylomatosis} =&gt; {STDs.vulvo.perineal.condylomatosis} 0.01377410 1.0000000 0.01377410 19.10526 10 ## [6] {STDs.condylomatosis, ## Schiller} =&gt; {STDs.vulvo.perineal.condylomatosis} 0.01377410 1.0000000 0.01377410 19.10526 10 ## [7] {STDs.vulvo.perineal.condylomatosis} =&gt; {STDs.condylomatosis} 0.05234160 1.0000000 0.05234160 18.61538 38 ## [8] {Hormonal.Contraceptives, ## STDs.vulvo.perineal.condylomatosis} =&gt; {STDs.condylomatosis} 0.03305785 1.0000000 0.03305785 18.61538 24 ## [9] {Smokes, ## STDs.vulvo.perineal.condylomatosis} =&gt; {STDs.condylomatosis} 0.01377410 1.0000000 0.01377410 18.61538 10 ## [10] {STDs.vulvo.perineal.condylomatosis, ## Schiller} =&gt; {STDs.condylomatosis} 0.01377410 1.0000000 0.01377410 18.61538 10 see the first 10 resulted rules ordered by support and lift ## lhs rhs support confidence coverage lift count ## [1] {Biopsy} =&gt; {Schiller} 0.06060606 0.8800000 0.06887052 9.259130 44 ## [2] {Schiller} =&gt; {Biopsy} 0.06060606 0.6376812 0.09504132 9.259130 44 ## [3] {STDs.vulvo.perineal.condylomatosis} =&gt; {STDs.condylomatosis} 0.05234160 1.0000000 0.05234160 18.615385 38 ## [4] {STDs.condylomatosis} =&gt; {STDs.vulvo.perineal.condylomatosis} 0.05234160 0.9743590 0.05371901 18.615385 38 ## [5] {Schiller} =&gt; {Hinselmann} 0.04407713 0.4637681 0.09504132 10.202899 32 ## [6] {Hinselmann} =&gt; {Schiller} 0.04407713 0.9696970 0.04545455 10.202899 32 ## [7] {Hormonal.Contraceptives, ## Schiller} =&gt; {Biopsy} 0.03856749 0.6363636 0.06060606 9.240000 28 ## [8] {Hormonal.Contraceptives, ## Biopsy} =&gt; {Schiller} 0.03856749 0.8484848 0.04545455 8.927536 28 ## [9] {Hormonal.Contraceptives, ## STDs.vulvo.perineal.condylomatosis} =&gt; {STDs.condylomatosis} 0.03305785 1.0000000 0.03305785 18.615385 24 ## [10] {Hormonal.Contraceptives, ## STDs.condylomatosis} =&gt; {STDs.vulvo.perineal.condylomatosis} 0.03305785 0.9600000 0.03443526 18.341053 24 see the results for Dx.HPV and Dx.Cancer ## lhs rhs support confidence ## [1] {Dx.HPV} =&gt; {Dx.Cancer} 0.02066116 0.9375000 ## [2] {Dx.Cancer} =&gt; {Dx.HPV} 0.02066116 0.8823529 ## [3] {Hormonal.Contraceptives,Dx.HPV} =&gt; {Dx.Cancer} 0.01515152 0.9166667 ## [4] {Hormonal.Contraceptives,Dx.Cancer} =&gt; {Dx.HPV} 0.01515152 0.9166667 ## coverage lift count ## [1] 0.02203857 40.03676 15 ## [2] 0.02341598 40.03676 15 ## [3] 0.01652893 39.14706 11 ## [4] 0.01652893 41.59375 11 Visualize 15 results by lift and support values graph plot parallel coordenates 4.10.1.2 K-means ## &#39;data.frame&#39;: 668 obs. of 10 variables: ## $ Age : int 18 15 52 46 42 51 26 45 44 27 ... ## $ Number.of.sexual.partners : num 4 1 5 3 3 3 1 1 3 1 ... ## $ First.sexual.intercourse : num 15 14 16 21 23 17 26 20 26 17 ... ## $ Num.of.pregnancies : num 1 1 4 4 2 6 3 5 4 3 ... ## $ Smokes..years. : num 0 0 37 0 0 34 0 0 0 0 ... ## $ Smokes..packs.year. : num 0 0 37 0 0 3.4 0 0 0 0 ... ## $ Hormonal.Contraceptives..years.: num 0 0 3 15 0 0 2 0 2 8 ... ## $ IUD..years. : num 0 0 0 0 0 7 7 0 0 0 ... ## $ STDs..number. : num 0 0 0 0 0 0 0 0 0 0 ... ## $ STDs..Number.of.diagnosis : int 0 0 0 0 0 0 0 0 0 0 ... Determine number of clusters within groups sum of squares A plot of the within groups sum of squares by number of clusters extracted K-Means Cluster Analysis compactness of the clustering (between_SS / total_SS) ## [1] 48.70424 get cluster means Group.1 Age Number.of.sexual.partners First.sexual.intercourse Num.of.pregnancies Smokes..years. Smokes..packs.year. Hormonal.Contraceptives..years. IUD..years. STDs..number. STDs..Number.of.diagnosis 1 20.28797 2.351266 16.00949 1.642405 0.570680 0.2395741 1.156772 0.1170886 0.1740506 0.0917722 2 42.66667 2.655172 18.47126 3.770115 3.856712 1.5365104 5.645402 1.3188506 0.2183908 0.1149425 3 30.52830 2.686793 18.05660 2.660377 1.167777 0.3667880 2.539828 0.7634717 0.1396226 0.0867925 plot clusters for visualize clusters Compute hierarchical k-means clustering ## [1] &quot;cluster&quot; &quot;centers&quot; &quot;totss&quot; &quot;withinss&quot; &quot;tot.withinss&quot; ## [6] &quot;betweenss&quot; &quot;size&quot; &quot;iter&quot; &quot;ifault&quot; &quot;data&quot; ## [11] &quot;hclust&quot; Visualize the tree alternative hierarchical clustering References http://www.rdatamining.com/examples/association-rules https://blog.aptitive.com/building-the-transactions-class-for-association-rule-mining-in-r-using-arules-and-apriori-c6be64268bc4 https://rpubs.com/lingyanzhou/examples-association-rules https://www.statmethods.net/advstats/cluster.html https://towardsdatascience.com/clustering-analysis-in-r-using-k-means-73eca4fb7967 https://rpubs.com/JanpuHou/278558 "],
["support-vector-machine.html", "4.11 Support Vector Machine", " 4.11 Support Vector Machine 2020-05-12 Read data and packages ### Packages # Load package &#39;caret&#39; library(caret) # Load package &#39;ellipse&#39; library(ellipse) # Load package &#39;e1071&#39; library(&quot;e1071&quot;) # Load package &#39;pROC&#39; library(pROC) ### Data data(iris) attach(iris) x &lt;- subset(iris, select=-Species) y &lt;- Species plot correlation between features classified by the outcome par(mfrow=c(1,1)) featurePlot(x=x, y=y, plot=&quot;ellipse&quot;) Identify species according to Petals width and length plot(x=Petal.Length, y=Petal.Width, col=Species, pch=20, axes=F, main=&quot;Iris Data Set&quot;, ylab=&quot;Petal Width&quot;, xlab=&quot;Petal Length&quot;) legend(x=4, y=0.5, cex=.8, legend=levels(Species), col=1:3, pch=20) axis(1) axis(2) 4.11.1 SVM classification 4.11.1.1 Linear - 2 features svm.iris.linear &lt;- svm(Species ~ Petal.Width + Petal.Length, data=iris, kernel=&quot;linear&quot;) summary(svm.iris.linear) ## ## Call: ## svm(formula = Species ~ Petal.Width + Petal.Length, data = iris, ## kernel = &quot;linear&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 1 ## ## Number of Support Vectors: 31 ## ## ( 3 15 13 ) ## ## ## Number of Classes: 3 ## ## Levels: ## setosa versicolor virginica accuracy pred.linear &lt;- predict(svm.iris.linear,x) print(table(pred.linear,y)) ## y ## pred.linear setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 47 4 ## virginica 0 3 46 classification plot plot(svm.iris.linear, data = iris, Petal.Width ~ Petal.Length, svSymbol = 17, dataSymbol = 20, symbolPalette = c(3,2,4), cex=2, color.palette=grey.colors) 4.11.1.2 Linear - 4 features svm.iris.linear.all &lt;- svm(Species ~ ., data=iris, kernel=&quot;linear&quot;) summary(svm.iris.linear.all) ## ## Call: ## svm(formula = Species ~ ., data = iris, kernel = &quot;linear&quot;) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: linear ## cost: 1 ## ## Number of Support Vectors: 29 ## ## ( 2 15 12 ) ## ## ## Number of Classes: 3 ## ## Levels: ## setosa versicolor virginica accuracy pred.all &lt;- predict(svm.iris.linear.all,x) print(table(pred.all,y)) ## y ## pred.all setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 46 1 ## virginica 0 4 49 classification plot plot(svm.iris.linear.all, data = iris, Petal.Width ~ Petal.Length, # para ver em 2D é preciso escolher duas variaveis svSymbol = 17, dataSymbol = 20, symbolPalette = c(3,2,4), cex=2, color.palette=grey.colors, slice = list(Sepal.Width = 3, Sepal.Length = 6.0)) # fixam-se as duas variaveis que não estão disponiveis 4.11.1.3 Radial - 2 features svm.iris &lt;- svm(Species ~ Petal.Width + Petal.Length, data=iris) summary(svm.iris) ## ## Call: ## svm(formula = Species ~ Petal.Width + Petal.Length, data = iris) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 1 ## ## Number of Support Vectors: 37 ## ## ( 5 16 16 ) ## ## ## Number of Classes: 3 ## ## Levels: ## setosa versicolor virginica accuracy pred &lt;- predict(svm.iris,x) print(table(pred,y)) ## y ## pred setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 47 3 ## virginica 0 3 47 classification plot plot(svm.iris, data = iris, Petal.Width ~ Petal.Length, svSymbol = 17, dataSymbol = 20, symbolPalette = c(3,2,4), cex=2, color.palette=grey.colors) 4.11.1.4 Radial - 4 features svm.iris.all &lt;- svm(Species ~ ., data=iris) summary(svm.iris.all) ## ## Call: ## svm(formula = Species ~ ., data = iris) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 1 ## ## Number of Support Vectors: 51 ## ## ( 8 22 21 ) ## ## ## Number of Classes: 3 ## ## Levels: ## setosa versicolor virginica accuracy pred.all &lt;- predict(svm.iris.all,x) print(table(pred.all,y)) ## y ## pred.all setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 48 2 ## virginica 0 2 48 classification plot plot(svm.iris.all, data = iris, Petal.Width ~ Petal.Length, # para ver em 2D seleccionam-se duas variaveis svSymbol = 17, dataSymbol = 20, symbolPalette = c(3,2,4), cex=2, color.palette=grey.colors, slice = list(Sepal.Width = 2.5, Sepal.Length = 3.5)) # as restantes variaveis são fixas para determinados valores 4.11.1.5 Tuning SVM parameters set.seed(523) svm.tune &lt;- tune(svm, train.x=x, train.y=y, kernel=&quot;radial&quot;, ranges=list(cost=10^(-1:2), gamma=c(.5,1,2))) print(svm.tune) ## ## Parameter tuning of &#39;svm&#39;: ## ## - sampling method: 10-fold cross validation ## ## - best parameters: ## cost gamma ## 1 0.5 ## ## - best performance: 0.04 with the best parameters (cost, gamma) fit a SVM svm.iris.tuned &lt;- svm(Species ~ ., data=iris, kernel=&quot;radial&quot;, cost=1, gamma=0.5) summary(svm.iris.tuned) ## ## Call: ## svm(formula = Species ~ ., data = iris, kernel = &quot;radial&quot;, cost = 1, ## gamma = 0.5) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 1 ## ## Number of Support Vectors: 59 ## ## ( 11 23 25 ) ## ## ## Number of Classes: 3 ## ## Levels: ## setosa versicolor virginica accuracy pred.tuned &lt;- predict(svm.iris.tuned,x) print(table(pred.tuned,y)) ## y ## pred.tuned setosa versicolor virginica ## setosa 50 0 0 ## versicolor 0 48 2 ## virginica 0 2 48 "],
["assignment-svm.html", "4.12 Assignment SVM", " 4.12 Assignment SVM 2020-05-12 Read data and packages library(caret) library(pROC) # Get data set from UCI repository - Breast Cancer Coimbra # - available from https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra # Define the filename filename &lt;- &quot;2.UploadedData/dataR2.csv&quot; # Load the CSV file from the local directory dataset &lt;- read.csv(filename, header=T) # Define Classification as factor dataset$Classification &lt;- factor(dataset$Classification, levels=1:2, labels=c(&quot;Control&quot;, &quot;Patient&quot;)) Holdout a validation set, by defining the indices of the training set set.seed(523) training.index &lt;- createDataPartition(dataset$Classification, p=0.8, list=FALSE) validation &lt;- dataset[-training.index,] dataset &lt;- dataset[training.index,] Levels of the class levels(dataset$Classification) ## [1] &quot;Control&quot; &quot;Patient&quot; Class distribution proportions &lt;- prop.table(table(dataset$Classification)) cbind(Frequency=table(dataset$Classification), Proportion=round(proportions*100,2)) Frequency Proportion Control 42 44.68 Patient 52 55.32 Statistical Summary summary(dataset) Age BMI Glucose Insulin HOMA Leptin Adiponectin Resistin MCP.1 Classification Min. :25.00 Min. :18.37 Min. : 60.00 Min. : 2.540 Min. : 0.5192 Min. : 4.311 Min. : 1.656 Min. : 3.21 Min. : 45.84 Control:42 1st Qu.:45.25 1st Qu.:23.00 1st Qu.: 85.00 1st Qu.: 4.389 1st Qu.: 0.9705 1st Qu.:12.555 1st Qu.: 5.468 1st Qu.: 7.11 1st Qu.: 271.35 Patient:52 Median :60.50 Median :28.02 Median : 92.00 Median : 6.121 Median : 1.4282 Median :22.396 Median : 8.353 Median :11.23 Median : 477.90 NA Mean :59.01 Mean :27.95 Mean : 98.65 Mean :10.623 Mean : 2.9147 Mean :27.994 Mean :10.153 Mean :15.13 Mean : 554.98 NA 3rd Qu.:71.75 3rd Qu.:31.25 3rd Qu.:102.00 3rd Qu.:12.451 3rd Qu.: 2.9926 3rd Qu.:39.788 3rd Qu.:11.820 3rd Qu.:17.35 3rd Qu.: 728.66 NA Max. :89.00 Max. :38.58 Max. :201.00 Max. :58.460 Max. :25.0503 Max. :90.280 Max. :38.040 Max. :82.10 Max. :1698.44 NA Learn SVM with radial kernel (all features) # Load package &#39;e1071&#39; library(&quot;e1071&quot;) svm.all &lt;- svm(Classification ~ ., data=dataset) Summary summary(svm.all) ## ## Call: ## svm(formula = Classification ~ ., data = dataset) ## ## ## Parameters: ## SVM-Type: C-classification ## SVM-Kernel: radial ## cost: 1 ## ## Number of Support Vectors: 78 ## ## ( 36 42 ) ## ## ## Number of Classes: 2 ## ## Levels: ## Control Patient classification plot for Glucose and Age plot(svm.all, data = dataset, Glucose ~ Age, # para ver em 2D é necessário selecionar duas variaveis svSymbol = 17, dataSymbol = 20, symbolPalette = c(2,3), color.palette=grey.colors, slice = list( # Age = 59.01, BMI = 27.95, # as restantes variaveis são fixadas para os seus valores médios # Glucose=98.65, Insulin=10.623, # mean value HOMA=2.9147, # mean value Leptin=27.994, # mean value Adiponectin=10.153, # mean value Resistin=15.13, # mean value MCP.1=554.98)) # mean value Run algorithms using 3 times 10-fold cross validation metric &lt;- &quot;ROC&quot; control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE, repeats = 3) set.seed(7) fit.svm.linear &lt;- train(Classification ~ ., data=dataset, method=&quot;svmLinear&quot;, metric=metric, trControl=control, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 10) set.seed(7) fit.svm.radial &lt;- train(Classification ~ ., data=dataset, method=&quot;svmRadial&quot;, metric=metric, trControl=control, preProcess = c(&quot;center&quot;, &quot;scale&quot;), tuneLength = 10) Summarize accuracy of models fit.models &lt;- list(linear=fit.svm.linear, radial=fit.svm.radial) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: linear, radial ## Number of resamples: 30 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## linear 0.4 0.6500000 0.7758333 0.7657778 0.8875 1 0 ## radial 0.5 0.8083333 0.8733333 0.8472222 0.9125 1 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## linear 0.0 0.6 0.750 0.700 0.75 1 0 ## radial 0.2 0.5 0.675 0.685 0.95 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## linear 0.0 0.6000000 0.8 0.7166667 0.8333333 1 0 ## radial 0.2 0.6666667 0.8 0.7622222 0.8333333 1 0 ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;3 x 10-fold CV -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) Inspect model linear print(fit.svm.linear) ## Support Vector Machines with Linear Kernel ## ## 94 samples ## 9 predictor ## 2 classes: &#39;Control&#39;, &#39;Patient&#39; ## ## Pre-processing: centered (9), scaled (9) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 85, 85, 83, 84, 84, 85, ... ## Resampling results: ## ## ROC Sens Spec ## 0.7657778 0.7 0.7166667 ## ## Tuning parameter &#39;C&#39; was held constant at a value of 1 getModelInfo(fit.svm.linear)$svmLinear$parameters parameter class label C numeric Cost Inspect model radial print(fit.svm.radial) ## Support Vector Machines with Radial Basis Function Kernel ## ## 94 samples ## 9 predictor ## 2 classes: &#39;Control&#39;, &#39;Patient&#39; ## ## Pre-processing: centered (9), scaled (9) ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 85, 85, 83, 84, 84, 85, ... ## Resampling results across tuning parameters: ## ## C ROC Sens Spec ## 0.25 0.7908333 0.7533333 0.6133333 ## 0.50 0.8015000 0.6650000 0.7088889 ## 1.00 0.8098889 0.6500000 0.7500000 ## 2.00 0.8458333 0.6766667 0.7544444 ## 4.00 0.8472222 0.6850000 0.7622222 ## 8.00 0.8148889 0.6500000 0.7433333 ## 16.00 0.8002222 0.6516667 0.7588889 ## 32.00 0.8057222 0.6466667 0.7700000 ## 64.00 0.7797778 0.6283333 0.7488889 ## 128.00 0.7604444 0.6366667 0.7377778 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.09382649 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 0.09382649 and C = 4. getModelInfo(fit.svm.radial)$svmRadial$parameters parameter class label sigma numeric Sigma C numeric Cost ROC complexity for models plot(fit.svm.radial) Improve Radial myGrid &lt;- expand.grid(C = c(1,2,4,8), sigma=0.09382649) set.seed(7) fit.svm.radial.tune &lt;- train(Classification ~ ., data=dataset, method=&quot;svmRadial&quot;, metric=metric, trControl=control, tuneGrid = myGrid) # ROC complexity for models print(fit.svm.radial.tune) ## Support Vector Machines with Radial Basis Function Kernel ## ## 94 samples ## 9 predictor ## 2 classes: &#39;Control&#39;, &#39;Patient&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 85, 85, 83, 84, 84, 85, ... ## Resampling results across tuning parameters: ## ## C ROC Sens Spec ## 1 0.8098889 0.6483333 0.7366667 ## 2 0.8458333 0.6950000 0.7677778 ## 4 0.8472222 0.6900000 0.7688889 ## 8 0.8148889 0.6583333 0.7366667 ## ## Tuning parameter &#39;sigma&#39; was held constant at a value of 0.09382649 ## ROC was used to select the optimal model using the largest value. ## The final values used for the model were sigma = 0.09382649 and C = 4. Summarize accuracy of models fit.models &lt;- list(linear=fit.svm.linear, radial=fit.svm.radial, tune=fit.svm.radial.tune) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: linear, radial, tune ## Number of resamples: 30 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## linear 0.4 0.6500000 0.7758333 0.7657778 0.8875 1 0 ## radial 0.5 0.8083333 0.8733333 0.8472222 0.9125 1 0 ## tune 0.5 0.8083333 0.8733333 0.8472222 0.9125 1 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## linear 0.00 0.6 0.750 0.700 0.75 1 0 ## radial 0.20 0.5 0.675 0.685 0.95 1 0 ## tune 0.25 0.5 0.750 0.690 0.80 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## linear 0.0 0.6000000 0.8 0.7166667 0.8333333 1 0 ## radial 0.2 0.6666667 0.8 0.7622222 0.8333333 1 0 ## tune 0.2 0.6166667 0.8 0.7688889 1.0000000 1 0 ROC curves for models par(mfrow=c(1,3)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;3 x 10-fold CV -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) Make predictions par(mfrow=c(1,1)) # Estimate skill of RF on the validation dataset predictions.prob &lt;- predict(fit.svm.radial.tune, validation, type=&quot;prob&quot;) predictions &lt;- predict(fit.svm.radial.tune, validation, type=&quot;raw&quot;) confusionMatrix(predictions, validation$Classification) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Control Patient ## Control 7 1 ## Patient 3 11 ## ## Accuracy : 0.8182 ## 95% CI : (0.5972, 0.9481) ## No Information Rate : 0.5455 ## P-Value [Acc &gt; NIR] : 0.007436 ## ## Kappa : 0.6271 ## ## Mcnemar&#39;s Test P-Value : 0.617075 ## ## Sensitivity : 0.7000 ## Specificity : 0.9167 ## Pos Pred Value : 0.8750 ## Neg Pred Value : 0.7857 ## Prevalence : 0.4545 ## Detection Rate : 0.3182 ## Detection Prevalence : 0.3636 ## Balanced Accuracy : 0.8083 ## ## &#39;Positive&#39; Class : Control ## plot ROC plot.roc(validation$Classification, predictions.prob$Patient, print.auc=T, axes=F, main=paste(&quot;3 x 10-fold CV -&quot;,fit.svm.radial.tune$method), debug=F) axis(1) axis(2) "],
["text-mining.html", "4.13 Text mining", " 4.13 Text mining 2020-05-19 all code from the professor Loading data library(&quot;tm&quot;) library(&quot;SnowballC&quot;) library(&quot;readr&quot;) library(&quot;dplyr&quot;) t1 &lt;- read_csv(&quot;2.UploadedData/all_sources_metadata_2020-03-13.csv&quot;) glimpse(t1) ## Rows: 29,500 ## Columns: 14 ## $ sha &lt;chr&gt; &quot;c630ebcdf30652f0422c3ec12a00b50241dc... ## $ source_x &lt;chr&gt; &quot;CZI&quot;, &quot;CZI&quot;, &quot;CZI&quot;, &quot;CZI&quot;, &quot;CZI&quot;, &quot;C... ## $ title &lt;chr&gt; &quot;Angiotensin-converting enzyme 2 (ACE... ## $ doi &lt;chr&gt; &quot;10.1007/s00134-020-05985-9&quot;, &quot;10.103... ## $ pmcid &lt;lgl&gt; NA, NA, NA, NA, NA, NA, NA, NA, NA, N... ## $ pubmed_id &lt;dbl&gt; 32125455, NA, NA, 32093211, 32125453,... ## $ license &lt;chr&gt; &quot;cc-by-nc&quot;, &quot;cc-by&quot;, &quot;cc-by&quot;, &quot;cc-by&quot;... ## $ abstract &lt;chr&gt; NA, NA, &quot;The geographic spread of 201... ## $ publish_time &lt;dbl&gt; 2020, 2020, 2020, 2020, 2020, 2020, 2... ## $ authors &lt;chr&gt; &quot;Zhang, Haibo; Penninger, Josef M.; L... ## $ journal &lt;chr&gt; &quot;Intensive Care Med&quot;, &quot;Cell Discovery... ## $ `Microsoft Academic Paper ID` &lt;dbl&gt; 2002765492, 3003430844, 3006065484, 1... ## $ `WHO #Covidence` &lt;chr&gt; &quot;#3252&quot;, &quot;#1861&quot;, &quot;#1043&quot;, &quot;#1999&quot;, &quot;... ## $ has_full_text &lt;lgl&gt; TRUE, TRUE, TRUE, TRUE, FALSE, TRUE, ... 4.13.1 Preprocessing # Remove lines with no abstract t2&lt;-subset(t1, !is.na(t1$abstract)) Create the text corpus corpus = Corpus(VectorSource(t2$abstract)) corpus[[1]][1] ## $content ## [1] &quot;The geographic spread of 2019 novel coronavirus (COVID-19) infections from the epicenter of Wuhan, China, has provided an opportunity to study the natural history of the recently emerged virus. Using publicly available event-date data from the ongoing epidemic, the present study investigated the incubation period and other time intervals that govern the epidemiological dynamics of COVID-19 infections. Our results show that the incubation period falls within the range of 2&amp;ndash;14 days with 95% confidence and has a mean of around 5 days when approximated using the best-fit lognormal distribution. The mean time from illness onset to hospital admission (for treatment and/or isolation) was estimated at 3&amp;ndash;4 days without truncation and at 5&amp;ndash;9 days when right truncated. Based on the 95th percentile estimate of the incubation period, we recommend that the length of quarantine should be at least 14 days. The median time delay of 13 days from illness onset to death (17 days with right truncation) should be considered when estimating the COVID-19 case fatality risk.&quot; t2$doi[1] ## [1] &quot;10.3390/jcm9020538&quot; Conversion to lowercase corpus = tm_map(corpus, PlainTextDocument) corpus = tm_map(corpus, tolower) corpus[[1]][1] ## $content ## [1] &quot;the geographic spread of 2019 novel coronavirus (covid-19) infections from the epicenter of wuhan, china, has provided an opportunity to study the natural history of the recently emerged virus. using publicly available event-date data from the ongoing epidemic, the present study investigated the incubation period and other time intervals that govern the epidemiological dynamics of covid-19 infections. our results show that the incubation period falls within the range of 2&amp;ndash;14 days with 95% confidence and has a mean of around 5 days when approximated using the best-fit lognormal distribution. the mean time from illness onset to hospital admission (for treatment and/or isolation) was estimated at 3&amp;ndash;4 days without truncation and at 5&amp;ndash;9 days when right truncated. based on the 95th percentile estimate of the incubation period, we recommend that the length of quarantine should be at least 14 days. the median time delay of 13 days from illness onset to death (17 days with right truncation) should be considered when estimating the covid-19 case fatality risk.&quot; Remove punctuation corpus = tm_map(corpus, removePunctuation) corpus[[1]][1] ## $content ## [1] &quot;the geographic spread of 2019 novel coronavirus covid19 infections from the epicenter of wuhan china has provided an opportunity to study the natural history of the recently emerged virus using publicly available eventdate data from the ongoing epidemic the present study investigated the incubation period and other time intervals that govern the epidemiological dynamics of covid19 infections our results show that the incubation period falls within the range of 2ndash14 days with 95 confidence and has a mean of around 5 days when approximated using the bestfit lognormal distribution the mean time from illness onset to hospital admission for treatment andor isolation was estimated at 3ndash4 days without truncation and at 5ndash9 days when right truncated based on the 95th percentile estimate of the incubation period we recommend that the length of quarantine should be at least 14 days the median time delay of 13 days from illness onset to death 17 days with right truncation should be considered when estimating the covid19 case fatality risk&quot; Remove stopwords corpus = tm_map(corpus, removeWords, c(&quot;cloth&quot;, stopwords(&quot;english&quot;))) corpus[[1]][1] ## $content ## [1] &quot; geographic spread 2019 novel coronavirus covid19 infections epicenter wuhan china provided opportunity study natural history recently emerged virus using publicly available eventdate data ongoing epidemic present study investigated incubation period time intervals govern epidemiological dynamics covid19 infections results show incubation period falls within range 2ndash14 days 95 confidence mean around 5 days approximated using bestfit lognormal distribution mean time illness onset hospital admission treatment andor isolation estimated 3ndash4 days without truncation 5ndash9 days right truncated based 95th percentile estimate incubation period recommend length quarantine least 14 days median time delay 13 days illness onset death 17 days right truncation considered estimating covid19 case fatality risk&quot; Stemming corpus = tm_map(corpus, stemDocument) corpus[[1]][1] ## $content ## [1] &quot;geograph spread 2019 novel coronavirus covid19 infect epicent wuhan china provid opportun studi natur histori recent emerg virus use public avail eventd data ongo epidem present studi investig incub period time interv govern epidemiolog dynam covid19 infect result show incub period fall within rang 2ndash14 day 95 confid mean around 5 day approxim use bestfit lognorm distribut mean time ill onset hospit admiss treatment andor isol estim 3ndash4 day without truncat 5ndash9 day right truncat base 95th percentil estim incub period recommend length quarantin least 14 day median time delay 13 day ill onset death 17 day right truncat consid estim covid19 case fatal risk&quot; Create document term matrix frequencies = DocumentTermMatrix(corpus) #Remove sparcity sparse = removeSparseTerms(frequencies, 0.995) 4.13.2 Representation Find correlation between terms findAssocs(sparse,&quot;quarantin&quot;,0.1) ## $quarantin ## measur complianc ## 0.13 0.11 Find correlation between terms findAssocs(sparse,&quot;treatment&quot;,0.2) ## $treatment ## treat effect clinic patient young trial asthma min korean ## 0.27 0.26 0.24 0.23 0.23 0.22 0.22 0.22 0.20 Convert the matrix into a data frame, a format widely used in ‘R’ for predictive modeling. Outcome will be the class tSparse = as.data.frame(as.matrix(sparse)) colnames(tSparse) = make.names(colnames(tSparse)) tSparse$recommended_id = t2$source_x Histogram wf=data.frame(term=colnames(tSparse[,-2250]),occurrences=colSums(Filter(is.numeric, tSparse))) library(ggplot2) p &lt;- ggplot(subset(wf, colSums(Filter(is.numeric, tSparse))&gt;10000), aes(term, occurrences)) p &lt;- p + geom_bar(stat=&quot;identity&quot;) p &lt;- p + theme(axis.text.x=element_text(angle=45, hjust=1)) p 4.13.3 Discovery wordcloud library(wordcloud) set.seed(42) #limit words by specifying min frequency wordcloud(colnames(tSparse[,-2250]),colSums(Filter(is.numeric, tSparse)), min.freq=7000) wordcloud with colors library(RColorBrewer) wordcloud(colnames(tSparse[,-2250]),colSums(Filter(is.numeric, tSparse)), min.freq=7000, colors=brewer.pal(6,&quot;Dark2&quot;)) 4.13.4 Output knowledge Check the baseline accuracy (proportion of majority class) prop.table(table(tSparse$recommended_id)) biorxiv CZI medrxiv PMC 0.0210339 0.0315136 0.0132298 0.9342228 continue for the random forest algorithm "],
["visual-data-mining.html", "4.14 Visual Data Mining", " 4.14 Visual Data Mining 2020-05-26 all code from the professor Download Overweight or obese population from OECD Data Loading data library(&quot;tidyverse&quot;) oecd.obse &lt;- read.csv(&quot;2.UploadedData/oecd_obese.csv&quot;, encoding=&quot;UTF-8&quot;, header= TRUE, sep=&quot;,&quot;) names(oecd.obse)[1]&lt;-&quot;LOCATION&quot; # change first column name See the data library(knitr) library(kableExtra) head(oecd.obse, 15) LOCATION INDICATOR SUBJECT MEASURE FREQUENCY TIME Value Flag.Codes AUS OVEROBESE MEASURED PC_POP15 A 1980 36.5 D AUS OVEROBESE MEASURED PC_POP15 A 1983 39.7 D AUS OVEROBESE MEASURED PC_POP15 A 1989 43.8 D AUS OVEROBESE MEASURED PC_POP15 A 1995 59.2 AUS OVEROBESE MEASURED PC_POP15 A 2007 61.2 D AUS OVEROBESE MEASURED PC_POP15 A 2011 63.4 D AUS OVEROBESE MEASURED PC_POP15 A 2014 63.4 D CAN OVEROBESE MEASURED PC_POP15 A 2004 56.8 CAN OVEROBESE MEASURED PC_POP15 A 2005 57.7 CAN OVEROBESE MEASURED PC_POP15 A 2008 60.0 CAN OVEROBESE MEASURED PC_POP15 A 2010 58.6 CAN OVEROBESE MEASURED PC_POP15 A 2013 60.3 FIN OVEROBESE MEASURED PC_POP15 A 1979 53.6 FIN OVEROBESE MEASURED PC_POP15 A 2000 63.6 FIN OVEROBESE MEASURED PC_POP15 A 2011 65.0 Explore the dataset summary(oecd.obse) LOCATION INDICATOR SUBJECT MEASURE FREQUENCY TIME Value Flag.Codes JPN :37 OVEROBESE:153 MEASURED:153 PC_POP15:153 A:153 Min. :1978 Min. :18.60 :143 GBR :26 NA NA NA NA 1st Qu.:1997 1st Qu.:29.40 B: 1 LUX :18 NA NA NA NA Median :2005 Median :53.30 D: 7 KOR :11 NA NA NA NA Mean :2002 Mean :47.15 E: 2 USA :10 NA NA NA NA 3rd Qu.:2010 3rd Qu.:61.20 NA NZL : 8 NA NA NA NA Max. :2014 Max. :71.30 NA (Other):43 NA NA NA NA NA NA NA 4.14.1 ggplot2 Real world data library(ggplot2) library(ggforce) s &lt;- ggplot(data=oecd.obse) s &lt;- s + aes(x=TIME, y=Value ) s &lt;- s + aes(color=LOCATION) #s &lt;- s + geom_smooth(method=&quot;lm&quot;) s &lt;- s + geom_line(size=1) s Clean it up, just keeping “measured” records oecd.obse.measure &lt;- filter(oecd.obse, SUBJECT == &quot;MEASURED&quot;) Plot value over time by Location s &lt;- ggplot(data=oecd.obse.measure) s &lt;- s + aes(x=TIME, y=Value ) s &lt;- s + aes(color=LOCATION) s &lt;- s + geom_line(size=1) s Zoom Mexico library(ggforce) t &lt;- ggplot(data=oecd.obse.measure) t &lt;- t + aes(x=TIME, y=Value ) t &lt;- t + aes(color=LOCATION) t &lt;- t + geom_line(size=1) t &lt;- t + facet_zoom(xy = LOCATION == c(&quot;MEX&quot;)) t 4.14.2 vdmR again filter data and see value library(vdmR) measured.obse &lt;- oecd.obse[which(oecd.obse$SUBJECT==&#39;MEASURED&#39;), ] quantile(measured.obse$Value) ## 0% 25% 50% 75% 100% ## 18.6 29.4 53.3 61.2 71.3 now we can use vdmR library to see the data in a browser. create a histogram: vhist(Value, measured.obse, \"hist01\", \"measured.obse\", color = LOCATION) and launch it with data table: vlaunch(measured.obse, \"main\", \"measured.obse\", browse=TRUE) Japan, Korea are below 1st quartiles, whereas USA, New Zealand, Austria, UK and Hungary are above 3rd quartile. Read new data annd merge it with initial data pyll &lt;- read.csv(&quot;2.UploadedData/PYLL.csv&quot;, encoding=&quot;UTF-8&quot;, header= TRUE, sep=&quot;,&quot;) names(pyll)[1]&lt;-&quot;LOCATION&quot; deaths_cancer &lt;- read.csv(&quot;2.UploadedData/Deaths_Cancer.csv&quot;, encoding=&quot;UTF-8&quot;, header= TRUE, sep=&quot;,&quot;) names(deaths_cancer)[1]&lt;-&quot;LOCATION&quot; merged &lt;- merge(oecd.obse,pyll, by.x=c(&#39;LOCATION&#39;,&#39;TIME&#39;), by.y=c(&#39;LOCATION&#39;,&#39;TIME&#39;)) dropping some columns and rename them merged &lt;- merged[,-c(8:12)] merged &lt;- merged[,-9] merged &lt;- merged[,-c(3:6)] colnames(merged) &lt;- c(&quot;LOCATION&quot;, &quot;TIME&quot;, &quot;OBESITY&quot;, &quot;PYLL&quot;) create a scatter plot: vscat(OBESITY, PYLL, merged, \"scat01\", \"measured.obse\") and launch it with data vlaunch(merged, \"main\", \"measured.obse\", browse=TRUE) merge new data and dropp some columns and rename them: merged&lt;- merge(merged,deaths_cancer, by.x=c(&#39;LOCATION&#39;,&#39;TIME&#39;), by.y=c(&#39;LOCATION&#39;,&#39;TIME&#39;)) merged &lt;- merged[,-c(5:8)] merged &lt;- merged[,-6] colnames(merged) &lt;- c(&quot;LOCATION&quot;, &quot;TIME&quot;, &quot;OBESITY&quot;, &quot;PYLL&quot;, &quot;DEATHS_CANCER&quot;) create a new scatter plot: vscat(OBESITY, DEATHS_CANCER, merged, \"scat02\", \"measured.obse\") and launch it with data vlaunch(merged, \"main\", \"measured.obse\", browse=TRUE) "],
["bayes.html", "5 BAYES", " 5 BAYES Modelação Estatística Bayesiana Conteúdos programáticos Inferência Bayesiana: Introdução à inferência Bayesiana: Probabilidade e parâmetros; Inferência frequentista clássica versus inferência Bayesiana; Fundamentos da inferência Bayesiana; Distribuições a priori; Distribuições a posteriori; Distribuições preditivas a posteriori; Modelos Bayesianos básicos; Modelação hierárquica; Avaliação dos modelos. Construção de modelos de inferência Bayesiana: Inferência Bayesiana com distribuições a priori conjugadas; Computação Bayesiana – métodos de Monte Carlo, métodos de Monte Carlo via Cadeias de Markov (MCMC), algoritmo de Metropolis-Hastings, algoritmo de Gibbs e outros algoritmos relacionados; Métodos de avaliação da qualidade dos modelos (escolha de valores iniciais, convergência, eficiência e precisão); Métodos de selecção de modelos; Aplicação dos métodos de inferência Bayesiana a problemas mais comuns de inferência estatística – modelos de regressão, análise de dados categóricos e modelos de síntese de evidência. Redes Bayesianas: Introdução às Redes Bayesianas: Motivação e exemplos; Probabilidade e aplicações médicas; Modelos gráficos de probabilidade; Semântica e factorização nas redes Bayesianas. Construção de redes Bayesianas a partir de dados: Aprendizagem automática; Estimação de parâmetros de redes Bayesianas; Aprendizagem da estrutura de redes Bayesianas; Aprendizagem com dados incompletos. "],
["bayesian-statistical-inference.html", "5.1 Bayesian statistical inference", " 5.1 Bayesian statistical inference 2020-03-03 5.1.1 Representing a Beta(a,b) distribution: # define the function pl.beta &lt;- function(a,b, asp = if(isLim) 1, ylim = if(isLim) c(0,1.1)) { if(isLim &lt;- a == 0 || b == 0 || a == Inf || b == Inf) { eps &lt;- 1e-10 x &lt;- c(0, eps, (1:7)/16, 1/2+c(-eps,0,eps), (9:15)/16, 1-eps, 1) } else { x &lt;- seq(0, 1, length = 1025) } Mean_dbeta&lt;-round(a/(a+b), digits=4) Variance_dbeta&lt;-round((a*b)/((a+b)^2*(a+b+1)), digits=4) Mode_dbeta&lt;-round((a-1)/(a+b-2), digits=4) P_50_0_percent_0&lt;-qbeta(0.5, a, b) P_50_0_percent&lt;-round(P_50_0_percent_0, digits=4) P_2_5_percent_0&lt;-qbeta(0.025, a, b) P_2_5_percent&lt;-round(P_2_5_percent_0, digits=4) P_97_5_percent_0&lt;-qbeta(0.975, a, b) P_97_5_percent&lt;-round(P_97_5_percent_0, digits=4) fx &lt;- cbind(dbeta(x, a,b)) f &lt;- fx; f[fx == Inf] &lt;- 1e100 matplot(x, f, ylab=&quot;&quot;, type=&quot;l&quot;, ylim=ylim, asp=asp, main = paste0 (sprintf(&quot;Beta(a=%g, b=%g)&quot;, a,b))) print(paste0 (&quot;Distribution: &quot;, sprintf(&quot;Beta(a=%g, b=%g)&quot;, a,b))) print(paste0 (&quot;Mean=&quot;, Mean_dbeta, &quot;; Variance=&quot;, Variance_dbeta,&quot;; Median=&quot;, P_50_0_percent, &quot;; Mode=&quot;, Mode_dbeta)) print(paste0 (&quot;Perc(2.5%)=&quot;, P_2_5_percent, &quot;; Perc(97.5%)=&quot;, P_97_5_percent)) } 5.1.1.1 Exercise You are performing a study to estimate the Case Fatality Rate of the new Corona Virus in a rural community in China. You observed that among the 119 people formally diagnosed with the disease, three have died and 116 have recovered. The case Fatality Rate is an estimate of the probability of dying among those affected by a disease and the proportion of deaths among affected subjects is used to estimate it. Use the Bayesian statistical inference approach to estimate the Case Fatality Rate for this community. Assuming an uninformative prior calculate the likelihood and the posterior distribution for the parameter of interest. Likelihood x Prior = Posterior \\(Bin(x,n)\\) x \\(Beta(a,b)\\) = \\(Beta(x+a,n-x+b)\\) \\(Bin(3,119)\\) x \\(Beta(1,1)\\) = \\(Beta(4,117)\\) Use the R software to draw the density of the posterior distribution and to calculate its mean, median, mode and the 95% credible interval for the parameter of interest. library(printr) # plot density pl.beta(4,117) ## [1] &quot;Distribution: Beta(a=4, b=117)&quot; ## [1] &quot;Mean=0.0331; Variance=0.0003; Median=0.0305; Mode=0.0252&quot; ## [1] &quot;Perc(2.5%)=0.0092; Perc(97.5%)=0.0713&quot; Based on the available evidence regarding the parameter of interest, what is the probability of the Case Fatality Rate of the new Corona Virus being higher than 5%? And being higher than 10%? Use the following R functions that allow calculating densities, probabilities and percentiles associated with Beta distributions: Functions to work with Beta distributions dbeta(x, a, b) Gives you the density at x of the distribution function Beta(a,b) pbeta(x, a, b) Gives you the probability of observing a value lower than or equal to x (P[X =&lt; x]) for the distribution function Beta(a,b) pbeta(x, a, b, lower.tail=FALSE) Gives you the probability of observing a value larger than x (P[X &gt; x]) for the distribution function Beta(a,b) qbeta(x, a, b) Gives you the percentile x of the distribution function Beta(a,b) rbeta(n, a, b) Gives you n randomly chosen observations from the distribution function Beta(a,b) p5.a &lt;- pbeta(0.05, 4, 117, lower.tail=FALSE) p10.a &lt;- pbeta(0.1, 4, 117, lower.tail=FALSE) P[X &gt; 0.05] = 0.14441 P[X &gt; 0.1] = 0.00157 Some days later, you perform an identical study in neighbouring community. In this community, you observed that among the 419 people formally diagnosed with the disease, eight have died and 411 have recovered. Using a Bayesian inference approach, estimate the Case Fatality Rate of the new Corona Virus taking into account the available data for this community and the previous knowledge regarding the parameter of interest. Calculate the new likelihood and the new posterior distribution for the parameter of interest. \\(Bin(x,n)\\) x \\(Beta(a,b)\\) = \\(Beta(x+a,n-x+b)\\) \\(Bin(8,419)\\) x \\(Beta(4,117)\\) = \\(Beta(12, 528)\\) Use the R software to draw the density of the new posterior distribution and to calculate its mean, median, mode and the 95% credible interval for the parameter of interest. Use the R function previously presented. # plot density pl.beta(12,528) ## [1] &quot;Distribution: Beta(a=12, b=528)&quot; ## [1] &quot;Mean=0.0222; Variance=0; Median=0.0216; Mode=0.0204&quot; ## [1] &quot;Perc(2.5%)=0.0116; Perc(97.5%)=0.0362&quot; Based on all the available evidence regarding the parameter of interest after performing this second study, what is the probability of the Case Fatality Rate of the new Corona Virus being higher than 5%? And being higher than 10%? Use the R functions previously presented. p5.b &lt;- pbeta(0.05, 12, 528, lower.tail=FALSE) p10.b &lt;- pbeta(0.1, 12, 528, lower.tail=FALSE) options(scipen=999) P[X &gt; 0.05] = 0.00034 P[X &gt; 0.1] = 0 Consider an alternative situation where you use all the observations of the first and second studies as if they were obtained in a single study. Assuming an uninformative prior calculate, in this case, the likelihood and the posterior distribution for the parameter of interest. Use the R software to draw the density of the posterior distribution and to calculate its mean, median, mode and the 95% credible interval for the parameter of interest. Use the R functions previously presented. \\(Bin(x,n)\\) x \\(Beta(a,b)\\) = \\(Beta(x+a,n-x+b)\\) \\(Bin(3+8,119+419)\\) x \\(Beta(1,1)\\) = \\(Beta(12, 528)\\) # plot density pl.beta(12,528) ## [1] &quot;Distribution: Beta(a=12, b=528)&quot; ## [1] &quot;Mean=0.0222; Variance=0; Median=0.0216; Mode=0.0204&quot; ## [1] &quot;Perc(2.5%)=0.0116; Perc(97.5%)=0.0362&quot; "],
["logistic-curve-with-bayes.html", "5.2 Logistic curve with Bayes", " 5.2 Logistic curve with Bayes 2020-03-24 5.2.1 A dose-response model Fit a logistic curve with ‘centred’ covariate \\((x_i - \\overline{x})\\): \\(r_i \\sim {\\sf Binom}(p_i, n_i)\\) \\(logit(p_i) = \\alpha + \\beta(x_i - \\overline{x})\\) \\(\\alpha \\sim N(0,10000)\\) \\(\\beta \\sim N(0,10000)\\) Fit a logistic curve with ‘un-centred’ covariate \\(x\\): \\(r_i \\sim {\\sf Binom}(p_i, n_i)\\) \\(logit(p_i) = \\alpha + \\beta(x_i)\\) \\(\\alpha \\sim N(0,10000)\\) \\(\\beta \\sim N(0,10000)\\) "],
["bayesian-nets.html", "5.3 Bayesian Nets", " 5.3 Bayesian Nets 2020-05-12 5.3.1 Inference in Bayesian nets Exercise from class: Design the network with SamIam and fit the conditional probabilities with R Read the network designed with SamIam library(bnlearn) mi.net&lt;-read.net(&quot;2.UploadedData/mi.vstructure.net&quot;) see the conditional probabilities defined mi.net ## ## Bayesian network parameters ## ## Parameters of node wine (multinomial distribution) ## ## Conditional probability table: ## TRUE FALSE ## 0.5 0.5 ## ## Parameters of node mi (multinomial distribution) ## ## Conditional probability table: ## ## , , smoking = TRUE ## ## wine ## mi TRUE FALSE ## TRUE 0.5 0.5 ## FALSE 0.5 0.5 ## ## , , smoking = FALSE ## ## wine ## mi TRUE FALSE ## TRUE 0.5 0.5 ## FALSE 0.5 0.5 ## ## ## Parameters of node smoking (multinomial distribution) ## ## Conditional probability table: ## TRUE FALSE ## 0.51 0.49 make a graph object mi.graph&lt;-bn.net(mi.net) plot(mi.graph) Read the data to fit the network data&lt;-read.csv(&quot;2.UploadedData/mi.dat&quot;) summary(data) smoking wine mi Mode :logical Mode :logical Mode :logical FALSE:59 FALSE:60 FALSE:60 TRUE :61 TRUE :60 TRUE :60 factorize variables data &lt;- as.data.frame(apply(data,2, as.factor)) fit this network with the data we have mi.fit&lt;-bn.fit(mi.graph,data) see the new conditional probabilities fitted from the data mi.fit ## ## Bayesian network parameters ## ## Parameters of node wine (multinomial distribution) ## ## Conditional probability table: ## FALSE TRUE ## 0.5 0.5 ## ## Parameters of node mi (multinomial distribution) ## ## Conditional probability table: ## ## , , smoking = FALSE ## ## wine ## mi FALSE TRUE ## FALSE 0.6470588 0.7500000 ## TRUE 0.3529412 0.2500000 ## ## , , smoking = TRUE ## ## wine ## mi FALSE TRUE ## FALSE 0.3333333 0.3461538 ## TRUE 0.6666667 0.6538462 ## ## ## Parameters of node smoking (multinomial distribution) ## ## Conditional probability table: ## FALSE TRUE ## 0.4916667 0.5083333 # export the results write.net(mi.fit,file=&quot;mi.fitinR.net&quot;) Again in SamIam, we can see the results in a graph and play with the options "],
["bayesian-temporal-nets.html", "5.4 Bayesian temporal Nets", " 5.4 Bayesian temporal Nets 2020-05-26 5.4.1 backstage functions from probability to rate library(gRain) rate &lt;- function(p, t) { out &lt;- c(-log(1 - p) / t) names(out) &lt;- paste(&quot;Rate&quot;,t,&quot;y&quot;,sep=&quot;&quot;) out } probability from one yearly time frame to another prob &lt;- function(p, t.orig = 5, t.final = 1) { out &lt;- c(1 - exp( -rate(p, t.orig) * t.final)) out } complement comp &lt;- function(v){ out &lt;- rep(v, each=2) for (i in 2:length(out)) if (i %% 2 == 0) out[i] &lt;- 1 - out[i-1] out } HR odds.p &lt;- function(p) { p / (1 - p) } p.odds &lt;- function(odds) { odds / (1 + odds) } surv.hr &lt;- function(ref, hr) {ref ^ hr} yn &lt;- c(&quot;yes&quot;,&quot;no&quot;) ny &lt;- c(&quot;no&quot;,&quot;yes&quot;) create dependencies using RR, OR or HR associate &lt;- function(ref, rr = NULL, or = NULL, hr = NULL) { if(!is.null(rr)) { prob &lt;- ref * rr } else if(!is.null(or)) { prob &lt;- p.odds((odds.p(ref) * or)) } else if(!is.null(hr)) { prob &lt;- 1 - ((1 - ref) ^ hr) } else prob &lt;- ref if (length(which(prob &gt; 1)) &gt; 0) { cat(&quot;Something went wrong... Prob &gt; 1!\\n&quot;) return(NULL) } else return(c(prob, ref)) } add a static node addStatic &lt;- function(cptList, nodeName, nodeStates=yn, nodeDependencies=NULL, nodeStatesProb=0.5) { if (!is.null(nodeDependencies) &amp;&amp; !is.na(nodeDependencies)) { deps &lt;- paste(nodeDependencies,collapse=&quot;+&quot;) form &lt;- as.formula(paste(&quot;~&quot;,nodeName,&quot;+&quot;,deps,sep=&quot;&quot;)) } else form &lt;- as.formula(paste(&quot;~&quot;,nodeName,sep=&quot;&quot;)) cptList[[length(cptList)+1]] &lt;- cptable(form, levels=nodeStates, values=comp(nodeStatesProb)) names(cptList[[length(cptList)]]) &lt;- nodeName cptList } add a temporal node addNode &lt;- function(cptList, nodeName, nodeStates=yn, nodeDependencies=NULL, nodeStatesProb=0.5, nodeTemporal=FALSE, nodeLeaps=NULL) { if (nodeTemporal) { transProb = prob(p = nodeStatesProb, t.orig = nodeLeaps, t.final = 1) cptList &lt;- addStatic(cptList = cptList, nodeName = paste(nodeName,&quot;1&quot;,sep=&quot;.&quot;), nodeStates = nodeStates, nodeDependencies = nodeDependencies, nodeStatesProb = transProb) for (i in 2:nodeLeaps) cptList &lt;- addStatic(cptList = cptList, nodeName = paste(nodeName,i,sep=&quot;.&quot;), nodeStates = nodeStates, nodeDependencies = c(nodeDependencies,paste(nodeName,i-1,sep=&quot;.&quot;)), nodeStatesProb = c(rep(1,length(transProb)),transProb)) } else cptList &lt;- addStatic(cptList = cptList, nodeName = nodeName, nodeStates = nodeStates, nodeDependencies = nodeDependencies, nodeStatesProb = nodeStatesProb) cptList } 5.4.2 building the model library(bnlearn) allCPT &lt;- NULL add a static independent node allCPT &lt;- addStatic(allCPT, nodeName=&quot;temp&quot;, nodeDependencies=NULL, nodeStatesProb=0.1) add a static dependent node allCPT &lt;- addStatic(allCPT, nodeName=&quot;other&quot;, nodeDependencies=&quot;temp&quot;,nodeStatesProb=c(0.4,0.7)) add a static doubly-dependent node allCPT &lt;- addStatic(allCPT, nodeName=&quot;another&quot;, nodeDependencies=c(&quot;temp&quot;,&quot;other&quot;),nodeStatesProb=c(0.4,0.7,0.3,0.5)) #### add a temporal independent node #allCPT &lt;- addNode(allCPT, nodeName=&quot;death&quot;, nodeDependencies=NULL,nodeStatesProb=c(0.2), nodeTemporal=TRUE, nodeLeaps=2) #### add a temporal dependent node #allCPT &lt;- addNode(allCPT, nodeName=&quot;adjdeath&quot;, nodeDependencies=&quot;temp&quot;,nodeStatesProb=c(0.2,0.1), nodeTemporal=TRUE, nodeLeaps=2) #### add a temporal dependent node (using OR) #allCPT &lt;- addNode(allCPT, nodeName=&quot;adjdeathOR&quot;, nodeDependencies=&quot;temp&quot;,nodeStatesProb=associate(ref=0.2, or=2), nodeTemporal=TRUE, nodeLeaps=10) #### add a temporal dependent node (using RR) #allCPT &lt;- addNode(allCPT, nodeName=&quot;adjdeathRR&quot;, nodeDependencies=&quot;temp&quot;,nodeStatesProb=associate(ref=0.2, rr=2), nodeTemporal=TRUE, nodeLeaps=10) #### add a temporal dependent node (using HR) #allCPT &lt;- addNode(allCPT, nodeName=&quot;adjdeathHR&quot;, nodeDependencies=&quot;temp&quot;,nodeStatesProb=associate(ref=0.2, hr=2), nodeTemporal=TRUE, nodeLeaps=10) compiling the network cat(&quot;:: Compiling the network... &quot;) ## :: Compiling the network... plist &lt;- compileCPT(allCPT, forceCheck = TRUE) net &lt;- grain(plist, details = 2) cat(&quot;DONE ::\\n&quot;) ## DONE :: plotting the network cat(&quot;:: Plotting the network... &quot;) ## :: Plotting the network... cat(&quot;(&quot;,length(net$universe$nodes),&quot; nodes) &quot;,sep=&quot;&quot;) ## (3 nodes) plot(as.bn(net)) print(summary(net)) ## Independence network: Compiled: TRUE Propagated: FALSE ## Nodes : chr [1:3] &quot;temp&quot; &quot;other&quot; &quot;another&quot; ## Number of cliques: 1 ## Maximal clique size: 3 ## Maximal state space in cliques: 8 ## Independence network: Compiled: TRUE Propagated: FALSE ## Nodes: chr [1:3] &quot;temp&quot; &quot;other&quot; &quot;another&quot; cat(&quot;DONE ::\\n&quot;) ## DONE :: plotting the network cat(&quot;:: Storing the network... &quot;) ## :: Storing the network... saveHuginNet(net, file=&quot;net.net&quot;) pdf(file=&quot;net.pdf&quot;, width=24, height=12) plot(net) dev.off() ## png ## 2 cat(&quot;DONE ::\\n&quot;) ## DONE :: querying the network cat(&quot;:: Querying the network...\\n&quot;) ## :: Querying the network... print(querygrain(net, nodes=c(&quot;other&quot;, &quot;temp&quot;), type=&quot;conditional&quot;)) ## other ## temp yes no ## yes 0.4 0.6 ## no 0.7 0.3 print(querygrain(net, nodes=c(&quot;other&quot;), type=&quot;joint&quot;)) ## other ## yes no ## 0.67 0.33 cat(&quot;DONE ::\\n&quot;) ## DONE :: setting evidence cat(&quot;:: Setting evidence in the network... &quot;) ## :: Setting evidence in the network... net1 &lt;- setFinding(net, nodes=c(&quot;temp&quot;), states=c(&quot;yes&quot;)) cat(&quot;DONE ::\\n&quot;) ## DONE :: building from data cat(&quot;:: Building from data\\n&quot;) ## :: Building from data data(&quot;cad1&quot;) head(cad1) Sex AngPec AMI QWave QWavecode STcode STchange SuffHeartF Hypertrophi Hyperchol Smoker Inherit Heartfail CAD Male None NotCertain No Usable Usable No No No No No No No No Male Atypical NotCertain No Usable Usable No No No No No No No No Female None Definite No Usable Usable No No No No No No No No Male None NotCertain No Usable Nonusable No No No No No No No No Male None NotCertain No Usable Nonusable No No No No No No No No Male None NotCertain No Usable Nonusable No No No No No No No No dag.cad &lt;- dag(~ CAD:Smoker:Inherit:Hyperchol + AngPec:CAD + Heartfail:CAD + QWave:CAD) plot(dag.cad) bn.cad &lt;- grain(dag.cad, data = cad1, smooth = 0.1) print(querygrain(bn.cad, nodes=c(&quot;CAD&quot;, &quot;Smoker&quot;), type=&quot;conditional&quot;)) ## CAD ## Smoker No Yes ## No 0.7172084 0.2827916 ## Yes 0.4933771 0.5066229 cat(&quot;DONE ::\\n&quot;) ## DONE :: "],
["temporal-modeling.html", "5.5 Temporal Modeling", " 5.5 Temporal Modeling 2020-05-19 5.5.1 Transition probabilities Consider 5 year survival for stage IV triple negative breast cancer patients is 50% How to model it, yearly? We will use the functions given by the professor to compute Rate and Probability. from probability to rate. rate &lt;- function(p, t) { out &lt;- c(-log(1 - p) / t) names(out) &lt;- paste(&quot;Rate&quot;,t,&quot;y&quot;,sep=&quot;&quot;) out } probability from one yearly time frame to another. prob &lt;- function(p, t.orig = 5, t.final = 1) { out &lt;- c(1 - exp( -rate(p, t.orig) * t.final)) out } complement comp &lt;- function(v){ out &lt;- rep(v, each=2) for (i in 2:length(out)) if (i %% 2 == 0) out[i] &lt;- 1 - out[i-1] out } In this exercise we apply the function prob() to have Prob(y(n)|~y(n-1)) p1y&lt;-prob(p = 0.5, t.orig = 5, t.final = 1) comp(p1y) ## Rate5y Rate5y ## 0.1294494 0.8705506 With these probabilities we can compute the network in SamIam. Those were the probabilities for y1, for the other 4 nodes the conditional probability table is as: The result is as expected: Now we add a new node for the subtype with probabilities 0.8 (luminal A) and 0.2 (tripleneg): With the new probabilities we have to compute new conditional probability tables for each one of the years. We know that with tripleneg the 5 year survival is 50%, but luminal A subtype has a 5 year survival of 90%. So we have to compute the probabilities for lunimal A. p1yA&lt;-prob(p = 0.9, t.orig = 5, t.final = 1) comp(p1yA) ## Rate5y Rate5y ## 0.3690427 0.6309573 The y1 conditional table will have the table: and for the other years, each one will be like: The final result Now you can play with it!! "],
["top.html", "6 TOP.HIDA", " 6 TOP.HIDA Tópicos avançados em HIDA Conteúdos programáticos Os conteúdos abrangerão as diversas áreas da análise inteligente de dados, nomeadamente metodologias de visualização, pré-processamento, exploração e classificação de dados, proporcionando também a expansão dos conteúdos de outras UC relacionadas. A escolha dos tópicos será feita em cada edição desta unidade curricular, tendo em consideração não só o perfil e interesses dos alunos, como também os tópicos que no momento se considerem ser os mais relevantes na área. "],
["seminar1.html", "6.1 Seminar 1", " 6.1 Seminar 1 2020-02-04 Physiological signal processing in affective computing by Susana Brás Susana Brás is Postdoctoral researcher at Institute of Electronics and Telematics Engineering of Aveiro (IEETA), University of Aveiro, Portugal. At this moment, she is focused on ECG biometric identification, emotional modulated environments and information extraction from large biomedical databases. Before, she was a PhD student at Abel Salazar Biomedical Sciences Institute, at University of Porto, Portugal. Her thesis was focused on automation in anesthesia, basically the drug distribution and effect (alterations in the brain electrical activity) were studied, modeled and a controller was presented for the hypnotic effect. Her background is in Applied Mathematics, from the Sciences Faculty, at University of Porto. 6.1.1 Bruno Lima Podemos definir sinais como dados indexados a uma variável temporal e que podem ser oriundos de campos tão diversos como a biomedicina ou as comunicações sem fios. Neste seminário foi-nos apresentado o conceito de computação afectiva que consiste em identificar emoções a partir do processamento de sinais fisiológicos. O conceito de emoção é de difícil definição ainda que a caracterização das emoções seja algo mais perceptível. Conseguimos reconhecer emoções através de respostas fisiológicas como sejam o batimento cardíaco, a expressão facial, a tensão, o riso, a rubescência ou a sudação embora estas reacções do corpo variem de pessoa para pessoa. Com a ajuda de disciplinas como a psicofisiologia ou a psiconeuroimunologia pretende-se fazer a caracterização das emoções que permitam treinar algoritmos que identifiquem essas emoções e que assim auxiliem sistemas a tomar respostas adequadas às emoções identificadas. O desafio é o de criar um sistema que reconheça, interprete, processe ou simule afectos humanos. Esta computação afectiva procurará ser uma ajuda perante diferentes estados emocionais. Podemos imaginar uma robot que seja de facto uma companhia para os humanos e um prestador de cuidados emocionais. A criação de um sistema como descrito está sujeito, não só, a grandes desafios tecnológicos (como sejam: a monitorização, armazenamento, processamento, data mining e machine learning) mas também desafios éticos. Com a identificação de emoções identificam-se também preferências e consequentemente surge a possibilidade de manipular essas preferências. No campo da saúde, a chamada ‘emotional analytics’ permite prever diagnósticos perante a identificação de emoções e consequentemente actuar preventivamente minimizando danos mais graves. Em campos como a saúde mental ou a saúde geriátrica a correcta classificação das emoções dos doentes é a chave para prestar os melhores cuidados de saúde. "],
["dealing-with-imbalanced-data-a-practical-approach.html", "6.2 Dealing with imbalanced data: a practical approach", " 6.2 Dealing with imbalanced data: a practical approach 2020-06-02 6.2.1 Introduction We define a dataset as imbalanced when the classification’s categories are not approximately equally represented. In real-world data ‘normal’ examples are the most frequent, while ‘abnormal’ or interesting’ examples are quite rare, that is why the problem of imbalanced data is so common (Chawla et al. 2002). In real life there are several examples of imbalanced data: A manufacturing assembly line where the number of defective products are significantly lower than those without defects A test to detect patients with cancer in a given residential area Credit card fraud detection Again, in classification problems where one class outnumbers the other we face a problem of imbalanced data. Usually, majority class is referred as negative class, while the minority is the positive class. For learning algorithms based on imbalanced data, positive class instances are submerged in the negative class (Gao et al. 2014). Imbalances can be classified as intrinsic – when is a direct result of the nature of the dataspace – or extrinsic – when imbalance depends on upon variable factors as time and storage. As an example of this last type of imbalance, we can think on a continuous stream of intrinsically balanced data that at a specific time period (due to same kind of error) the acquired data can be imbalanced, and in this case the data set would be and intrinsic imbalanced data (He and Garcia 2009). It is not appropriate to evaluate learning algorithms performance using its predictive accuracy when we are dealing with imbalanced data. In domains where we are interested is the positive class rather than the negative one, we need a higher prediction capability for the former, although traditional data mining algorithms do not behave properly in the instance of imbalanced data (Chawla et al. 2002). So, the main problems with imbalanced data arise when we want to learn from it. Imbalanced data compromise significantly the performance of most standard learning algorithms, this algorithms are not able to represent properly the distributive characteristics of the data and consequently fail to deliver acceptable accuracies across the classes of the data (He and Garcia 2009). There are two major approaches to deal with imbalanced data: resampling methods (external methods) and imbalanced learning algorithms (internal methods) (Gao et al. 2014). The former uses resampling methods on the original imbalanced data in order to obtain a balanced input to train traditional learning algorithms. While the latter use modified learning algorithms so that are able to use original data without rebalance it (Gao et al. 2014). 6.2.2 Methods In this study, I aim to identify and describe R packages available for dealing with imbalanced data. Also, for the exemplification on how to deal with imbalanced data, I use the Cervical cancer Data Set (CCDS) from UCI repository (Dua and Graff 2017). A preliminary exploratory analysis is done in order to identify variables with excess of missing data, with the amelia package (Honaker, King, and Blackwell 2011). After excluding those variables with excessive number of missings, data is balanced for the outcome Dx with diferent methods and resulting balanced data is trained with a regression tree algorithm. With the caret package I do the downsamplig (randomly subset the negative class to match the size of the positive class) and the upsampling (sampling with replacement from the positive class until match the size of the negative class). while, with SMOTE (DMwR package) and ROSE (rose package) methods, negative classs is down-sampled and the positive class is up-sampled. All analysis is performed in RStudio with R programming lenguage (R Core Team 2013) 6.2.3 Results A total of 11 packages were identified and 10 of them are on CRAN. The package IRIC is corrently only available on github. These packages represent a variaty of solutions to deal with imbalanced data. The packages caret, mlr and DMwR are packages for machine learning and data mining that include functions for balance imbalanced data. package descriptive description ROSE Random Over-Sampling Examples The package provides functions to deal with binary classification problems in the presence of imbalanced classes. Synthetic balanced samples are generated according to ROSE (Menardi and Torelli, 2013). Functions that implement more traditional remedies to the class imbalance are also provided, as well as different metrics to evaluate a learner accuracy. These are estimated by holdout, bootstrap or cross-validation methods. themis Extra Recipes Steps for Dealing with Unbalanced Data A dataset with an uneven number of cases in each class is said to be unbalanced. Many models produce a subpar performance on unbalanced datasets. A dataset can be balanced by increasing the number of minority cases using SMOTE 2011 &lt;arXiv:1106.1813&gt;, BorderlineSMOTE 2005 &lt;doi:10.1007/11538059_91&gt;; and ADASYN 2008 &lt;https://ieeexplore.ieee.org/document/4633969&gt;;. Or by decreasing the number of majority cases using NearMiss 2003 &lt;https://www.site.uottawa.ca/~nat/Workshop2003/jzhang.pdf&gt;; or Tomek link removal 1976 &lt;https://ieeexplore.ieee.org/document/4309452&gt;;. imbalance Preprocessing Algorithms for Imbalanced Datasets Class imbalance usually damages the performance of classifiers. Thus, it is important to treat data before applying a classifier algorithm. This package includes recent resampling algorithms in the literature: (Barua et al. 2014) &lt;doi:10.1109/tkde.2012.232&gt;;; (Das et al. 2015) &lt;doi:10.1109/tkde.2014.2324567&gt;;, (Zhang et al. 2014) &lt;doi:10.1016/j.inffus.2013.12.003&gt;;; (Gao et al. 2014) &lt;doi:10.1016/j.neucom.2014.02.006&gt;;; (Almogahed et al. 2014) &lt;doi:10.1007/s00500-014-1484-5&gt;;. It also includes an useful interface to perform oversampling. smotefamily A Collection of Oversampling Techniques for Class Imbalance Problem Based on SMOTE A collection of various oversampling techniques developed from SMOTE is provided. SMOTE is a oversampling technique which synthesizes a new minority instance between a pair of one minority instance and one of its K nearest neighbor. (see &lt;https://www.jair.org/media/953/live-953-2037-jair.pdf&gt;; for more information) Other techniques adopt this concept with other criteria in order to generate balanced dataset for class imbalance problem ebmc Ensemble-Based Methods for Class Imbalance Problem Four ensemble-based methods (SMOTEBoost, RUSBoost, UnderBagging, and SMOTEBagging) for class imbalance problem are implemented for binary classification. Such methods adopt ensemble methods and data re-sampling techniques to improve model performance in presence of class imbalance problem. One special feature offers the possibility to choose multiple supervised learning algorithms to build weak learners within ensemble models. References: Nitesh V. Chawla, Aleksandar Lazarevic, Lawrence O. Hall, and Kevin W. Bowyer (2003) &lt;doi:10.1007/978-3-540-39804-2_12&gt;;, Chris Seiffert, Taghi M. Khoshgoftaar, Jason Van Hulse, and Amri Napolitano (2010) &lt;doi:10.1109/TSMCA.2009.2029559&gt;;, R. Barandela, J. S. Sanchez, R. M. Valdovinos (2003) &lt;doi:10.1007/s10044-003-0192-z&gt;;, Shuo Wang and Xin Yao (2009) &lt;doi:10.1109/CIDM.2009.4938667&gt;;, Yoav Freund and Robert E. Schapire (1997) &lt;doi:10.1006/jcss.1997.1504&gt;;. unbalanced Racing for Unbalanced Methods Selection A dataset is said to be unbalanced when the class of interest (minority class) is much rarer than normal behaviour (majority class). The cost of missing a minority class is typically much higher that missing a majority class. Most learning systems are not prepared to cope with unbalanced data and several techniques have been proposed. This package implements some of most well-known techniques and propose a racing algorithm to select adaptively the most appropriate strategy for a given unbalanced task. ebal Entropy reweighting to create balanced samples Package implements entropy balancing, a data preprocessing procedure that allows users to reweight a dataset such that the covariate distributions in the reweighted data satisfy a set of user specified moment conditions. This can be useful to create balanced samples in observational studies with a binary treatment where the control group data can be reweighted to match the covariate moments in the treatment group. Entropy balancing can also be used to reweight a survey sample to known characteristics from a target population. IRIC Integrated R Library for Imbalanced Classification IRIC is an R library for imbalanced classification, which will bring convenience to users by integrating a wide set of solutions into one library. caret Classification and Regression Training Misc functions for training and plotting classification and regression models. mlr3 Machine Learning in R - Next Generation Efficient, object-oriented programming on the building blocks of machine learning. Provides ‘R6’ objects for tasks, learners, resamplings, and measures. The package is geared towards scalability and larger datasets by supporting parallelization and out-of-memory data-backends like databases. While ‘mlr3’ focuses on the core computational operations, add-on packages provide additional functionality. DMwR Functions and data for “Data Mining with R” This package includes functions and data accompanying the book “Data Mining with R, learning with case studies” To evaluate the use of different methods for balance the date, I use the CCDS data. Let’s load an look to the CCDS data: library(tidyverse) ## read the data dataset&lt;-read.csv(&quot;2.UploadedData/risk_factors_cervical_cancer.csv&quot;,na.strings = c(&quot;NA&quot;,&quot;?&quot;,&quot;&quot;)) # exclude redundante variable dataset&lt;-dataset %&gt;% select(-STDs, -Dx.Cancer, -Dx.CIN, -Dx.HPV) # identified categorical variables categorical&lt;-c(&quot;Smokes&quot;, &quot;Hormonal.Contraceptives&quot;, &quot;IUD&quot;, &quot;STDs.condylomatosis&quot;,&quot;STDs.cervical.condylomatosis&quot;, &quot;STDs.vulvo.perineal.condylomatosis&quot;, &quot;STDs.syphilis&quot;, &quot;STDs.pelvic.inflammatory.disease&quot;, &quot;STDs.genital.herpes&quot;,&quot;STDs.molluscum.contagiosum&quot;, &quot;STDs.AIDS&quot;, &quot;STDs.HIV&quot;, &quot;STDs.Hepatitis.B&quot;, &quot;STDs.HPV&quot;, &quot;Hinselmann&quot;, &quot;Schiller&quot;, &quot;Dx&quot;, &quot;Citology&quot;, &quot;Biopsy&quot;) # factorize it dataset&lt;-dataset %&gt;% mutate_at(categorical, ~factor(.,levels = 0:1, labels = c(&quot;no&quot;,&quot;yes&quot;))) library(Amelia) # missings on original data missmap(dataset) Amelia package allow us to see how missing data are distributed on our dataset. Alternatively we can just count missings by variable. apply(dataset, MARGIN = 2, function(x) sum(is.na(x))) ## Age Number.of.sexual.partners ## 0 26 ## First.sexual.intercourse Num.of.pregnancies ## 7 56 ## Smokes Smokes..years. ## 13 13 ## Smokes..packs.year. Hormonal.Contraceptives ## 13 108 ## Hormonal.Contraceptives..years. IUD ## 108 117 ## IUD..years. STDs..number. ## 117 105 ## STDs.condylomatosis STDs.cervical.condylomatosis ## 105 105 ## STDs.vaginal.condylomatosis STDs.vulvo.perineal.condylomatosis ## 105 105 ## STDs.syphilis STDs.pelvic.inflammatory.disease ## 105 105 ## STDs.genital.herpes STDs.molluscum.contagiosum ## 105 105 ## STDs.AIDS STDs.HIV ## 105 105 ## STDs.Hepatitis.B STDs.HPV ## 105 105 ## STDs..Number.of.diagnosis STDs..Time.since.first.diagnosis ## 0 787 ## STDs..Time.since.last.diagnosis Dx ## 787 0 ## Hinselmann Schiller ## 0 0 ## Citology Biopsy ## 0 0 Do to the excessive among of missings, I excluded from the analysis, the variables STDs..Time.since.first.diagnosis and STDs..Time.since.last.diagnosis. Although it would be advised to do an imputation of values where we have missing data, I did’nt do it. dataset&lt;-dataset %&gt;% select(-STDs..Time.since.first.diagnosis, -STDs..Time.since.last.diagnosis) prop.table(table(dataset$Dx)) no yes 0.972028 0.027972 We will use this dataset to train a regression tree algorithm in order to predict the outcome Dx. But, we have a clear problem of imbalanced data if we want to predict Dx from which 97.2% are ‘no’ and only 2.8% are ‘yes’. ggplot(dataset, aes(x=Dx)) + geom_bar() + theme_bw() Now, I use four different methods to balance the data. Let’s start balancing the data by downsampling (with the caret package): library(caret) set.seed(1) down_train &lt;- downSample(x = dataset[, !colnames(dataset) %in% &quot;Dx&quot;], y = dataset$Dx) # we have to remane the Class variable names(down_train)[30]&lt;-&quot;Dx&quot; table(down_train$Dx) no yes 24 24 Now the upsampling: set.seed(1) up_train &lt;- upSample(x = dataset[, !colnames(dataset) %in% &quot;Dx&quot;], y = dataset$Dx) # we have to remane the Class variable names(up_train)[30]&lt;-&quot;Dx&quot; table(up_train$Dx) no yes 834 834 Is time for SMOTE: library(DMwR) set.seed(1) smote_train &lt;- SMOTE(Dx ~ ., data = dataset, perc.over = 400,perc.under=200) table(smote_train$Dx) no yes 192 120 and now ROSE: library(ROSE) set.seed(1) rose_train &lt;- ROSE(Dx ~ ., data = dataset)$data table(rose_train$Dx) no yes 346 322 With the new 4 datasets we can fit the regresssion trees metric &lt;- &quot;ROC&quot; control &lt;- trainControl(method = &quot;repeatedcv&quot;, number = 10, repeats = 5, summaryFunction=twoClassSummary, classProbs=TRUE, savePredictions = TRUE) set.seed(2) fit.rpart.down &lt;- train(Dx ~ ., data=down_train, method=&quot;rpart&quot;, metric=metric, trControl=control, na.action=na.exclude) set.seed(2) fit.rpart.up &lt;- train(Dx ~ ., data=up_train, method=&quot;rpart&quot;, metric=metric, trControl=control, na.action=na.exclude) set.seed(2) fit.rpart.smote &lt;- train(Dx ~ ., data=smote_train, method=&quot;rpart&quot;, metric=metric, trControl=control, na.action=na.exclude) set.seed(2) fit.rpart.rose &lt;- train(Dx ~ ., data=rose_train, method=&quot;rpart&quot;, metric=metric, trControl=control, na.action=na.exclude) And now we can compare the results. fit.models &lt;- list(down.rpart=fit.rpart.down, up.rpart=fit.rpart.up, smote.rpart= fit.rpart.smote, rose.rpart= fit.rpart.rose) results &lt;- resamples(fit.models) Let’s look at the ROC curves library(pROC) par(mfrow=c(2,2)) # rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$yes, # #main=fit, # debug=F, print.auc=T)}) for (i in 1:4){ plot.roc(fit.models[[i]]$pred$obs,fit.models[[i]]$pred$yes, main= names(fit.models)[i], debug=F, print.auc=T)} we can also see it in one plot: # combine the results ggroc&lt;-rbind(data.frame(method = &quot;Down&quot;, sens=roc(fit.rpart.down$pred$obs,fit.rpart.down$pred$yes)$sensitivities, spec=roc(fit.rpart.down$pred$obs,fit.rpart.down$pred$yes)$specificities), data.frame(method = &quot;Up&quot;, sens=roc(fit.rpart.up$pred$obs,fit.rpart.up$pred$yes)$sensitivities, spec=roc(fit.rpart.up$pred$obs,fit.rpart.up$pred$yes)$specificities), data.frame(method = &quot;SMOTE&quot;, sens=roc(fit.rpart.smote$pred$obs,fit.rpart.smote$pred$yes)$sensitivities, spec=roc(fit.rpart.smote$pred$obs,fit.rpart.smote$pred$yes)$specificities), data.frame(method = &quot;ROSE&quot;, sens=roc(fit.rpart.rose$pred$obs,fit.rpart.rose$pred$yes)$sensitivities, spec=roc(fit.rpart.rose$pred$obs,fit.rpart.rose$pred$yes)$specificities) ) custom_col &lt;- c(&quot;#009E73&quot;, &quot;#0072B2&quot;, &quot;#D55E00&quot;, &quot;#CC79A7&quot;) ggplot(ggroc, aes(x= 1-spec, y= sens, group = method)) + geom_line(aes(color = method), size = 1) + scale_color_manual(values = custom_col) + geom_abline(intercept = 0, slope = 1, color = &quot;gray&quot;, size = 1) + theme_bw() And compare accuracies dotplot(results) In this exercise we obtained the best results with ROSE dataset. Although we can have a problem of overfitting. 6.2.4 Conclusion Here, I presented some simple examples on how to deal with imbalanced data, with some pratical examples. There are more sophisticated methods available from the R packages presented on the table, but they require a more sophisticated data pre-processing. In a real world work, before balance the data we must do a data partition between train and test datasets. Methods to balance the data must be done on training dataset and the trained model is applied on the imbalanced test set. Real world framework do exploratory data analysis look for missing data exclude variables with more than 80% missings try to input data to other remaining missings do data partition between training data set and testing data set balance your training dataset train your algorithms with the training dataset test them in your testing dataset DO NOT balance your testing dataset References "],
["lab.html", "7 LAB.HIDA", " 7 LAB.HIDA Projeto Laboratorial em HIDA Conteúdos programáticos Conteúdos a definir em cada edição mediante discussão e coordenação próxima com entidades associadas ao programa por via de projetos de investigação ou contrato laboral com os alunos. Estas instituições serão unidades de investigação, instituições prestadoras de cuidados de saúde ou indústria. A escolha dos problemas na área da análise inteligente de dados que serão desenvolvidos no projeto laboratorial será feita em cada edição desta unidade curricular, tendo em consideração não só o perfil e interesses dos alunos, como também os tópicos que no momento se considerem ser os mais relevantes na área. Os problemas a resolver poderão envolver a análise de tempos até ao acontecimento de determinado evento (morte, diagnóstico de doença, recidiva, etc.), a análise de grandes fluxos de dados, a análise de dado longitudinais, etc. "],
["costs-4-health.html", "7.1 Costs 4 Health", " 7.1 Costs 4 Health https://github.com/balima78/lab.heads This is the repository for a Shiny app developed for the discipline Laboratory Project from HEADS doctoral program. The app is available from here: https://balima.shinyapps.io/lab_heads/ "],
["references.html", "References", " References "]
]
