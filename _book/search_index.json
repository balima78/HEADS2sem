[
["index.html", "HEADS 2nd semester", " HEADS 2nd semester Bruno A Lima 2020-02-09 "],
["preface.html", "Preface", " Preface This is a book written in Markdown through RStudio. The bookdown package (Xie 2015) can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) In this book, I will try to compile all the homeworks from the 2nd semester 2019/2020 of HEADS PhD programme. An exhaustive explanation using the bookdown package (Xie 2019) can be found at bookdown: Authoring Books and Technical Documents with R Markdown and this is only a sample book, which was built on top of R Markdown and knitr This book is publish through NETLIFY as described by C.M. References "],
["intro.html", "1 Introduction", " 1 Introduction All the files and code of this book are available in the repository: https://github.com/balima78/HEADS2sem. All the students can contribute to this compilation sending me their own homeworks and correcting the mistakes I will certainly do. You can send me your files and code to balima78@gmail.com. When possible you must send me .Rmd files and data as .csv. You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 1. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter @ref(top.hida). Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 1.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 1.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 1.1. knitr::kable( head(iris, 6), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 1.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2019) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["compstat.html", "2 COMPSTAT", " 2 COMPSTAT Estatística Computacional Conteúdos programáticos Estatística computacional Porque usar computação em estatística? Ferramentas e software para estatística computacional Estatística computacional utilizando grandes infra-estruturas de dados Sinopses de dados Estatísticas suficientes Histogramas Micro-Clusters Fading Statistics Estimativas de densidade Máxima verosimilhança Expectation-Maximization Kernel Estimation Estimativas e Simulação Métodos Jackknife Validação cruzada Geração de números aleatórios Métodos de Monte Carlo Métodos de Bootstrap Análise numérica Visualização de dados complexos Análise de componentes principais Bivariate smoothing Splines "],
["lesson-1.html", "2.1 Lesson 1", " 2.1 Lesson 1 2020-02-03 Exercises Solutions (just copied teacher’s solutions) Open the database Anesthesia-BD.csv data = read.csv(&quot;2.UploadedData/Anesthesia-BD.csv&quot;) ###load dat str(data) ###see data structure ## &#39;data.frame&#39;: 387 obs. of 9 variables: ## $ Subject : Factor w/ 387 levels &quot;Sbj001&quot;,&quot;Sbj002&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 1 2 2 2 2 2 2 1 ... ## $ Age : int 50 67 71 51 69 78 70 58 80 63 ... ## $ TypeSurgery : Factor w/ 3 levels &quot;CAB&quot;,&quot;CAB + Valve&quot;,..: 3 1 1 3 1 2 1 3 3 2 ... ## $ SurgStatus : Factor w/ 2 levels &quot;Elective&quot;,&quot;Urgent&quot;: 1 1 2 1 2 2 1 2 1 2 ... ## $ Diastolic : num 45.5 52.6 56.7 71.4 58.1 ... ## $ Systolic : num 112.1 109 98.2 150.8 131.5 ... ## $ Cross_Clamp_Time: int 83 67 83 51 63 114 94 138 42 172 ... ## $ AdvEvent : int 0 0 0 0 1 0 0 0 0 0 ... Factor variable AdvEvent data$AdvEvent &lt;- factor(data$AdvEvent, levels = c(0,1),labels=c(&quot;No&quot;,&quot;Yes&quot;)) For the columns which contain numeric values, create a new summary table in which the rows are the mean, the standard deviation, and the median of each of the numeric columns. T &lt;- sapply(data[,sapply(data,is.numeric)],function(x){ return(rbind(mean(x,na.rm = T),sd(x,na.rm = T),median(x,na.rm = T))) } ) rownames(T) = c(&quot;Mean&quot;,&quot;SD&quot;,&quot;Median&quot;) T ## Age Diastolic Systolic Cross_Clamp_Time ## Mean 67.27390 60.93076 132.04705 72.00521 ## SD 11.25574 11.35387 21.22864 32.04680 ## Median 67.00000 60.52591 130.00410 67.00000 Considering only the subjects who had CAB or Valve surgery, how many had adverse events? How many males and females in the two groups (having or not having adverse events)? (count and percentage) # create an index with the observation that comply with the conditions ind = which(data$TypeSurgery==&quot;CAB&quot; | data$TypeSurgery==&quot;Valve&quot; ) # table these data counts table(data[ind,&quot;AdvEvent&quot;]) ## ## No Yes ## 231 80 # and frequencies prop.table(table(data[ind,&quot;AdvEvent&quot;])) ## ## No Yes ## 0.7427653 0.2572347 # now by gender table(data[ind,&quot;Gender&quot;],data[ind,&quot;AdvEvent&quot;]) ## ## No Yes ## Female 60 21 ## Male 171 59 # and their frequencies prop.table(table(data[ind,&quot;Gender&quot;],data[ind,&quot;AdvEvent&quot;])) ## ## No Yes ## Female 0.19292605 0.06752412 ## Male 0.54983923 0.18971061 4.Merge the two datasets (Anesthesia-BD.csv and Anesthesia-BD2.csv) by the subject identification number. data2 = read.csv(&quot;2.UploadedData/Anesthesia-BD2.csv&quot;) ###load data Anesthesia-BD2.csv # I had to rename the first column, I don&#39;t know why names(data2)[1]&lt;-&quot;SubjectID&quot; str(data2) ## &#39;data.frame&#39;: 363 obs. of 7 variables: ## $ SubjectID : Factor w/ 363 levels &quot;Sbj001&quot;,&quot;Sbj002&quot;,..: 3 25 30 32 36 39 46 56 60 63 ... ## $ Diabetes : int 0 0 1 1 0 0 0 1 0 0 ... ## $ RFLastA1cLevel : num 5.7 5.6 7.1 6.7 6.7 6.3 6 6.5 5.3 4.9 ... ## $ ChronicLungDisease: Factor w/ 4 levels &quot;Mild&quot;,&quot;Moderate&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Hypertension : int 1 1 1 1 1 1 1 1 1 1 ... ## $ CHF : int 0 1 1 1 1 1 1 0 1 1 ... ## $ Euroscore : Factor w/ 330 levels &quot;#N/A&quot;,&quot;0.501743134&quot;,..: 8 64 4 92 249 36 195 125 214 224 ... The merge must be such that only the common subjects should be present in the final database (natural join). Ma = merge(data,data2,by.x = &quot;Subject&quot;,by.y=&quot;SubjectID&quot;,all = F) dim(Ma) ## [1] 363 15 The merge must be such that if some subject is not in one of the databases, the subject should be in the database, and missing information must be not available (full outer join). Mb = merge(data,data2,by.x = &quot;Subject&quot;,by.y=&quot;SubjectID&quot;,all = T) dim(Mb) ## [1] 387 15 Consider the following statement: “For people without diabetes, the normal range for the hemoglobin A1c level is between 4% and 5.6%. Hemoglobin A1c levels between 5.7% and 6.4% mean you have a higher chance of getting diabetes. Levels of 6.5% or higher mean you have diabetes.” Create a new variable with three factors (normal, prediabetes, and diabetes), taking into consideration the values the A1c levels. Compare the results obtained with the variable Diabetes (assuming that 1 means to have diabetes and 0 no diabetes). diab &lt;- ifelse(Mb$RFLastA1cLevel&lt;5.7,0,ifelse(Mb$RFLastA1cLevel&lt;6.4,1,2)) Mb$Diabetes2 &lt;- factor(x = diab, levels = 0:2, labels =c(&quot;Normal&quot;,&quot;PreDiabetes&quot;,&quot;Diabetes&quot;)) Mb$Diabetes &lt;- factor(Mb$Diabetes, levels=0:1, labels=c(&quot;Normal&quot;,&quot;Diabetes&quot;)) table(Mb$Diabetes,Mb$Diabetes2) ## ## Normal PreDiabetes Diabetes ## Normal 120 99 11 ## Diabetes 7 35 83 Create a table with the comparison between the groups having or not having adverse events for all the variables available in the combined database. Use the appropriate measures for each type of variable. str(Mb) ## &#39;data.frame&#39;: 387 obs. of 16 variables: ## $ Subject : Factor w/ 387 levels &quot;Sbj001&quot;,&quot;Sbj002&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 1 2 2 2 2 2 2 1 ... ## $ Age : int 50 67 71 51 69 78 70 58 80 63 ... ## $ TypeSurgery : Factor w/ 3 levels &quot;CAB&quot;,&quot;CAB + Valve&quot;,..: 3 1 1 3 1 2 1 3 3 2 ... ## $ SurgStatus : Factor w/ 2 levels &quot;Elective&quot;,&quot;Urgent&quot;: 1 1 2 1 2 2 1 2 1 2 ... ## $ Diastolic : num 45.5 52.6 56.7 71.4 58.1 ... ## $ Systolic : num 112.1 109 98.2 150.8 131.5 ... ## $ Cross_Clamp_Time : int 83 67 83 51 63 114 94 138 42 172 ... ## $ AdvEvent : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 2 1 1 1 1 1 ... ## $ Diabetes : Factor w/ 2 levels &quot;Normal&quot;,&quot;Diabetes&quot;: 2 1 1 1 1 1 1 1 1 2 ... ## $ RFLastA1cLevel : num 7.9 5.5 5.7 6 5.8 NA 5.9 6 5.2 11.2 ... ## $ ChronicLungDisease: Factor w/ 4 levels &quot;Mild&quot;,&quot;Moderate&quot;,..: 3 3 1 3 3 3 3 3 3 3 ... ## $ Hypertension : int 1 1 1 0 1 0 1 1 1 0 ... ## $ CHF : int 1 0 0 0 0 1 1 1 1 1 ... ## $ Euroscore : Factor w/ 330 levels &quot;#N/A&quot;,&quot;0.501743134&quot;,..: 47 206 8 289 159 96 119 98 77 4 ... ## $ Diabetes2 : Factor w/ 3 levels &quot;Normal&quot;,&quot;PreDiabetes&quot;,..: 3 1 2 2 2 NA 2 2 1 3 ... Mb$Hypertension &lt;- factor(Mb$Hypertension, levels=0:1, labels=c(&quot;No&quot;,&quot;Yes&quot;)) Mb$CHF &lt;- factor(Mb$CHF, levels=0:1, labels=c(&quot;No&quot;,&quot;Yes&quot;)) Mb$Subject &lt;- as.character(Mb$Subject) #windows() par(mfrow=c(2,3)) for (i in which(sapply(Mb,is.numeric))){ hist(Mb[,i],xlab=colnames(Mb)[i]) } par(mfrow=c(1,1)) table &lt;- by(Mb[,which(sapply(Mb,is.numeric))], INDICES = Mb$AdvEvent, FUN = function(x) apply(x,2,quantile,na.rm=T)) table ## Mb$AdvEvent: No ## Age Diastolic Systolic Cross_Clamp_Time RFLastA1cLevel ## 0% 25 26.42047 88.70903 15 4.7 ## 25% 59 53.30694 115.03977 51 5.6 ## 50% 67 60.75382 129.50300 68 5.8 ## 75% 76 68.64874 143.12238 83 6.4 ## 100% 92 94.35495 180.95547 241 12.2 ## -------------------------------------------------------- ## Mb$AdvEvent: Yes ## Age Diastolic Systolic Cross_Clamp_Time RFLastA1cLevel ## 0% 24 39.42266 92.34727 13.0 4.600 ## 25% 62 52.93295 119.55877 48.0 5.500 ## 50% 69 59.94571 131.48461 64.0 5.800 ## 75% 77 65.97349 153.50990 88.5 6.425 ## 100% 88 92.51766 193.75080 298.0 11.700 table$pvalue &lt;- apply(Mb[,which(sapply(Mb,is.numeric))],2, function(x) wilcox.test(x[which(Mb$AdvEvent==&quot;No&quot;)],x[which(Mb$AdvEvent==&quot;Yes&quot;)])$p.value) table ## Age Diastolic Systolic Cross_Clamp_Time RFLastA1cLevel ## 0% 25 26.42047 88.70903 15 4.7 ## 25% 59 53.30694 115.03977 51 5.6 ## 50% 67 60.75382 129.50300 68 5.8 ## 75% 76 68.64874 143.12238 83 6.4 ## 100% 92 94.35495 180.95547 241 12.2 ## -------------------------------------------------------- ## Age Diastolic Systolic Cross_Clamp_Time RFLastA1cLevel ## 0% 24 39.42266 92.34727 13.0 4.600 ## 25% 62 52.93295 119.55877 48.0 5.500 ## 50% 69 59.94571 131.48461 64.0 5.800 ## 75% 77 65.97349 153.50990 88.5 6.425 ## 100% 88 92.51766 193.75080 298.0 11.700 ## -------------------------------------------------------- ## Age Diastolic Systolic Cross_Clamp_Time ## 0.09297764 0.41375034 0.09079886 0.76575185 ## RFLastA1cLevel ## 0.78804579 Mb$Euroscore&lt;-as.character(Mb$Euroscore) table2 &lt;- by(Mb[,which(sapply(Mb,is.factor))], INDICES = Mb$AdvEvent, FUN = function(x){sapply(x,table)}) table2 ## Mb$AdvEvent: No ## $Gender ## ## Female Male ## 78 200 ## ## $TypeSurgery ## ## CAB CAB + Valve Valve ## 156 47 75 ## ## $SurgStatus ## ## Elective Urgent ## 148 130 ## ## $AdvEvent ## ## No Yes ## 278 0 ## ## $Diabetes ## ## Normal Diabetes ## 170 93 ## ## $ChronicLungDisease ## ## Mild Moderate No Severe ## 34 4 226 1 ## ## $Hypertension ## ## No Yes ## 49 215 ## ## $CHF ## ## No Yes ## 186 79 ## ## $Diabetes2 ## ## Normal PreDiabetes Diabetes ## 91 100 70 ## ## -------------------------------------------------------- ## Mb$AdvEvent: Yes ## $Gender ## ## Female Male ## 31 78 ## ## $TypeSurgery ## ## CAB CAB + Valve Valve ## 41 29 39 ## ## $SurgStatus ## ## Elective Urgent ## 64 45 ## ## $AdvEvent ## ## No Yes ## 0 109 ## ## $Diabetes ## ## Normal Diabetes ## 65 33 ## ## $ChronicLungDisease ## ## Mild Moderate No Severe ## 16 0 79 3 ## ## $Hypertension ## ## No Yes ## 20 78 ## ## $CHF ## ## No Yes ## 50 47 ## ## $Diabetes2 ## ## Normal PreDiabetes Diabetes ## 36 35 25 "],
["stats.html", "3 STATS", " 3 STATS Modelação Estatística Conteúdos programáticos Regressão logística. Análise de sobrevida. Análise de dados longitudinais. Causalidade. "],
["learn.html", "4 LEARN", " 4 LEARN Macine Learning Conteúdos programáticos O processo de extração de conhecimento de dados Compreensão do negócio Compreensão dos dados Pré-processamento de dados Modelação de dados Avaliação de modelos de extração de conhecimento de dados Implantação prática dos resultados de modelação Aprendizagem automática Aprender conceitos a partir de dados Processos indutivos vs processos dedutivos Viés indutivo Validação de modelos Medidas de erro e processos de estimação Aprendizagem supervisionada Árvores de decisão Redes Bayesianas Redes neuronais Deep learning e análise de grandes bases de dados Aprendizagem não supervisionada Análise de clusters Deteção de casos extremos e anomalias Associação e análise de padrões frequentes Interpretação de grandes bases de dados "],
["lesson-1-1.html", "4.1 Lesson 1", " 4.1 Lesson 1 2020-02-04 This exercise was done as described by: https://machinelearningmastery.com/machine-learning-in-r-step-by-step/ but using a data set from: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra# Read the data dataset &lt;- read.csv(&quot;2.UploadedData/dataR2.csv&quot;) Factor variable Classification dataset$Classification&lt;-as.factor(dataset$Classification) NOTE: in order to use the caret packages in all its’ capabilities you must do install.packages(&quot;caret&quot;, dependencies = T) create a validation dataset and use the remaing for training library(caret) ## Loading required package: lattice ## Loading required package: ggplot2 # create a list of 80% of the rows in the original dataset we can use for training validation_index &lt;- createDataPartition(dataset$Classification, p=0.80, list=FALSE) # select 20% of the data for validation validation &lt;- dataset[-validation_index,] # use the remaining 80% of data to training and testing the models dataset &lt;- dataset[validation_index,] dimensions of the new dataset dim(dataset) ## [1] 94 10 list types for each attribute sapply(dataset, class) ## Age BMI Glucose Insulin HOMA ## &quot;integer&quot; &quot;numeric&quot; &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; ## Leptin Adiponectin Resistin MCP.1 Classification ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;factor&quot; take a peek at the first 5 rows of the data head(dataset) ## Age BMI Glucose Insulin HOMA Leptin Adiponectin Resistin ## 2 83 20.69049 92 3.115 0.7068973 8.8438 5.429285 4.06405 ## 3 82 23.12467 91 4.498 1.0096511 17.9393 22.432040 9.27715 ## 4 68 21.36752 77 3.226 0.6127249 9.8827 7.169560 12.76600 ## 5 86 21.11111 92 3.549 0.8053864 6.6994 4.819240 10.57635 ## 6 49 22.85446 92 3.226 0.7320869 6.8317 13.679750 10.31760 ## 7 89 22.70000 77 4.690 0.8907873 6.9640 5.589865 12.93610 ## MCP.1 Classification ## 2 468.786 1 ## 3 554.697 1 ## 4 928.220 1 ## 5 773.920 1 ## 6 530.410 1 ## 7 1256.083 1 list the levels for the class levels(dataset$Classification) ## [1] &quot;1&quot; &quot;2&quot; summarize the class distribution percentage &lt;- prop.table(table(dataset$Classification)) * 100 cbind(freq=table(dataset$Classification), percentage=percentage) ## freq percentage ## 1 42 44.68085 ## 2 52 55.31915 summarize attribute distributions summary(dataset) ## Age BMI Glucose Insulin ## Min. :24.0 Min. :18.37 Min. : 60.00 Min. : 2.432 ## 1st Qu.:44.0 1st Qu.:22.86 1st Qu.: 85.25 1st Qu.: 4.367 ## Median :53.5 Median :26.84 Median : 92.50 Median : 5.925 ## Mean :56.8 Mean :27.24 Mean : 98.40 Mean :10.243 ## 3rd Qu.:70.5 3rd Qu.:31.15 3rd Qu.:102.00 3rd Qu.:10.539 ## Max. :89.0 Max. :38.58 Max. :201.00 Max. :58.460 ## HOMA Leptin Adiponectin Resistin ## Min. : 0.5079 Min. : 4.47 Min. : 2.194 Min. : 3.210 ## 1st Qu.: 0.9314 1st Qu.:11.50 1st Qu.: 5.512 1st Qu.: 6.745 ## Median : 1.3933 Median :18.43 Median : 8.353 Median :11.230 ## Mean : 2.7983 Mean :26.04 Mean :10.243 Mean :15.124 ## 3rd Qu.: 2.6326 3rd Qu.:38.57 3rd Qu.:12.050 3rd Qu.:20.056 ## Max. :25.0503 Max. :90.28 Max. :36.060 Max. :82.100 ## MCP.1 Classification ## Min. : 63.61 1:42 ## 1st Qu.: 303.91 2:52 ## Median : 501.24 ## Mean : 541.72 ## 3rd Qu.: 710.92 ## Max. :1698.44 split input and output x &lt;- dataset[,1:9] y &lt;- dataset[,10] boxplot for each attribute on one image par(mfrow=c(2,5)) for(i in 1:9) { boxplot(x[,i], main=names(dataset)[i]) } barplot for class breakdown par(mfrow=c(1,1)) plot(y) scatterplot matrix featurePlot(x=x, y=y, plot=&quot;ellipse&quot;) box and whisker plots for each attribute featurePlot(x=x, y=y, plot=&quot;box&quot;) density plots for each attribute by class value scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) featurePlot(x=x, y=y, plot=&quot;density&quot;, scales=scales) Run algorithms using 10-fold cross validation control &lt;- trainControl(method=&quot;cv&quot;, number=10) metric &lt;- &quot;Accuracy&quot; linear algorithms set.seed(7) fit.lda &lt;- train(Classification~., data=dataset, method=&quot;lda&quot;, metric=metric, trControl=control) nonlinear algorithms CART set.seed(7) fit.cart &lt;- train(Classification~., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) kNN set.seed(7) fit.knn &lt;- train(Classification~., data=dataset, method=&quot;knn&quot;, metric=metric, trControl=control) advanced algorithms SVM set.seed(7) fit.svm &lt;- train(Classification~., data=dataset, method=&quot;svmRadial&quot;, metric=metric, trControl=control) Random Forest set.seed(7) fit.rf &lt;- train(Classification~., data=dataset, method=&quot;rf&quot;, metric=metric, trControl=control) summarize accuracy of models results &lt;- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf)) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: lda, cart, knn, svm, rf ## Number of resamples: 10 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## lda 0.6000000 0.6666667 0.7777778 0.7662626 0.8136364 1.0000000 0 ## cart 0.5555556 0.5757576 0.6666667 0.6691919 0.6916667 0.8888889 0 ## knn 0.3333333 0.4444444 0.5555556 0.5584848 0.6666667 0.8181818 0 ## svm 0.6666667 0.7194444 0.7777778 0.7722222 0.7777778 1.0000000 0 ## rf 0.5555556 0.6166667 0.7777778 0.7318182 0.8080808 0.9000000 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## lda 0.2000000 0.3161351 0.56071429 0.52818883 0.6377171 1.0000000 0 ## cart 0.0000000 0.1738095 0.30769231 0.32913041 0.4186992 0.7804878 0 ## knn -0.4210526 -0.1538462 0.07631579 0.08344325 0.3076923 0.6451613 0 ## svm 0.3076923 0.4375000 0.55000000 0.54145403 0.5500000 1.0000000 0 ## rf 0.1000000 0.2269231 0.53815789 0.45478292 0.6125000 0.8000000 0 compare accuracy of models dotplot(results) summarize Best Model print(fit.rf) ## Random Forest ## ## 94 samples ## 9 predictor ## 2 classes: &#39;1&#39;, &#39;2&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 85, 85, 83, 84, 84, 85, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.7309091 0.4539833 ## 5 0.7318182 0.4547829 ## 9 0.7318182 0.4536009 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 5. estimate skill of LDA on the validation dataset predictions &lt;- predict(fit.rf, validation) confusionMatrix(predictions, validation$Classification) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 2 ## 1 5 5 ## 2 5 7 ## ## Accuracy : 0.5455 ## 95% CI : (0.3221, 0.7561) ## No Information Rate : 0.5455 ## P-Value [Acc &gt; NIR] : 0.5869 ## ## Kappa : 0.0833 ## ## Mcnemar&#39;s Test P-Value : 1.0000 ## ## Sensitivity : 0.5000 ## Specificity : 0.5833 ## Pos Pred Value : 0.5000 ## Neg Pred Value : 0.5833 ## Prevalence : 0.4545 ## Detection Rate : 0.2273 ## Detection Prevalence : 0.4545 ## Balanced Accuracy : 0.5417 ## ## &#39;Positive&#39; Class : 1 ## In this post you discovered step-by-step how to complete your first machine learning project in R "],
["bayes.html", "5 BAYES", " 5 BAYES Modelação Estatística Bayesiana Conteúdos programáticos Inferência Bayesiana: Introdução à inferência Bayesiana: Probabilidade e parâmetros; Inferência frequentista clássica versus inferência Bayesiana; Fundamentos da inferência Bayesiana; Distribuições a priori; Distribuições a posteriori; Distribuições preditivas a posteriori; Modelos Bayesianos básicos; Modelação hierárquica; Avaliação dos modelos. Construção de modelos de inferência Bayesiana: Inferência Bayesiana com distribuições a priori conjugadas; Computação Bayesiana – métodos de Monte Carlo, métodos de Monte Carlo via Cadeias de Markov (MCMC), algoritmo de Metropolis-Hastings, algoritmo de Gibbs e outros algoritmos relacionados; Métodos de avaliação da qualidade dos modelos (escolha de valores iniciais, convergência, eficiência e precisão); Métodos de selecção de modelos; Aplicação dos métodos de inferência Bayesiana a problemas mais comuns de inferência estatística – modelos de regressão, análise de dados categóricos e modelos de síntese de evidência. Redes Bayesianas: Introdução às Redes Bayesianas: Motivação e exemplos; Probabilidade e aplicações médicas; Modelos gráficos de probabilidade; Semântica e factorização nas redes Bayesianas. Construção de redes Bayesianas a partir de dados: Aprendizagem automática; Estimação de parâmetros de redes Bayesianas; Aprendizagem da estrutura de redes Bayesianas; Aprendizagem com dados incompletos. "],
["top-hida.html", "6 TOP.HIDA", " 6 TOP.HIDA Tópicos avançados em HIDA Conteúdos programáticos Os conteúdos abrangerão as diversas áreas da análise inteligente de dados, nomeadamente metodologias de visualização, pré-processamento, exploração e classificação de dados, proporcionando também a expansão dos conteúdos de outras UC relacionadas. A escolha dos tópicos será feita em cada edição desta unidade curricular, tendo em consideração não só o perfil e interesses dos alunos, como também os tópicos que no momento se considerem ser os mais relevantes na área. "],
["seminar1.html", "6.1 Seminar 1", " 6.1 Seminar 1 2020-02-04 Physiological signal processing in affective computing by Susana Brás Susana Brás is Postdoctoral researcher at Institute of Electronics and Telematics Engineering of Aveiro (IEETA), University of Aveiro, Portugal. At this moment, she is focused on ECG biometric identification, emotional modulated environments and information extraction from large biomedical databases. Before, she was a PhD student at Abel Salazar Biomedical Sciences Institute, at University of Porto, Portugal. Her thesis was focused on automation in anesthesia, basically the drug distribution and effect (alterations in the brain electrical activity) were studied, modeled and a controller was presented for the hypnotic effect. Her background is in Applied Mathematics, from the Sciences Faculty, at University of Porto. 6.1.1 Bruno Lima Podemos definir sinais como dados indexados a uma variável temporal e que podem ser oriundos de campos tão diversos como a biomedicina ou as comunicações sem fios. Neste seminário foi-nos apresentado o conceito de computação afectiva que consiste em identificar emoções a partir do processamento de sinais fisiológicos. O conceito de emoção é de difícil definição ainda que a caracterização das emoções seja algo mais perceptível. Conseguimos reconhecer emoções através de respostas fisiológicas como sejam o batimento cardíaco, a expressão facial, a tensão, o riso, a rubescência ou a sudação embora estas reacções do corpo variem de pessoa para pessoa. Com a ajuda de disciplinas como a psicofisiologia ou a psiconeuroimunologia pretende-se fazer a caracterização das emoções que permitam treinar algoritmos que identifiquem essas emoções e que assim auxiliem sistemas a tomar respostas adequadas às emoções identificadas. O desafio é o de criar um sistema que reconheça, interprete, processe ou simule afectos humanos. Esta computação afectiva procurará ser uma ajuda perante diferentes estados emocionais. Podemos imaginar uma robot que seja de facto uma companhia para os humanos e um prestador de cuidados emocionais. A criação de um sistema como descrito está sujeito, não só, a grandes desafios tecnológicos (como sejam: a monitorização, armazenamento, processamento, data mining e machine learning) mas também desafios éticos. Com a identificação de emoções identificam-se também preferências e consequentemente surge a possibilidade de manipular essas preferências. No campo da saúde, a chamada ‘emotional analytics’ permite prever diagnósticos perante a identificação de emoções e consequentemente actuar preventivamente minimizando danos mais graves. Em campos como a saúde mental ou a saúde geriátrica a correcta classificação das emoções dos doentes é a chave para prestar os melhores cuidados de saúde. "],
["references.html", "References", " References "]
]
