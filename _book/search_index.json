[
["index.html", "HEADS 2nd semester", " HEADS 2nd semester Bruno A Lima 2020-02-26 "],
["preface.html", "Preface", " Preface This is a book written in Markdown through RStudio. The bookdown package (Xie 2015) can be installed from CRAN or Github: install.packages(&quot;bookdown&quot;) # or the development version # devtools::install_github(&quot;rstudio/bookdown&quot;) In this book, I will try to compile all the homeworks from the 2nd semester 2019/2020 of HEADS PhD programme. An exhaustive explanation using the bookdown package (Xie 2019) can be found at bookdown: Authoring Books and Technical Documents with R Markdown and this is only a sample book, which was built on top of R Markdown and knitr This book is publish through NETLIFY as described by C.M. References "],
["intro.html", "1 Introduction", " 1 Introduction All the files and code of this book are available in the repository: https://github.com/balima78/HEADS2sem. All the students can contribute to this compilation sending me their own homeworks and correcting the mistakes I will certainly do. You can send me your files and code to balima78@gmail.com. When possible you must send me .Rmd files and data as .csv. Here, .Rmd files are named by number from 1000 to 9000 (CLLS - Class LLesson Student) followed by a breave description. You can label chapter and section titles using {#label} after them, e.g., we can reference Chapter 6. If you do not manually label them, there will be automatic labels anyway, e.g., Chapter 2. Figures and tables with captions will be placed in figure and table environments, respectively. par(mar = c(4, 4, .1, .1)) plot(pressure, type = &#39;b&#39;, pch = 19) Figure 1.1: Here is a nice figure! Reference a figure by its code chunk label with the fig: prefix, e.g., see Figure 1.1. Similarly, you can reference tables generated from knitr::kable(), e.g., see Table 1.1. knitr::kable( head(iris, 6), caption = &#39;Here is a nice table!&#39;, booktabs = TRUE ) Table 1.1: Here is a nice table! Sepal.Length Sepal.Width Petal.Length Petal.Width Species 5.1 3.5 1.4 0.2 setosa 4.9 3.0 1.4 0.2 setosa 4.7 3.2 1.3 0.2 setosa 4.6 3.1 1.5 0.2 setosa 5.0 3.6 1.4 0.2 setosa 5.4 3.9 1.7 0.4 setosa You can write citations, too. For example, we are using the bookdown package (Xie 2019) in this sample book, which was built on top of R Markdown and knitr (Xie 2015). References "],
["compstat.html", "2 COMPSTAT", " 2 COMPSTAT Estatística Computacional Conteúdos programáticos Estatística computacional Porque usar computação em estatística? Ferramentas e software para estatística computacional Estatística computacional utilizando grandes infra-estruturas de dados Sinopses de dados Estatísticas suficientes Histogramas Micro-Clusters Fading Statistics Estimativas de densidade Máxima verosimilhança Expectation-Maximization Kernel Estimation Estimativas e Simulação Métodos Jackknife Validação cruzada Geração de números aleatórios Métodos de Monte Carlo Métodos de Bootstrap Análise numérica Visualização de dados complexos Análise de componentes principais Bivariate smoothing Splines "],
["lesson-1.html", "2.1 Lesson 1", " 2.1 Lesson 1 2020-02-03 Exercises Solutions (just copied teacher’s solutions) Open the database Anesthesia-BD.csv data = read.csv(&quot;2.UploadedData/Anesthesia-BD.csv&quot;) ###load dat str(data) ###see data structure ## &#39;data.frame&#39;: 387 obs. of 9 variables: ## $ Subject : Factor w/ 387 levels &quot;Sbj001&quot;,&quot;Sbj002&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 1 2 2 2 2 2 2 1 ... ## $ Age : int 50 67 71 51 69 78 70 58 80 63 ... ## $ TypeSurgery : Factor w/ 3 levels &quot;CAB&quot;,&quot;CAB + Valve&quot;,..: 3 1 1 3 1 2 1 3 3 2 ... ## $ SurgStatus : Factor w/ 2 levels &quot;Elective&quot;,&quot;Urgent&quot;: 1 1 2 1 2 2 1 2 1 2 ... ## $ Diastolic : num 45.5 52.6 56.7 71.4 58.1 ... ## $ Systolic : num 112.1 109 98.2 150.8 131.5 ... ## $ Cross_Clamp_Time: int 83 67 83 51 63 114 94 138 42 172 ... ## $ AdvEvent : int 0 0 0 0 1 0 0 0 0 0 ... Factor variable AdvEvent data$AdvEvent &lt;- factor(data$AdvEvent, levels = c(0,1),labels=c(&quot;No&quot;,&quot;Yes&quot;)) For the columns which contain numeric values, create a new summary table in which the rows are the mean, the standard deviation, and the median of each of the numeric columns. T &lt;- sapply(data[,sapply(data,is.numeric)],function(x){ return(rbind(mean(x,na.rm = T),sd(x,na.rm = T),median(x,na.rm = T))) } ) rownames(T) = c(&quot;Mean&quot;,&quot;SD&quot;,&quot;Median&quot;) T ## Age Diastolic Systolic Cross_Clamp_Time ## Mean 67.27390 60.93076 132.04705 72.00521 ## SD 11.25574 11.35387 21.22864 32.04680 ## Median 67.00000 60.52591 130.00410 67.00000 Considering only the subjects who had CAB or Valve surgery, how many had adverse events? How many males and females in the two groups (having or not having adverse events)? (count and percentage) # create an index with the observation that comply with the conditions ind = which(data$TypeSurgery==&quot;CAB&quot; | data$TypeSurgery==&quot;Valve&quot; ) # table these data counts table(data[ind,&quot;AdvEvent&quot;]) ## ## No Yes ## 231 80 # and frequencies prop.table(table(data[ind,&quot;AdvEvent&quot;])) ## ## No Yes ## 0.7427653 0.2572347 # now by gender table(data[ind,&quot;Gender&quot;],data[ind,&quot;AdvEvent&quot;]) ## ## No Yes ## Female 60 21 ## Male 171 59 # and their frequencies prop.table(table(data[ind,&quot;Gender&quot;],data[ind,&quot;AdvEvent&quot;])) ## ## No Yes ## Female 0.19292605 0.06752412 ## Male 0.54983923 0.18971061 4.Merge the two datasets (Anesthesia-BD.csv and Anesthesia-BD2.csv) by the subject identification number. data2 = read.csv(&quot;2.UploadedData/Anesthesia-BD2.csv&quot;) ###load data Anesthesia-BD2.csv # I had to rename the first column, I don&#39;t know why names(data2)[1]&lt;-&quot;SubjectID&quot; str(data2) ## &#39;data.frame&#39;: 363 obs. of 7 variables: ## $ SubjectID : Factor w/ 363 levels &quot;Sbj001&quot;,&quot;Sbj002&quot;,..: 3 25 30 32 36 39 46 56 60 63 ... ## $ Diabetes : int 0 0 1 1 0 0 0 1 0 0 ... ## $ RFLastA1cLevel : num 5.7 5.6 7.1 6.7 6.7 6.3 6 6.5 5.3 4.9 ... ## $ ChronicLungDisease: Factor w/ 4 levels &quot;Mild&quot;,&quot;Moderate&quot;,..: 1 1 1 1 1 1 1 1 1 1 ... ## $ Hypertension : int 1 1 1 1 1 1 1 1 1 1 ... ## $ CHF : int 0 1 1 1 1 1 1 0 1 1 ... ## $ Euroscore : Factor w/ 330 levels &quot;#N/A&quot;,&quot;0.501743134&quot;,..: 8 64 4 92 249 36 195 125 214 224 ... The merge must be such that only the common subjects should be present in the final database (natural join). Ma = merge(data,data2,by.x = &quot;Subject&quot;,by.y=&quot;SubjectID&quot;,all = F) dim(Ma) ## [1] 363 15 The merge must be such that if some subject is not in one of the databases, the subject should be in the database, and missing information must be not available (full outer join). Mb = merge(data,data2,by.x = &quot;Subject&quot;,by.y=&quot;SubjectID&quot;,all = T) dim(Mb) ## [1] 387 15 Consider the following statement: “For people without diabetes, the normal range for the hemoglobin A1c level is between 4% and 5.6%. Hemoglobin A1c levels between 5.7% and 6.4% mean you have a higher chance of getting diabetes. Levels of 6.5% or higher mean you have diabetes.” Create a new variable with three factors (normal, prediabetes, and diabetes), taking into consideration the values the A1c levels. Compare the results obtained with the variable Diabetes (assuming that 1 means to have diabetes and 0 no diabetes). diab &lt;- ifelse(Mb$RFLastA1cLevel&lt;5.7,0,ifelse(Mb$RFLastA1cLevel&lt;6.4,1,2)) Mb$Diabetes2 &lt;- factor(x = diab, levels = 0:2, labels =c(&quot;Normal&quot;,&quot;PreDiabetes&quot;,&quot;Diabetes&quot;)) Mb$Diabetes &lt;- factor(Mb$Diabetes, levels=0:1, labels=c(&quot;Normal&quot;,&quot;Diabetes&quot;)) table(Mb$Diabetes,Mb$Diabetes2) ## ## Normal PreDiabetes Diabetes ## Normal 120 99 11 ## Diabetes 7 35 83 Create a table with the comparison between the groups having or not having adverse events for all the variables available in the combined database. Use the appropriate measures for each type of variable. str(Mb) ## &#39;data.frame&#39;: 387 obs. of 16 variables: ## $ Subject : Factor w/ 387 levels &quot;Sbj001&quot;,&quot;Sbj002&quot;,..: 1 2 3 4 5 6 7 8 9 10 ... ## $ Gender : Factor w/ 2 levels &quot;Female&quot;,&quot;Male&quot;: 2 2 1 2 2 2 2 2 2 1 ... ## $ Age : int 50 67 71 51 69 78 70 58 80 63 ... ## $ TypeSurgery : Factor w/ 3 levels &quot;CAB&quot;,&quot;CAB + Valve&quot;,..: 3 1 1 3 1 2 1 3 3 2 ... ## $ SurgStatus : Factor w/ 2 levels &quot;Elective&quot;,&quot;Urgent&quot;: 1 1 2 1 2 2 1 2 1 2 ... ## $ Diastolic : num 45.5 52.6 56.7 71.4 58.1 ... ## $ Systolic : num 112.1 109 98.2 150.8 131.5 ... ## $ Cross_Clamp_Time : int 83 67 83 51 63 114 94 138 42 172 ... ## $ AdvEvent : Factor w/ 2 levels &quot;No&quot;,&quot;Yes&quot;: 1 1 1 1 2 1 1 1 1 1 ... ## $ Diabetes : Factor w/ 2 levels &quot;Normal&quot;,&quot;Diabetes&quot;: 2 1 1 1 1 1 1 1 1 2 ... ## $ RFLastA1cLevel : num 7.9 5.5 5.7 6 5.8 NA 5.9 6 5.2 11.2 ... ## $ ChronicLungDisease: Factor w/ 4 levels &quot;Mild&quot;,&quot;Moderate&quot;,..: 3 3 1 3 3 3 3 3 3 3 ... ## $ Hypertension : int 1 1 1 0 1 0 1 1 1 0 ... ## $ CHF : int 1 0 0 0 0 1 1 1 1 1 ... ## $ Euroscore : Factor w/ 330 levels &quot;#N/A&quot;,&quot;0.501743134&quot;,..: 47 206 8 289 159 96 119 98 77 4 ... ## $ Diabetes2 : Factor w/ 3 levels &quot;Normal&quot;,&quot;PreDiabetes&quot;,..: 3 1 2 2 2 NA 2 2 1 3 ... Mb$Hypertension &lt;- factor(Mb$Hypertension, levels=0:1, labels=c(&quot;No&quot;,&quot;Yes&quot;)) Mb$CHF &lt;- factor(Mb$CHF, levels=0:1, labels=c(&quot;No&quot;,&quot;Yes&quot;)) Mb$Subject &lt;- as.character(Mb$Subject) #windows() par(mfrow=c(2,3)) for (i in which(sapply(Mb,is.numeric))){ hist(Mb[,i],xlab=colnames(Mb)[i]) } par(mfrow=c(1,1)) table &lt;- by(Mb[,which(sapply(Mb,is.numeric))], INDICES = Mb$AdvEvent, FUN = function(x) apply(x,2,quantile,na.rm=T)) table ## Mb$AdvEvent: No ## Age Diastolic Systolic Cross_Clamp_Time RFLastA1cLevel ## 0% 25 26.42047 88.70903 15 4.7 ## 25% 59 53.30694 115.03977 51 5.6 ## 50% 67 60.75382 129.50300 68 5.8 ## 75% 76 68.64874 143.12238 83 6.4 ## 100% 92 94.35495 180.95547 241 12.2 ## -------------------------------------------------------- ## Mb$AdvEvent: Yes ## Age Diastolic Systolic Cross_Clamp_Time RFLastA1cLevel ## 0% 24 39.42266 92.34727 13.0 4.600 ## 25% 62 52.93295 119.55877 48.0 5.500 ## 50% 69 59.94571 131.48461 64.0 5.800 ## 75% 77 65.97349 153.50990 88.5 6.425 ## 100% 88 92.51766 193.75080 298.0 11.700 table$pvalue &lt;- apply(Mb[,which(sapply(Mb,is.numeric))],2, function(x) wilcox.test(x[which(Mb$AdvEvent==&quot;No&quot;)],x[which(Mb$AdvEvent==&quot;Yes&quot;)])$p.value) table ## Age Diastolic Systolic Cross_Clamp_Time RFLastA1cLevel ## 0% 25 26.42047 88.70903 15 4.7 ## 25% 59 53.30694 115.03977 51 5.6 ## 50% 67 60.75382 129.50300 68 5.8 ## 75% 76 68.64874 143.12238 83 6.4 ## 100% 92 94.35495 180.95547 241 12.2 ## -------------------------------------------------------- ## Age Diastolic Systolic Cross_Clamp_Time RFLastA1cLevel ## 0% 24 39.42266 92.34727 13.0 4.600 ## 25% 62 52.93295 119.55877 48.0 5.500 ## 50% 69 59.94571 131.48461 64.0 5.800 ## 75% 77 65.97349 153.50990 88.5 6.425 ## 100% 88 92.51766 193.75080 298.0 11.700 ## -------------------------------------------------------- ## Age Diastolic Systolic Cross_Clamp_Time ## 0.09297764 0.41375034 0.09079886 0.76575185 ## RFLastA1cLevel ## 0.78804579 Mb$Euroscore&lt;-as.character(Mb$Euroscore) table2 &lt;- by(Mb[,which(sapply(Mb,is.factor))], INDICES = Mb$AdvEvent, FUN = function(x){sapply(x,table)}) table2 ## Mb$AdvEvent: No ## $Gender ## ## Female Male ## 78 200 ## ## $TypeSurgery ## ## CAB CAB + Valve Valve ## 156 47 75 ## ## $SurgStatus ## ## Elective Urgent ## 148 130 ## ## $AdvEvent ## ## No Yes ## 278 0 ## ## $Diabetes ## ## Normal Diabetes ## 170 93 ## ## $ChronicLungDisease ## ## Mild Moderate No Severe ## 34 4 226 1 ## ## $Hypertension ## ## No Yes ## 49 215 ## ## $CHF ## ## No Yes ## 186 79 ## ## $Diabetes2 ## ## Normal PreDiabetes Diabetes ## 91 100 70 ## ## -------------------------------------------------------- ## Mb$AdvEvent: Yes ## $Gender ## ## Female Male ## 31 78 ## ## $TypeSurgery ## ## CAB CAB + Valve Valve ## 41 29 39 ## ## $SurgStatus ## ## Elective Urgent ## 64 45 ## ## $AdvEvent ## ## No Yes ## 0 109 ## ## $Diabetes ## ## Normal Diabetes ## 65 33 ## ## $ChronicLungDisease ## ## Mild Moderate No Severe ## 16 0 79 3 ## ## $Hypertension ## ## No Yes ## 20 78 ## ## $CHF ## ## No Yes ## 50 47 ## ## $Diabetes2 ## ## Normal PreDiabetes Diabetes ## 36 35 25 "],
["lesson-2.html", "2.2 Lesson 2", " 2.2 Lesson 2 2020-02-10 Classe Exercises (code given by the professor) library(stream) ## Loading required package: proxy ## ## Attaching package: &#39;proxy&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## as.dist, dist ## The following object is masked from &#39;package:base&#39;: ## ## as.matrix #par(mfrow=c(2,2)) keep sample mean definitions itera &lt;- 2000 lbound &lt;- -3 ubound &lt;- 8 mu1 &lt;- 0 mu2 &lt;- 5 set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=0, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=10, sigma=2) iteration # initialization n &lt;- sum &lt;- 0 plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(-3,13), main=&quot;Sample Mean&quot;) # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) sum &lt;- sum + xi n &lt;- n + 1 points(x=i, y=xi, pch=20, cex=.1, col=&quot;black&quot;) points(x=i, y=sum / n, pch=20, cex=.5, col=&quot;red&quot;) } keep sample mean over a sliding window definitions w &lt;- 100 set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=0, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=10, sigma=2) iteration # initialization n &lt;- sum &lt;- 0 plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(-3,13), main=&quot;Sliding Mean&quot;) window &lt;- NULL # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) sum &lt;- sum + xi n &lt;- n + 1 window &lt;- c(window, xi) if (length(window) &gt; w) { sum &lt;- sum - window[1] n &lt;- n - 1 window &lt;- window[-1] } points(x=i, y=xi, pch=20, cex=.1, col=&quot;black&quot;) points(x=i, y=sum / n, pch=20, cex=.5, col=&quot;red&quot;) } keep sample mean over a alpha weighted sliding window definitions w &lt;- 100 eps &lt;- 0.05 alpha &lt;- eps^(1/w) set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=0, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=10, sigma=2) iteration # initialization n &lt;- sum &lt;- 0 plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(-3,13), main=&quot;Weighted Sliding Mean&quot;) window &lt;- NULL # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) sum &lt;- sum * alpha + xi n &lt;- n * alpha + 1 window &lt;- c(window, xi) if (length(window) &gt; w) { sum &lt;- sum - window[1] * alpha ^ (w-1) n &lt;- n - 1 * alpha ^ (w-1) window &lt;- window[-1] } points(x=i, y=xi, pch=20, cex=.1, col=&quot;black&quot;) points(x=i, y=sum / n, pch=20, cex=.5, col=&quot;red&quot;) } keep sample mean over a alpha fading sliding window definitions w &lt;- 100 eps &lt;- 0.05 alpha &lt;- eps^(1/w) set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=0, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=10, sigma=2) iteration # initialization n &lt;- sum &lt;- 0 plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(-3,13), main=&quot;Fading Sliding Mean&quot;) window &lt;- NULL # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) sum &lt;- sum * alpha + xi n &lt;- n * alpha + 1 points(x=i, y=xi, pch=20, cex=.1, col=&quot;black&quot;) points(x=i, y=sum / n, pch=20, cex=.5, col=&quot;red&quot;) } "],
["assignment-histogram.html", "2.3 assignment Histogram", " 2.3 assignment Histogram lesson 2 (cont.) professor’s solution 2020-02-13 Using the stream package in R, keep current sample histogram of a 1-dimensional data stream using different window models. keep sample histogram defenitions library(stream) #par(mfrow=c(2,2)) # definitions itera &lt;- 2000 lbound &lt;- -3 ubound &lt;- 8 nbins &lt;- 10 mu1 &lt;- 0 mu2 &lt;- 5 iteration # initialization set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=mu1, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=mu2, sigma=2) splits &lt;- seq(from=lbound, to=ubound, by=(ubound-lbound)/nbins) splits &lt;- c(splits, +Inf) # extra split for comparison bins &lt;- rep(0, length(splits)) names(bins) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(lbound,ubound), main=&quot;Sample Distribution Heat Map&quot;) cat(&quot;\\n*************************\\n&quot;, &quot;Sample Distribution&quot;,&quot;\\n&quot;) ## ## ************************* ## Sample Distribution # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) bin &lt;- which(xi &lt;= splits)[1] bins[bin] &lt;- bins[bin] + 1 sapply(2:(length(splits)-1), function(u){ upper &lt;- splits[u] lower &lt;- splits[u-1] lines(x=c(i,i), y=c(lower,upper), lwd=2, col=gray(1 - bins[u] / sum(bins))) }) if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(bins) } } ## ## ### Iteration 500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 ## 13 43 90 141 137 66 9 1 0 0 ## &lt;=8 &lt;=Inf ## 0 0 ## ## ### Iteration 1000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 ## 19 64 184 311 279 123 18 2 0 0 ## &lt;=8 &lt;=Inf ## 0 0 ## ## ### Iteration 1500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 ## 19 65 184 311 281 142 73 135 158 85 ## &lt;=8 &lt;=Inf ## 36 11 ## ## ### Iteration 2000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 ## 19 65 184 311 284 159 121 262 309 198 ## &lt;=8 &lt;=Inf ## 71 17 keep sample histogram over a sliding window definitions and iteration # definitions w &lt;- 500 # initialization set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=mu1, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=mu2, sigma=2) splits &lt;- seq(from=lbound, to=ubound, by=(ubound-lbound)/nbins) splits &lt;- c(splits, +Inf) # extra split for comparison bins &lt;- rep(0, length(splits)) names(bins) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) window &lt;- NULL plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(lbound,ubound), main=&quot;Sliding Distribution Heat Map&quot;) cat(&quot;\\n*************************\\n&quot;, &quot;Sliding Distribution&quot;,&quot;\\n&quot;) ## ## ************************* ## Sliding Distribution # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) bin &lt;- which(xi &lt;= splits)[1] bins[bin] &lt;- bins[bin] + 1 window &lt;- c(window, xi) if (length(window) &gt; w) { oldbin &lt;- which(window[1] &lt;= splits)[1] bins[oldbin] &lt;- bins[oldbin] - 1 window &lt;- window[-1] } sapply(2:(length(splits)-1), function(u){ upper &lt;- splits[u] lower &lt;- splits[u-1] lines(x=c(i,i), y=c(lower,upper), lwd=2, col=gray(1 - bins[u] / sum(bins))) }) if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(bins) } } ## ## ### Iteration 500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 ## 13 43 90 141 137 66 9 1 0 0 ## &lt;=8 &lt;=Inf ## 0 0 ## ## ### Iteration 1000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 ## 6 21 94 170 142 57 9 1 0 0 ## &lt;=8 &lt;=Inf ## 0 0 ## ## ### Iteration 1500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 ## 0 1 0 0 2 19 55 133 158 85 ## &lt;=8 &lt;=Inf ## 36 11 ## ## ### Iteration 2000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 ## 0 0 0 0 3 17 48 127 151 113 ## &lt;=8 &lt;=Inf ## 35 6 keep sample histogram over a alpha weighted sliding window definitions w &lt;- 500 eps &lt;- 0.05 alpha &lt;- eps^(1/w) iteration # initialization set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=mu1, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=mu2, sigma=2) splits &lt;- seq(from=lbound, to=ubound, by=(ubound-lbound)/nbins) splits &lt;- c(splits, +Inf) # extra split for comparison bins &lt;- rep(0, length(splits)) names(bins) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) window &lt;- NULL plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(lbound,ubound), main=&quot;Weighted Distribution Heat Map&quot;) cat(&quot;\\n*************************\\n&quot;, &quot;Weighted Distribution&quot;,&quot;\\n&quot;) ## ## ************************* ## Weighted Distribution # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) bin &lt;- which(xi &lt;= splits)[1] bins &lt;- bins * alpha bins[bin] &lt;- bins[bin] + 1 window &lt;- c(window, xi) if (length(window) &gt; w) { oldbin &lt;- which(window[1] &lt;= splits)[1] bins[oldbin] &lt;- bins[oldbin] - alpha^w window &lt;- window[-1] } sapply(2:(length(splits)-1), function(u){ upper &lt;- splits[u] lower &lt;- splits[u-1] lines(x=c(i,i), y=c(lower,upper), lwd=2, col=gray(1 - bins[u] / sum(bins))) }) if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(bins) } } ## ## ### Iteration 500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 ## 3.2949751 16.9824961 29.3163498 44.6117439 41.4464742 19.9554876 ## &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 &lt;=Inf ## 3.2883843 0.1384587 0.0000000 0.0000000 0.0000000 0.0000000 ## ## ### Iteration 1000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 ## 1.8919283 5.7544636 34.7204532 49.2432118 47.7857598 15.7306842 ## &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 &lt;=Inf ## 3.7872339 0.1206349 0.0000000 0.0000000 0.0000000 0.0000000 ## ## ### Iteration 1500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 ## 2.536993e-17 1.959926e-01 -2.532696e-15 3.832639e-15 5.080926e-01 ## &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 ## 7.867248e+00 1.678585e+01 3.859057e+01 5.039598e+01 2.697502e+01 ## &lt;=8 &lt;=Inf ## 1.327758e+01 4.438031e+00 ## ## ### Iteration 2000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 ## 1.268497e-18 -1.087978e-17 -1.266348e-16 1.916320e-16 6.979810e-01 ## &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 ## 5.225829e+00 1.513803e+01 4.131708e+01 5.152468e+01 3.188672e+01 ## &lt;=8 &lt;=Inf ## 1.020274e+01 3.041313e+00 keep sample histogram over a alpha fading window definitions w &lt;- 500 eps &lt;- 0.05 alpha &lt;- eps^(1/w) iteration # initialization set.seed(523) stream1 &lt;- DSD_Gaussians(k=1, d=1, mu=mu1, sigma=2) stream2 &lt;- DSD_Gaussians(k=1, d=1, mu=mu2, sigma=2) splits &lt;- seq(from=lbound, to=ubound, by=(ubound-lbound)/nbins) splits &lt;- c(splits, +Inf) # extra split for comparison bins &lt;- rep(0, length(splits)) names(bins) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) plot(NULL, xlab=&quot;Iteration&quot;,ylab=&quot;Value&quot;, xlim=c(0,itera), ylim=c(lbound,ubound), main=&quot;Fading Distribution Heat Map&quot;) cat(&quot;\\n*************************\\n&quot;, &quot;Fading Distribution&quot;,&quot;\\n&quot;) ## ## ************************* ## Fading Distribution # iterative step for (i in 1:itera){ xi &lt;- ifelse(i&lt;=1000, get_points(stream1)[1,1], get_points(stream2)[1,1]) bin &lt;- which(xi &lt;= splits)[1] bins &lt;- bins * alpha bins[bin] &lt;- bins[bin] + 1 sapply(2:(length(splits)-1), function(u){ upper &lt;- splits[u] lower &lt;- splits[u-1] lines(x=c(i,i), y=c(lower,upper), lwd=2, col=gray(1 - bins[u] / sum(bins))) }) if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(bins) } } ## ## ### Iteration 500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 ## 3.2949751 16.9824961 29.3163498 44.6117439 41.4464742 19.9554876 ## &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 &lt;=Inf ## 3.2883843 0.1384587 0.0000000 0.0000000 0.0000000 0.0000000 ## ## ### Iteration 1000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 ## 2.0566770 6.6035884 36.1862707 51.4737990 49.8580835 16.7284586 ## &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 &lt;=Inf ## 3.9516531 0.1275578 0.0000000 0.0000000 0.0000000 0.0000000 ## ## ### Iteration 1500 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 &lt;=2.5 ## 0.1028339 0.5261720 1.8093135 2.5736899 3.0009968 8.7036712 ## &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 &lt;=8 &lt;=Inf ## 16.9834370 38.5969517 50.3959783 26.9750167 13.2775825 4.4380306 ## ## ### Iteration 2000 ## &lt;=-3 &lt;=-1.9 &lt;=-0.8 &lt;=0.3 &lt;=1.4 ## 0.005141693 0.026308601 0.090465677 0.128684497 0.848030834 ## &lt;=2.5 &lt;=3.6 &lt;=4.7 &lt;=5.8 &lt;=6.9 ## 5.661012898 15.987200946 43.246924516 54.044482016 33.235467534 ## &lt;=8 &lt;=Inf ## 10.866619313 3.263214833 "],
["assignment-matrix.html", "2.4 assignment Matrix", " 2.4 assignment Matrix Assignment 1 (deadline Feb 17) 2020-02-15 Using R, keep counters of equal-width grid-cells (base counters for micro-cluster definitions) of a 2-dimensional continuous data stream using different window models (landmark, sliding, weighted, fading). The code should work with an evolving stream from a single record of the PhysioNet Challenge https://physionetchallenges.github.io/2020/ This training set consists of 6,877 (male: 3,699; female: 3,178) 12-ECG recordings lasting from 6 seconds to 60 seconds. Each recording was sampled at 500 Hz. All data is provided in WFDB format with a MATLAB v4 file and a header containing patient sex, age, and diagnosis (Dx) information at the end of the header file. The code should be applicable to one 12-dimensional record file. library(stream) library(R.matlab) ## R.matlab v3.6.2 (2018-09-26) successfully loaded. See ?R.matlab for help. ## ## Attaching package: &#39;R.matlab&#39; ## The following object is masked from &#39;package:stream&#39;: ## ## evaluate ## The following objects are masked from &#39;package:base&#39;: ## ## getOption, isOpen # after unzip the files, I took the first file fA1&lt;-readMat(&quot;2.UploadedData/A0001.mat&quot;)$val fA1&lt;-t(fA1) I selected two columns (4 and 5) corresponding to the readings and make the streams # select columns a&lt;-4 b&lt;-5 # data stream strA1&lt;-DSD_Memory(fA1[,c(a,b)], loop = T) definitions set.seed(1) reset_stream(strA1) # number of iterations itera &lt;- 2000 #defining bounds for matrix lbound &lt;- min(strA1[[2]]) ubound &lt;- max(strA1[[2]]) nbins &lt;- 10 # first we create two equal size vectors splits &lt;- seq(from=lbound, to=ubound, by=(ubound-lbound)/nbins) splits &lt;- c(splits)[-1] # drop first value binsA &lt;- rep(0, length(splits)) names(binsA) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) binsB &lt;- rep(0, length(splits)) names(binsB) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) # and then a matrix 10x10 mx&lt;-matrix(0,10,10) rownames(mx)&lt;-colnames(mx)&lt;-names(binsA) 2.4.1 keep sample matrix iteration # iterative step for (i in 1:itera){ str&lt;-get_points(strA1) ai &lt;- str[1,1] bi &lt;- str[1,2] binA &lt;- which(ai &lt;= splits)[1] binsA[binA] &lt;- binsA[binA] + 1 binB &lt;- which(bi &lt;= splits)[1] binsB[binB] &lt;- binsB[binB] + 1 mx[binA,binB]&lt;-mx[binA,binB] + 1 # with the vectors positions, we update the matrix counts # printing the two vectors just for control if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(binsA) print(binsB) } } ## ## ### Iteration 500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 38 57 121 209 31 8 16 7 ## &lt;=464.6 &lt;=547 ## 5 8 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 26 11 85 310 58 3 7 0 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 1000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 110 105 237 372 50 25 44 19 ## &lt;=464.6 &lt;=547 ## 18 20 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 56 48 130 538 178 29 17 4 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 1500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 145 140 363 543 102 47 70 29 ## &lt;=464.6 &lt;=547 ## 33 28 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 109 64 165 840 232 52 30 8 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 2000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 204 189 459 720 177 56 83 36 ## &lt;=464.6 &lt;=547 ## 39 37 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 128 78 269 1113 311 56 35 10 ## &lt;=464.6 &lt;=547 ## 0 0 heat map with the values of the matrix heatmap(mx, Colv = NA, Rowv = NA, scale=&quot;none&quot;, symm = F, revC = T, main = &quot;Sample Distribution Heat Map&quot;) 2.4.2 keep sample matrix over a sliding window definitions and initialization set.seed(1) reset_stream(strA1) w &lt;- 500 binsA &lt;- rep(0, length(splits)) names(binsA) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) binsB &lt;- rep(0, length(splits)) names(binsB) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) mx&lt;-matrix(0,10,10) rownames(mx)&lt;-colnames(mx)&lt;-names(binsA) windowA &lt;- windowB &lt;- NULL iteration # iterative step for (i in 1:itera){ str&lt;-get_points(strA1) ai &lt;- str[1,1] bi &lt;- str[1,2] binA &lt;- which(ai &lt;= splits)[1] binsA[binA] &lt;- binsA[binA] + 1 binB &lt;- which(bi &lt;= splits)[1] binsB[binB] &lt;- binsB[binB] + 1 mx[binA,binB]&lt;-mx[binA,binB] + 1 windowA &lt;- c(windowA, ai) # defining two windows windowB &lt;- c(windowB, bi) # one for each stream if (length(windowA) &gt; w) { oldbinA &lt;- which(windowA[1] &lt;= splits)[1] binsA[oldbinA] &lt;- binsA[oldbinA] - 1 windowA &lt;- windowA[-1] oldbinB &lt;- which(windowB[1] &lt;= splits)[1] binsB[oldbinB] &lt;- binsB[oldbinB] - 1 windowB &lt;- windowB[-1] mx[oldbinA,oldbinB]&lt;-mx[oldbinA,oldbinB] - 1 # droping the first value from the window } # printing the two vectors just for control if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(binsA) print(binsB) } } ## ## ### Iteration 500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 38 57 121 209 31 8 16 7 ## &lt;=464.6 &lt;=547 ## 5 8 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 26 11 85 310 58 3 7 0 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 1000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 72 48 116 163 19 17 28 12 ## &lt;=464.6 &lt;=547 ## 13 12 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 30 37 45 228 120 26 10 4 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 1500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 35 35 126 171 52 22 26 10 ## &lt;=464.6 &lt;=547 ## 15 8 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 53 16 35 302 54 23 13 4 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 2000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 59 49 96 177 75 9 13 7 ## &lt;=464.6 &lt;=547 ## 6 9 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 19 14 104 273 79 4 5 2 ## &lt;=464.6 &lt;=547 ## 0 0 heat map with the values of the matrix heatmap(mx, Colv = NA, Rowv = NA, scale=&quot;none&quot;, symm = F, revC = T, main = &quot;Sliding Distribution Heat Map&quot;) 2.4.3 keep sample matrix over a alpha weighted sliding window definitions and initialization set.seed(1) reset_stream(strA1) w &lt;- 500 eps &lt;- 0.05 alpha &lt;- eps^(1/w) # defining an alpha value # initialization set.seed(1) binsA &lt;- rep(0, length(splits)) names(binsA) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) binsB &lt;- rep(0, length(splits)) names(binsB) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) mx&lt;-matrix(0,10,10) rownames(mx)&lt;-colnames(mx)&lt;-names(binsA) windowA &lt;- windowB &lt;- NULL iteration # iterative step for (i in 1:itera){ str&lt;-get_points(strA1) ai &lt;- str[1,1] bi &lt;- str[1,2] binA &lt;- which(ai &lt;= splits)[1] binsA &lt;-binsA * alpha binsA[binA] &lt;- binsA[binA] + 1 binB &lt;- which(bi &lt;= splits)[1] binsB &lt;-binsB * alpha binsB[binB] &lt;- binsB[binB] + 1 mx&lt;-mx * alpha #multiplying the matrix by alpha before updating it mx[binA,binB]&lt;-mx[binA,binB] + 1 # updating the matrix with a new count windowA &lt;- c(windowA, ai) windowB &lt;- c(windowB, bi) if (length(window) &gt; w) { oldbinA &lt;- which(windowA[1] &lt;= splits)[1] binsA[oldbinA] &lt;- binsA[oldbinA] - 1 windowA &lt;- windowA[-1] oldbinB &lt;- which(windowB[1] &lt;= splits)[1] binsB[oldbinB] &lt;- binsB[oldbinB] - 1 windowB &lt;- windowB[-1] mx[oldbinA,oldbinB]&lt;-mx[oldbinA,oldbinB] - alpha^w } if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(binsA) print(binsB) } } ## ## ### Iteration 500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 ## 9.862771 12.716282 40.696190 79.798698 6.004151 1.962992 3.740664 ## &lt;=382.2 &lt;=464.6 &lt;=547 ## 1.514095 1.058584 1.679942 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 ## 6.0146182 2.6308680 22.1702078 118.5148834 7.7376949 0.5858054 ## &lt;=299.8 &lt;=382.2 &lt;=464.6 &lt;=547 ## 1.3802920 0.0000000 0.0000000 0.0000000 ## ## ### Iteration 1000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 ## 26.342501 19.498601 38.162601 57.694790 5.353712 4.551458 6.840510 ## &lt;=382.2 &lt;=464.6 &lt;=547 ## 2.790566 3.056812 2.694537 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 ## 12.4932951 5.5643186 12.3378996 89.6498784 40.6335931 3.9734132 ## &lt;=299.8 &lt;=382.2 &lt;=464.6 &lt;=547 ## 2.1006708 0.2330195 0.0000000 0.0000000 ## ## ### Iteration 1500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 ## 8.272278 12.665316 48.309442 52.748796 11.978027 11.448122 9.207763 ## &lt;=382.2 &lt;=464.6 &lt;=547 ## 4.034632 5.569572 3.149725 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 ## 21.548245 7.717544 11.387921 99.318171 15.379753 5.650347 3.944561 ## &lt;=382.2 &lt;=464.6 &lt;=547 ## 2.437131 0.000000 0.000000 ## ## ### Iteration 2000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 ## 11.103121 12.089897 37.567685 66.923853 29.056388 2.525527 3.223038 ## &lt;=382.2 &lt;=464.6 &lt;=547 ## 1.606848 1.437631 1.869565 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 ## 5.0810228 3.3434787 17.5203610 122.1273764 16.7587452 0.9993678 ## &lt;=299.8 &lt;=382.2 &lt;=464.6 &lt;=547 ## 1.0919750 0.4812265 0.0000000 0.0000000 heat map with the values of the matrix heatmap(mx, Colv = NA, Rowv = NA, scale=&quot;none&quot;, symm = F, revC = T, main = &quot;Weighted Distribution&quot;) 2.4.4 keep sample matrix over a alpha fading window definitions and initialization set.seed(1) reset_stream(strA1) w &lt;- 500 eps &lt;- 0.05 alpha &lt;- eps^(1/w) binsA &lt;- rep(0, length(splits)) names(binsA) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) binsB &lt;- rep(0, length(splits)) names(binsB) &lt;- paste(&quot;&lt;=&quot;,splits,sep=&quot;&quot;) mx&lt;-matrix(0,10,10) rownames(mx)&lt;-colnames(mx)&lt;-names(binsA) iteration # iterative step for (i in 1:itera){ str&lt;-get_points(strA1) ai &lt;- str[1,1] bi &lt;- str[1,2] binA &lt;- which(ai &lt;= splits)[1] binsA[binA] &lt;- binsA[binA] + 1 binB &lt;- which(bi &lt;= splits)[1] binsB[binB] &lt;- binsB[binB] + 1 mx&lt;-mx * alpha # multiplying the matrix by alpha before updating it mx[binA,binB]&lt;-mx[binA,binB] + 1 # updating the matix if (i %% 500 == 0){ cat(&quot;\\n ### Iteration&quot;,i, &quot;\\n&quot;) print(binsA) print(binsB) } } ## ## ### Iteration 500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 38 57 121 209 31 8 16 7 ## &lt;=464.6 &lt;=547 ## 5 8 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 26 11 85 310 58 3 7 0 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 1000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 110 105 237 372 50 25 44 19 ## &lt;=464.6 &lt;=547 ## 18 20 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 56 48 130 538 178 29 17 4 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 1500 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 145 140 363 543 102 47 70 29 ## &lt;=464.6 &lt;=547 ## 33 28 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 109 64 165 840 232 52 30 8 ## &lt;=464.6 &lt;=547 ## 0 0 ## ## ### Iteration 2000 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 204 189 459 720 177 56 83 36 ## &lt;=464.6 &lt;=547 ## 39 37 ## &lt;=-194.6 &lt;=-112.2 &lt;=-29.8 &lt;=52.6 &lt;=135 &lt;=217.4 &lt;=299.8 &lt;=382.2 ## 128 78 269 1113 311 56 35 10 ## &lt;=464.6 &lt;=547 ## 0 0 heat map with the values of the matrix heatmap(mx, Colv = NA, Rowv = NA, scale=&quot;none&quot;, symm = F, revC = T, main = &quot;Fading Distribution&quot;) 2.4.5 Bonus for a diferent solution https://rpubs.com/franzbischoff/hida_assign1 "],
["lesson-3.html", "2.5 Lesson 3", " 2.5 Lesson 3 2020-02-17 Classe Exercises (almost all code given by the professor) knitr::opts_chunk$set(echo = TRUE, warning = F, message = F) 2.5.1 Kernel density estimation data(faithful) hist(faithful$waiting, density = 10, main=&quot;wainting time to next eruption&quot;, xlab=&quot;minutes&quot;, freq=F) lines(density(faithful$waiting),lwd=2,col=4) lines(density(faithful$waiting, bw = 5),lwd=2,col=5) # defining a banwith = 5 in density function lines(density(faithful$waiting, bw = 10),lwd=2,col=6) # defining a banwith = 10 in density function 2.5.2 Conditional density With data from Coimbra Breast Cancer (Glucose | Classification) breast&lt;-read.csv(&quot;2.UploadedData/dataR2.csv&quot;) hist(breast$Glucose, density = 10, main=&quot;Glucose level&quot;, xlab=&quot;&quot;, freq=F, ylim = c(0,0.05)) lines(density(breast$Glucose), lwd=2,col=&quot;black&quot;) lines(density(subset(breast,Classification == 1)$Glucose), lwd=2,col=&quot;green&quot;) lines(density(subset(breast,Classification == 2)$Glucose), lwd=2,col=&quot;red&quot;) legend(130,0.03, legend=c(&quot;P(Glucose)&quot;,&quot;P(Glucose|Control)&quot;,&quot;P(Glucose|Patient)&quot;), col=c(&quot;black&quot;,&quot;green&quot;,&quot;red&quot;), lty = 1, cex = 0.8 ) 2.5.3 Expectation Maximization for the two latent variables Example with Age distribution in the breast cancer data set # read data breast&lt;-read.csv(&quot;2.UploadedData/dataR2.csv&quot;) # Expectation Maximization for the two latent variables (clusters) m&lt;-2 # the total number of clusters (m) set.seed(0) param.mean&lt;-sample(breast$Age,m) # initial parameters param.sd&lt;-rep(sd(breast$Age)/m,m) # initial parameters # E-step (Expectation): that estimates the probability of each point belonging to each cluster xx&lt;-breast$Age ## definition of M-step method alternative &lt;- 1 ## iteraction process itera &lt;-10 # maximum number of iterations eps&lt;-0.01 # minimum error to convergence for (k in 1:itera) { densities&lt;-sapply(1:m, function(i){dnorm(xx, mean=param.mean[i], sd=param.sd[i])}) relevance&lt;-t(apply(densities,1,function(l){l/sum(l)})) attrib&lt;-t(apply(relevance,1, function(l){replace(rep(0,m),which.max(l)[1],1)})) # Re-estimates the parameters of the probability distribution of each class if (alternative == 1) { ## M-step (alternative 1: crisp) param.mean.new&lt;-sapply(1:m, function(j){mean(breast$Age[attrib[,j]==1])}) param.sd.new&lt;-sapply(1:m, function(j){sqrt(sum(relevance[,j]*(breast$Age-param.mean[j])^2)/sum(relevance[,j]))}) } else { ## M-step (alternative 2: soft) param.mean.new&lt;-sapply(1:m, function(j){sum(breast$Age*relevance[,j])/sum(relevance[,j])}) param.sd.new&lt;-sapply(1:m, function(j){sqrt(sum(relevance[,j]*(breast$Age-param.mean[j])^2)/sum(relevance[,j]))}) } # convergence the maximum number of iterations (k). cat(&quot;iteration&quot;, itera, &quot;evolution:&quot;, dif&lt;-sum(abs(param.mean.new - param.mean)), &quot;\\n&quot;) if(dif &lt; eps) break # the accepted error to converge (e) param.mean&lt;-param.mean.new param.sd&lt;-param.sd.new } ## iteration 10 evolution: 13.44278 ## iteration 10 evolution: 9.497791 ## iteration 10 evolution: 4.21416 ## iteration 10 evolution: 0.4824254 ## iteration 10 evolution: 0.4839737 ## iteration 10 evolution: 0 Print the two new density lines with a large banwith # plot histogram and density function for Age hist(breast$Age, density = 20, main=&quot;Glucose level&quot;, xlab=&quot;&quot;, freq=F, ylim = c(0,0.03)) lines(density(breast$Age), lwd=2,col=&quot;black&quot;) set.seed(0) lines(density(rnorm(100, param.mean[1], param.sd[1]), bw = 12),lwd=2,col=5) lines(density(rnorm(100, param.mean[2], param.sd[2]), bw = 12),lwd=2,col=4) 2.5.4 Data Generation Comparing the distribution from data generated from 3 different ways ## length of data set generation n&lt;-100 # original sample data set sample&lt;-breast$Age # bootstrapped sample data set sample.obs&lt;-sample(breast$Age, size=n, replace = TRUE) # gaussian generated data set sample.simple&lt;-rnorm(n,mean=mean(breast$Age), sd=sd(breast$Age)) # EM model generated data set classes&lt;-apply(attrib, 1, which.max) # using attrib define previously, to calculate the number of observations in each class class.prop&lt;-prop.table(table(classes)) # calculate percentage of each class set.seed(523) sample.classes&lt;-sample(1:m, size=n, replace=TRUE, prob=class.prop) sample.prop&lt;-prop.table(table(sample.classes)) sample.values&lt;-round(unlist(sapply(1:m, function(j){ rnorm(n*sample.prop[j], # generating number with the proportions of the two classes we identified previously mean=param.mean[j], # parameters defined previously sd=param.sd[j]) # parameters defined previously })),0) Printing the distribution of the 4 data samples par(mfrow=c(2,2)) hist(breast$Age, freq = F, main = &quot;Observed data&quot;) lines(density(breast$Age)) hist(sample.obs, freq = F, main = &quot;data points sampled from observed data&quot;) lines(density(sample.obs)) hist(sample.simple, freq = F, main = &quot;data points generated from sample distribution&quot;) lines(density(sample.simple)) hist(sample.values, freq = F, main = &quot;data points generated from EM (m=2)&quot;) lines(density(sample.values)) "],
["assignment-glucose.html", "2.6 assignment Glucose", " 2.6 assignment Glucose Assignment from lesson 3 2020-02-26 Using only the base and stats packages in R, define the iterative steps of ExpectationMaximization applied to the Glucose variable of the Breast Cancer Coimbra data set, searching for 2 or more latent classes. 2.6.1 Defining a function for Expectation-Maximization algorithm # function for E-M algorithm emf&lt;-function(xx = vv, # a vector with the data m = 2, # number of clustres method = &quot;crisp&quot;, # method used for parameters estimation in each class; options: &quot;crisp&quot; or &quot;soft&quot; itera = 10, # number of iterations eps = 0.01, # the accepted error to converge (e) params = &quot;random&quot;, # initial parameters, if &quot;random&quot; &#39;m&#39; values from &#39;xx&#39; are randomly select, or we can input a vector with &#39;m&#39; values main=&quot;Glucose level&quot;, # histogram title seed = 0 # value for set.seed() ){ medias=NA desvpadrao=NA if (params == &quot;random&quot;) { set.seed(seed) param.mean&lt;-sample(xx,m) param.sd&lt;-rep(sd(xx)/m,m) } else { param.mean&lt;-params param.sd&lt;-rep(sd(xx)/m,m) } for (k in 1:itera){ densities&lt;-sapply(1:m, function(i){dnorm(xx, mean=param.mean[i], sd=param.sd[i])}) relevance&lt;-t(apply(densities,1,function(l){l/sum(l)})) attrib&lt;-t(apply(relevance,1, function(l){replace(rep(0,m),which.max(l)[1],1)})) # Re-estimates the parameters of the probability distribution of each class if (method == &quot;crisp&quot;) { ## M-step (alternative 1: crisp) param.mean.new&lt;-sapply(1:m, function(j){mean(xx[attrib[,j]==1])}) param.sd.new&lt;-sapply(1:m, function(j){sqrt(sum(relevance[,j]*(xx-param.mean[j])^2)/sum(relevance[,j]))}) } else { ## M-step (alternative 2: soft) param.mean.new&lt;-sapply(1:m, function(j){sum(xx*relevance[,j])/sum(relevance[,j])}) param.sd.new&lt;-sapply(1:m, function(j){sqrt(sum(relevance[,j]*(xx-param.mean[j])^2)/sum(relevance[,j]))}) } # convergence the maximum number of iterations (k). cat(&quot;iteration&quot;, k, &quot;evolution:&quot;, dif&lt;-sum(abs(param.mean.new - param.mean)), &quot;\\n&quot;) if(dif &lt; eps) break # the accepted error to converge (e) param.mean&lt;-param.mean.new param.sd&lt;-param.sd.new } ## ploting the histogram and density curve for the vector of data hist(xx, density = 20, main=main, xlab=&quot;&quot;, freq=F) lines(density(xx), lwd=2,col=&quot;black&quot;) ## save means, std deviations and classes&#39; attributes on a list result&lt;-list(medias = param.mean, desvpadrao = param.sd, classe = as.data.frame(attrib)) } 2.6.2 2 clusters density curves # reading the data breast&lt;-read.csv(&quot;2.UploadedData/dataR2.csv&quot;) par(mfrow=c(1,2)) # run the EM function for Glucose with 2 clusters and max 20 iterations res&lt;-emf(xx = breast$Glucose, itera = 20, m=2, method = &quot;crisp&quot;, main = &quot;Glucose values&quot;, seed = 0) ## iteration 1 evolution: 17.79331 ## iteration 2 evolution: 9.991458 ## iteration 3 evolution: 9.361819 ## iteration 4 evolution: 1.118374 ## iteration 5 evolution: 2.419946 ## iteration 6 evolution: 5.454559 ## iteration 7 evolution: 0 # add density curves for each class lines(density(rnorm(100, res$medias[1], res$desvpadrao[1]), bw = 12),lwd=2,col=5) lines(density(rnorm(100, res$medias[2], res$desvpadrao[2]), bw = 12),lwd=2,col=6) ## column with 2 classes attach(res$classe) classe2&lt;-ifelse(V1 == 1, 5 , 6) # create &#39;m&#39; classes detach() # Plot the values in which each point has a color corresponding to the class to which it belongs plot(1:length(breast$Glucose), breast$Glucose, col=classe2, pch=20, xlab=&quot;index&quot;, ylab=&quot;Glucose values&quot;, main = &quot; Glucose colored by class (m=2)&quot;) 2.6.3 3 clusters density curves par(mfrow=c(1,2)) # run the EM function for Glucose with 3 clusters and max 20 iterations res&lt;-emf(xx = breast$Glucose, itera = 20, m=3, method = &quot;crisp&quot;, main = &quot;Glucose values&quot;, seed = 0) ## iteration 1 evolution: 22.28802 ## iteration 2 evolution: 16.6689 ## iteration 3 evolution: 3.445327 ## iteration 4 evolution: 0.3056396 ## iteration 5 evolution: 0 # add density curves for each class lines(density(rnorm(100, res$medias[1], res$desvpadrao[1]), bw = 12),lwd=2,col=5) lines(density(rnorm(100, res$medias[2], res$desvpadrao[2]), bw = 12),lwd=2,col=6) lines(density(rnorm(100, res$medias[3], res$desvpadrao[3]), bw = 12),lwd=2,col=7) ## column with 3 classes attach(res$classe) classe3&lt;-ifelse(V1 == 1, 5, ifelse(V2 == 1, 6, 7)) # create &#39;m&#39; classes detach() plot(1:length(breast$Glucose), breast$Glucose, col=classe3, pch=20, xlab=&quot;index&quot;, ylab=&quot;Glucose values&quot;, main = &quot; Glucose colored by class (m=3)&quot;) "],
["stats.html", "3 STATS", " 3 STATS Modelação Estatística Conteúdos programáticos Regressão logística. Análise de sobrevivência. Análise de dados longitudinais. Causalidade. "],
["lesson-1-1.html", "3.1 Lesson 1", " 3.1 Lesson 1 exercise 1 The BMD_weight.sav database contains 190 densitometry records. library(foreign) data1 &lt;- read.spss(file=&quot;2.UploadedData/BMD_weight_new.sav&quot;, use.missing=TRUE) Calculate the body mass index. data1$IMC &lt;- ((data1$peso)/((data1$altura)/100)^2) Calculate the mean and standard deviation of total bone mineral density mean(data1$IMC) ## [1] 26.42143 sd(data1$IMC) ## [1] 4.299248 Analyze the relationship between bone mineral density and BMI using linear regression. plot(x = data1$IMC, y=data1$bmdtot) cor.test(data1$IMC, data1$bmdtot, method = &quot;pearson&quot;) ## ## Pearson&#39;s product-moment correlation ## ## data: data1$IMC and data1$bmdtot ## t = 4.4216, df = 186, p-value = 1.664e-05 ## alternative hypothesis: true correlation is not equal to 0 ## 95 percent confidence interval: ## 0.1729248 0.4324277 ## sample estimates: ## cor ## 0.3084029 regressao &lt;- lm(formula = bmdtot ~ IMC, data=data1) summary(regressao) ## ## Call: ## lm(formula = bmdtot ~ IMC, data = data1) ## ## Residuals: ## Min 1Q Median 3Q Max ## -0.28807 -0.07629 -0.00038 0.08273 0.37375 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.593934 0.052316 11.353 &lt; 2e-16 *** ## IMC 0.008639 0.001954 4.422 1.66e-05 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 0.1155 on 186 degrees of freedom ## (2 observations deleted due to missingness) ## Multiple R-squared: 0.09511, Adjusted R-squared: 0.09025 ## F-statistic: 19.55 on 1 and 186 DF, p-value: 1.664e-05 Compare the BMDTOT average according to the type of medication, adjusting for BMI and age. regressaoM &lt;- lm(formula = bmdtot ~ Medication + IMC + idade, data=data1) Check the assumptions for the model used in d). plot(regressaoM$fitted.values, regressaoM$residuals) hist(regressaoM$residuals) or we can plot everything plot(regressaoM) exercise 2 The data in the score2013.sav file refer to 1768 admissions to pediatric intensive care units in several Portuguese hospital units. data2 &lt;- read.spss(file=&quot;2.UploadedData/score2013.sav&quot;, use.missing=TRUE) The minimum systolic tension of the first 12 hours of hospitalization and mechanical ventilation at some point in the first hour of hospitalization are potential predictors of mortality in the ICU. summary(data2) ## Length Class Mode ## sexo 1768 factor numeric ## peso 1768 factor numeric ## outcome 1768 factor numeric ## apneia 1768 factor numeric ## ventil 1768 factor numeric ## hmedica 1768 factor numeric ## CPAP 1768 factor numeric ## Pupil 1768 factor numeric ## TAS12 1768 factor numeric ## temp12 1768 factor numeric ## entub 1768 factor numeric Study the individual association of each of them with mortality first. data2$TAS12 &lt;- as.numeric(factor(data2$TAS12)) logistic1 &lt;- glm(outcome ~ TAS12, data = data2, family = &quot;binomial&quot;) logistic2 &lt;- glm(outcome ~ ventil, data = data2, family = &quot;binomial&quot;) summary(logistic1) ## ## Call: ## glm(formula = outcome ~ TAS12, family = &quot;binomial&quot;, data = data2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.1116 -0.3953 -0.2934 -0.2225 3.0054 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -0.054551 0.215283 -0.253 0.8 ## TAS12 -0.051158 0.004748 -10.776 &lt;2e-16 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 948.95 on 1767 degrees of freedom ## Residual deviance: 825.01 on 1766 degrees of freedom ## AIC: 829.01 ## ## Number of Fisher Scoring iterations: 6 summary(logistic2) ## ## Call: ## glm(formula = outcome ~ ventil, family = &quot;binomial&quot;, data = data2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.4776 -0.4776 -0.3022 -0.3022 2.4936 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -3.0634 0.1617 -18.940 &lt; 2e-16 *** ## ventilSim 0.9500 0.1952 4.868 1.13e-06 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 948.95 on 1767 degrees of freedom ## Residual deviance: 923.16 on 1766 degrees of freedom ## AIC: 927.16 ## ## Number of Fisher Scoring iterations: 5 exp(cbind(OR = coef(logistic1), confint(logistic1))) ## OR 2.5 % 97.5 % ## (Intercept) 0.9469106 0.6185266 1.4405861 ## TAS12 0.9501285 0.9411981 0.9589037 exp(cbind(OR = coef(logistic2), confint(logistic2))) ## OR 2.5 % 97.5 % ## (Intercept) 0.04672897 0.03348726 0.06324236 ## ventilSim 2.58560403 1.77721401 3.82737660 Then test the interaction of these two variables and interpret the results. logistic3 &lt;- glm(outcome ~ ventil + TAS12 + ventil * TAS12, data = data2, family = &quot;binomial&quot;) summary(logistic3) ## ## Call: ## glm(formula = outcome ~ ventil + TAS12 + ventil * TAS12, family = &quot;binomial&quot;, ## data = data2) ## ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -1.2779 -0.3739 -0.2875 -0.2301 3.0171 ## ## Coefficients: ## Estimate Std. Error z value Pr(&gt;|z|) ## (Intercept) -1.323763 0.464494 -2.850 0.004373 ** ## ventilSim 1.669344 0.529036 3.155 0.001603 ** ## TAS12 -0.031758 0.008677 -3.660 0.000252 *** ## ventilSim:TAS12 -0.024409 0.010569 -2.310 0.020914 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## (Dispersion parameter for binomial family taken to be 1) ## ## Null deviance: 948.95 on 1767 degrees of freedom ## Residual deviance: 811.29 on 1764 degrees of freedom ## AIC: 819.29 ## ## Number of Fisher Scoring iterations: 6 exp(cbind(OR = coef(logistic3), confint(logistic3))) ## OR 2.5 % 97.5 % ## (Intercept) 0.2661319 0.1024458 0.6380664 ## ventilSim 5.3086860 1.9349881 15.5012115 ## TAS12 0.9687412 0.9524027 0.9854715 ## ventilSim:TAS12 0.9758865 0.9557194 0.9962234 Using all the variables in the database, build a model for mortality and study the model’s discrimination and calibration. accuracy pred&lt;-predict(logistic3, type = &quot;response&quot;) pred01&lt;-ifelse(pred&lt;0.5, 0, 1) table(pred01,data2$outcome) ## ## pred01 Vivo Falecido ## 0 1624 121 ## 1 10 13 acc&lt;-(1624+13)/(1624+121+10+13) acc ## [1] 0.925905 AUC library(pROC) g &lt;- roc(outcome ~ pred, data = data2) plot(g) Goodness of fit test library(ResourceSelection) hoslem.test(data2$outcome, pred01) ## ## Hosmer and Lemeshow goodness of fit (GOF) test ## ## data: data2$outcome, pred01 ## X-squared = 1768, df = 8, p-value &lt; 2.2e-16 "],
["lesson-2-1.html", "3.2 Lesson 2", " 3.2 Lesson 2 Length of stay is an important indicator used in health care assessment. The UCIP_PRISM.sav database contains the record of 812 pediatric admissions in 3 intensive care units (Variable P1: Coimbra, Lisboa and Porto). We want to compare the hospitalization time (Variable tempoint) of the three units. However, we must take into account that the units may receive patients with different levels of severity. The PRISM variable is an indicator of the patient’s severity at the date of admission. library(foreign) aula2 &lt;- read.spss(file=&quot;2.UploadedData/UCIP_PRISM HEADS.sav&quot;, use.missing=TRUE) summary(aula2) ## Length Class Mode ## P1 812 factor numeric ## PRISM 812 -none- numeric ## tempoint 812 -none- numeric Make a histogram of the length of stay hist(aula2$tempoint) 2.Make a linear regression model with the dependent variable length of stay (tempoint), and the intensive care unit (P1) and PRISM variables as independent variables. regressao &lt;- lm(formula = tempoint ~ P1 + PRISM, data=aula2) summary(regressao) ## ## Call: ## lm(formula = tempoint ~ P1 + PRISM, data = aula2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -14.987 -6.568 -3.944 -0.047 255.598 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 3.84611 1.36895 2.810 0.00508 ** ## P1Lisboa 4.27584 1.54619 2.765 0.00581 ** ## P1Porto 2.70428 1.54758 1.747 0.08094 . ## PRISM 0.18291 0.07643 2.393 0.01693 * ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 18.04 on 808 degrees of freedom ## Multiple R-squared: 0.01558, Adjusted R-squared: 0.01192 ## F-statistic: 4.261 on 3 and 808 DF, p-value: 0.005353 3.Check that the model assumptions in question b) are not satisfied. plot(regressao$fitted.values, regressao$residuals) hist(regressao$residuals) or you can plot everything plot(regressao) 4.Transform the length of stay variable into the Neperian logarithm of length of stay (tempoint -&gt; logtempoint). aula2$logtempoint &lt;- log(aula2$tempoint) 5.Analyze the differences in the logarithm of length of stay between the 3 units, adjusting to the severity of the patients, ie, remake the model in question b) but with the logarithm of length of stay as dependent variable. logregressao &lt;- lm(formula = logtempoint ~ P1 + PRISM, data=aula2) summary(logregressao) ## ## Call: ## lm(formula = logtempoint ~ P1 + PRISM, data = aula2) ## ## Residuals: ## Min 1Q Median 3Q Max ## -2.4230 -0.9225 -0.1198 0.7848 4.2611 ## ## Coefficients: ## Estimate Std. Error t value Pr(&gt;|t|) ## (Intercept) 0.734535 0.085985 8.543 &lt; 2e-16 *** ## P1Lisboa 0.396173 0.097117 4.079 4.96e-05 *** ## P1Porto 0.319145 0.097205 3.283 0.00107 ** ## PRISM 0.026849 0.004801 5.593 3.06e-08 *** ## --- ## Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 ## ## Residual standard error: 1.133 on 808 degrees of freedom ## Multiple R-squared: 0.05785, Adjusted R-squared: 0.05435 ## F-statistic: 16.54 on 3 and 808 DF, p-value: 1.954e-10 Geometric mean ratio (percentage value) (exp(logregressao$coefficients)-1)*100 ## (Intercept) P1Lisboa P1Porto PRISM ## 108.451271 48.612650 37.595032 2.721276 6.Check that the model assumptions in question e) are satisfied. plot(logregressao$fitted.values, logregressao$residuals) hist(logregressao$residuals) or you can plot everything plot(logregressao) "],
["learn.html", "4 LEARN", " 4 LEARN Macine Learning Conteúdos programáticos O processo de extração de conhecimento de dados Compreensão do negócio Compreensão dos dados Pré-processamento de dados Modelação de dados Avaliação de modelos de extração de conhecimento de dados Implantação prática dos resultados de modelação Aprendizagem automática Aprender conceitos a partir de dados Processos indutivos vs processos dedutivos Viés indutivo Validação de modelos Medidas de erro e processos de estimação Aprendizagem supervisionada Árvores de decisão Redes Bayesianas Redes neuronais Deep learning e análise de grandes bases de dados Aprendizagem não supervisionada Análise de clusters Deteção de casos extremos e anomalias Associação e análise de padrões frequentes Interpretação de grandes bases de dados "],
["lesson-1-2.html", "4.1 Lesson 1", " 4.1 Lesson 1 2020-02-04 This exercise was done as described by: https://machinelearningmastery.com/machine-learning-in-r-step-by-step/ but using a data set from: https://archive.ics.uci.edu/ml/datasets/Breast+Cancer+Coimbra# Read the data dataset &lt;- read.csv(&quot;2.UploadedData/dataR2.csv&quot;) Factor variable Classification dataset$Classification&lt;-as.factor(dataset$Classification) NOTE: in order to use the caret packages in all its’ capabilities you must do install.packages(\"caret\", dependencies = T) create a validation dataset and use the remaing for training library(caret) # create a list of 80% of the rows in the original dataset we can use for training validation_index &lt;- createDataPartition(dataset$Classification, p=0.80, list=FALSE) # select 20% of the data for validation validation &lt;- dataset[-validation_index,] # use the remaining 80% of data to training and testing the models dataset &lt;- dataset[validation_index,] dimensions of the new dataset dim(dataset) ## [1] 94 10 list types for each attribute sapply(dataset, class) ## Age BMI Glucose Insulin HOMA ## &quot;integer&quot; &quot;numeric&quot; &quot;integer&quot; &quot;numeric&quot; &quot;numeric&quot; ## Leptin Adiponectin Resistin MCP.1 Classification ## &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;numeric&quot; &quot;factor&quot; take a peek at the first 5 rows of the data head(dataset) ## Age BMI Glucose Insulin HOMA Leptin Adiponectin Resistin ## 1 48 23.50000 70 2.707 0.4674087 8.8071 9.702400 7.99585 ## 2 83 20.69049 92 3.115 0.7068973 8.8438 5.429285 4.06405 ## 3 82 23.12467 91 4.498 1.0096511 17.9393 22.432040 9.27715 ## 4 68 21.36752 77 3.226 0.6127249 9.8827 7.169560 12.76600 ## 5 86 21.11111 92 3.549 0.8053864 6.6994 4.819240 10.57635 ## 6 49 22.85446 92 3.226 0.7320869 6.8317 13.679750 10.31760 ## MCP.1 Classification ## 1 417.114 1 ## 2 468.786 1 ## 3 554.697 1 ## 4 928.220 1 ## 5 773.920 1 ## 6 530.410 1 list the levels for the class levels(dataset$Classification) ## [1] &quot;1&quot; &quot;2&quot; summarize the class distribution percentage &lt;- prop.table(table(dataset$Classification)) * 100 cbind(freq=table(dataset$Classification), percentage=percentage) ## freq percentage ## 1 42 44.68085 ## 2 52 55.31915 summarize attribute distributions summary(dataset) ## Age BMI Glucose Insulin ## Min. :24.00 Min. :18.37 Min. : 60.00 Min. : 2.432 ## 1st Qu.:45.25 1st Qu.:23.00 1st Qu.: 85.25 1st Qu.: 4.337 ## Median :58.50 Median :27.47 Median : 93.00 Median : 5.814 ## Mean :58.04 Mean :27.56 Mean : 98.93 Mean :10.649 ## 3rd Qu.:70.50 3rd Qu.:31.25 3rd Qu.:102.75 3rd Qu.:12.269 ## Max. :89.00 Max. :38.58 Max. :201.00 Max. :58.460 ## HOMA Leptin Adiponectin Resistin ## Min. : 0.4674 Min. : 4.311 Min. : 1.656 Min. : 3.210 ## 1st Qu.: 0.9201 1st Qu.:11.121 1st Qu.: 5.896 1st Qu.: 6.704 ## Median : 1.3505 Median :19.391 Median : 8.788 Median :11.230 ## Mean : 2.9317 Mean :26.293 Mean :10.773 Mean :14.895 ## 3rd Qu.: 3.0387 3rd Qu.:37.280 3rd Qu.:12.750 3rd Qu.:18.156 ## Max. :25.0503 Max. :90.280 Max. :38.040 Max. :82.100 ## MCP.1 Classification ## Min. : 45.84 1:42 ## 1st Qu.: 264.68 2:52 ## Median : 458.79 ## Mean : 546.47 ## 3rd Qu.: 736.70 ## Max. :1698.44 split input and output x &lt;- dataset[,1:9] y &lt;- dataset[,10] boxplot for each attribute on one image par(mfrow=c(2,5)) for(i in 1:9) { boxplot(x[,i], main=names(dataset)[i]) } barplot for class breakdown par(mfrow=c(1,1)) plot(y) scatterplot matrix scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) featurePlot(x=x, y=y, plot=&quot;ellipse&quot;) box and whisker plots for each attribute scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) featurePlot(x=x, y=y, plot=&quot;box&quot;, scales=scales) density plots for each attribute by class value scales &lt;- list(x=list(relation=&quot;free&quot;), y=list(relation=&quot;free&quot;)) featurePlot(x=x, y=y, plot=&quot;density&quot;, scales=scales) Run algorithms using 10-fold cross validation control &lt;- trainControl(method=&quot;cv&quot;, number=10) metric &lt;- &quot;Accuracy&quot; linear algorithms set.seed(7) fit.lda &lt;- train(Classification~., data=dataset, method=&quot;lda&quot;, metric=metric, trControl=control) nonlinear algorithms CART set.seed(7) fit.cart &lt;- train(Classification~., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) kNN set.seed(7) fit.knn &lt;- train(Classification~., data=dataset, method=&quot;knn&quot;, metric=metric, trControl=control) advanced algorithms SVM set.seed(7) fit.svm &lt;- train(Classification~., data=dataset, method=&quot;svmRadial&quot;, metric=metric, trControl=control) Random Forest set.seed(7) fit.rf &lt;- train(Classification~., data=dataset, method=&quot;rf&quot;, metric=metric, trControl=control) summarize accuracy of models results &lt;- resamples(list(lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf)) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: lda, cart, knn, svm, rf ## Number of resamples: 10 ## ## Accuracy ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## lda 0.4000000 0.6166667 0.6666667 0.6949495 0.7651515 1.0000000 0 ## cart 0.3636364 0.5555556 0.5555556 0.5585859 0.6500000 0.6666667 0 ## knn 0.4000000 0.4583333 0.5555556 0.5718182 0.6388889 0.8181818 0 ## svm 0.6000000 0.6750000 0.7525253 0.7471717 0.7777778 0.8888889 0 ## rf 0.3000000 0.6750000 0.7777778 0.7040404 0.7777778 0.8888889 0 ## ## Kappa ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## lda -0.1538462 0.21756757 0.34146341 0.39529062 0.5137845 1.0000000 0 ## cart -0.2222222 0.01315789 0.12142857 0.09593592 0.2443694 0.3414634 0 ## knn -0.1538462 -0.05357143 0.05263158 0.13504244 0.2557692 0.6451613 0 ## svm 0.2000000 0.35609756 0.50125313 0.48402743 0.5440789 0.7804878 0 ## rf -0.4000000 0.30270270 0.53815789 0.40038353 0.5500000 0.7692308 0 compare accuracy of models dotplot(results) summarize Best Model print(fit.rf) ## Random Forest ## ## 94 samples ## 9 predictor ## 2 classes: &#39;1&#39;, &#39;2&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold) ## Summary of sample sizes: 85, 85, 83, 84, 84, 85, ... ## Resampling results across tuning parameters: ## ## mtry Accuracy Kappa ## 2 0.7040404 0.4003835 ## 5 0.6929293 0.3669710 ## 9 0.6625253 0.3087049 ## ## Accuracy was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 2. estimate skill of LDA on the validation dataset predictions &lt;- predict(fit.rf, validation) confusionMatrix(predictions, validation$Classification) ## Confusion Matrix and Statistics ## ## Reference ## Prediction 1 2 ## 1 7 5 ## 2 3 7 ## ## Accuracy : 0.6364 ## 95% CI : (0.4066, 0.828) ## No Information Rate : 0.5455 ## P-Value [Acc &gt; NIR] : 0.2622 ## ## Kappa : 0.2787 ## ## Mcnemar&#39;s Test P-Value : 0.7237 ## ## Sensitivity : 0.7000 ## Specificity : 0.5833 ## Pos Pred Value : 0.5833 ## Neg Pred Value : 0.7000 ## Prevalence : 0.4545 ## Detection Rate : 0.3182 ## Detection Prevalence : 0.5455 ## Balanced Accuracy : 0.6417 ## ## &#39;Positive&#39; Class : 1 ## In this post you discovered step-by-step how to complete your first machine learning project in R "],
["lesson-2-2.html", "4.2 Lesson 2", " 4.2 Lesson 2 2020-02-11 # Load package &#39;caret&#39; library(caret) # Load package &#39;pROC&#39; library(pROC) continuation of the previous classe. All the code was provided by the professor. # loading and preparing the data dataset &lt;- read.csv(&quot;2.UploadedData/dataR2.csv&quot;) # Define Classification as factor dataset$Classification &lt;- factor(dataset$Classification, levels=1:2, labels=c(&quot;Control&quot;, &quot;Patient&quot;)) # Holdout a validation set, by defining the indices of the training set training.index &lt;- createDataPartition(dataset$Classification, p=0.8, list=FALSE) validation &lt;- dataset[-training.index,] dataset &lt;- dataset[training.index,] Dimensions (should be 116 observations and 10 variables) dim(dataset) ## [1] 94 10 # Split input and output input &lt;- dataset[,-10] output &lt;- dataset[,10] 4.2.1 Test harness Define the metric metric &lt;- \"ROC\" Define the estimation method control &lt;- trainControl(...) Train the model with chosen metric and method (output is factor with “Yes” or “No”) fit.model &lt;- train(output ~ ., data=dataset, method=\"nnet\", metric=metric, trControl=control) Plot ROC curve plot.roc(fit.model$pred$obs,fit.model$pred$Yes, main=fit.model$method, print.auc=T) # Run algorithm using different validation approaches metric &lt;- &quot;ROC&quot; 4.2.1.1 Run algorithm using 20% hold-out validation control &lt;- trainControl(method=&quot;LGOCV&quot;, p=0.8, number=1, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE) set.seed(7) fit.cart.hold &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.nb.hold &lt;- train(Classification ~ ., data=dataset, method=&quot;naive_bayes&quot;, metric=metric, trControl=control) Summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.hold, nb=fit.nb.hold) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, nb ## Number of resamples: 1 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.6125 0.6125 0.6125 0.6125 0.6125 0.6125 0 ## nb 0.7250 0.7250 0.7250 0.7250 0.7250 0.7250 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.625 0.625 0.625 0.625 0.625 0.625 0 ## nb 0.750 0.750 0.750 0.750 0.750 0.750 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.6 0.6 0.6 0.6 0.6 0.6 0 ## nb 0.5 0.5 0.5 0.5 0.5 0.5 0 ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;20% Hold-out -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) 4.2.1.2 Run algorithm using multiple 20% hold-out validation control &lt;- trainControl(method=&quot;LGOCV&quot;, p=0.8, number=25, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE) set.seed(7) fit.cart.mhold &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.nb.mhold &lt;- train(Classification ~ ., data=dataset, method=&quot;naive_bayes&quot;, metric=metric, trControl=control) Summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.mhold, nb=fit.nb.mhold) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, nb ## Number of resamples: 25 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.46875 0.6750 0.7375 0.7435 0.81875 0.9500 0 ## nb 0.58750 0.6625 0.7625 0.7505 0.81250 0.9625 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.125 0.5 0.625 0.625 0.750 1 0 ## nb 0.375 0.5 0.750 0.680 0.875 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.5 0.7 0.8 0.772 0.9 1.0 0 ## nb 0.4 0.5 0.6 0.624 0.7 0.9 0 ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;25 x 20% Hold-out -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) 4.2.1.3 Run algorithm using 10-fold cross validation control &lt;- trainControl(method=&quot;cv&quot;, number=10, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE, repeats = 1) set.seed(7) fit.cart.cv &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.nb.cv &lt;- train(Classification ~ ., data=dataset, method=&quot;naive_bayes&quot;, metric=metric, trControl=control) Summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.cv, nb=fit.nb.cv) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, nb ## Number of resamples: 10 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.5208333 0.66875 0.7375 0.7305833 0.7900000 0.875 0 ## nb 0.6500000 0.70000 0.7250 0.7796667 0.8833333 0.950 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.40 0.50 0.75 0.645 0.75 0.8 0 ## nb 0.75 0.75 0.90 0.880 1.00 1.0 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.2 0.65 0.80 0.7700000 1.0 1.0000000 0 ## nb 0.0 0.25 0.45 0.4166667 0.6 0.6666667 0 ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;10-fold CV -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) 4.2.1.4 Run algorithm using 25 times 10-fold cross validation control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE, repeats = 25) set.seed(7) fit.cart.rcv &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.nb.rcv &lt;- train(Classification ~ ., data=dataset, method=&quot;naive_bayes&quot;, metric=metric, trControl=control) Summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.rcv, nb=fit.nb.rcv) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, nb ## Number of resamples: 250 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.225 0.625 0.7291667 0.7235133 0.8333333 1 0 ## nb 0.150 0.680 0.8000000 0.7798867 0.9000000 1 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.00 0.50 0.6 0.6094 0.75 1 0 ## nb 0.25 0.75 1.0 0.8752 1.00 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.2 0.6 0.8 0.7606667 1.0 1 0 ## nb 0.0 0.2 0.4 0.3948000 0.6 1 0 ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;25 x 10-fold CV -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) 4.2.1.5 Run algorithm using leave-one-out validation control &lt;- trainControl(method=&quot;LOOCV&quot;, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE) set.seed(7) fit.cart.loo &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.nb.loo &lt;- train(Classification ~ ., data=dataset, method=&quot;naive_bayes&quot;, metric=metric, trControl=control) Summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.loo, nb=fit.nb.loo) #results &lt;- resamples(fit.models) #summary(results) summary(fit.models) ## Length Class Mode ## rpart 23 train list ## nb 23 train list ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;Leave-One-Out -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) 4.2.1.6 Run algorithm using bootstrap validation control &lt;- trainControl(method=&quot;boot_all&quot;, number=25, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE) set.seed(7) fit.cart.boot &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.nb.boot &lt;- train(Classification ~ ., data=dataset, method=&quot;naive_bayes&quot;, metric=metric, trControl=control) Summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.boot, nb=fit.nb.boot) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, nb ## Number of resamples: 25 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.5433333 0.6339869 0.7230576 0.7038937 0.7843137 0.8299595 0 ## nb 0.6235294 0.7147059 0.7443609 0.7555786 0.7843137 0.9084249 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.3888889 0.5294118 0.6 0.6453699 0.7647059 0.9375 0 ## nb 0.6470588 0.7647059 0.8 0.8213312 0.8750000 1.0000 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.4210526 0.6111111 0.7058824 0.6897188 0.7647059 0.9473684 0 ## nb 0.2352941 0.3684211 0.4666667 0.4724700 0.5714286 0.8095238 0 ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$Patient, main=paste(&quot;25 x Bootstrap -&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) 4.2.2 Make predictions Estimate skill of GLM Step AIC on the validation dataset par(mfrow=c(1,1)) predictions.prob &lt;- predict(fit.nb.boot, validation, type=&quot;prob&quot;) predictions &lt;- predict(fit.nb.boot, validation, type=&quot;raw&quot;) confusionMatrix(predictions, validation$Classification) ## Confusion Matrix and Statistics ## ## Reference ## Prediction Control Patient ## Control 9 7 ## Patient 1 5 ## ## Accuracy : 0.6364 ## 95% CI : (0.4066, 0.828) ## No Information Rate : 0.5455 ## P-Value [Acc &gt; NIR] : 0.2622 ## ## Kappa : 0.3016 ## ## Mcnemar&#39;s Test P-Value : 0.0771 ## ## Sensitivity : 0.9000 ## Specificity : 0.4167 ## Pos Pred Value : 0.5625 ## Neg Pred Value : 0.8333 ## Prevalence : 0.4545 ## Detection Rate : 0.4091 ## Detection Prevalence : 0.7273 ## Balanced Accuracy : 0.6583 ## ## &#39;Positive&#39; Class : Control ## plot.roc(validation$Classification, predictions.prob$Patient, print.auc=T) "],
["lesson-3-1.html", "4.3 Lesson 3", " 4.3 Lesson 3 2020-02-18 4.3.1 Using ‘rpart’ and the Breast Cancer Coimbra data set # Define the filename filename &lt;- &quot;2.UploadedData/dataR2.csv&quot; # Load the CSV file from the local directory dataset &lt;- read.csv(filename, header=T) dataset$Classification&lt;-factor(dataset$Classification, levels=1:2, labels=c(&quot;Controls&quot;,&quot;Patients&quot;)) library(rpart) # learn tree with Gini impurity tree.gini&lt;-rpart(Classification~.,data = dataset) # learn tree with information gain tree.information&lt;-rpart(Classification~., data=dataset, parms=list(split=&quot;information&quot;)) Summary Information summary(tree.information) ## Call: ## rpart(formula = Classification ~ ., data = dataset, parms = list(split = &quot;information&quot;)) ## n= 116 ## ## CP nsplit rel error xerror xstd ## 1 0.38461538 0 1.0000000 1.0000000 0.10300524 ## 2 0.08653846 1 0.6153846 0.8653846 0.10092601 ## 3 0.03846154 3 0.4423077 0.5769231 0.09069378 ## 4 0.01000000 7 0.2307692 0.5769231 0.09069378 ## ## Variable importance ## Age Glucose BMI HOMA Resistin Insulin ## 25 21 12 11 9 7 ## MCP.1 Leptin Adiponectin ## 6 6 3 ## ## Node number 1: 116 observations, complexity param=0.3846154 ## predicted class=Patients expected loss=0.4482759 P(node) =1 ## class counts: 52 64 ## probabilities: 0.448 0.552 ## left son=2 (50 obs) right son=3 (66 obs) ## Primary splits: ## Glucose &lt; 91.5 to the left, improve=11.586660, (0 missing) ## HOMA &lt; 2.169985 to the left, improve= 8.039333, (0 missing) ## Age &lt; 37 to the left, improve= 7.453390, (0 missing) ## Resistin &lt; 13.71318 to the left, improve= 6.993164, (0 missing) ## Insulin &lt; 10.285 to the left, improve= 6.048777, (0 missing) ## Surrogate splits: ## HOMA &lt; 1.231021 to the left, agree=0.698, adj=0.30, (0 split) ## Age &lt; 37 to the left, agree=0.664, adj=0.22, (0 split) ## Insulin &lt; 4.8075 to the left, agree=0.629, adj=0.14, (0 split) ## Adiponectin &lt; 22.12789 to the right, agree=0.612, adj=0.10, (0 split) ## MCP.1 &lt; 532.317 to the right, agree=0.612, adj=0.10, (0 split) ## ## Node number 2: 50 observations, complexity param=0.08653846 ## predicted class=Controls expected loss=0.3 P(node) =0.4310345 ## class counts: 35 15 ## probabilities: 0.700 0.300 ## left son=4 (15 obs) right son=5 (35 obs) ## Primary splits: ## Age &lt; 44.5 to the left, improve=6.641431, (0 missing) ## Resistin &lt; 13.24805 to the left, improve=6.556913, (0 missing) ## BMI &lt; 31.01992 to the right, improve=5.052061, (0 missing) ## Insulin &lt; 3.4445 to the right, improve=3.236703, (0 missing) ## MCP.1 &lt; 228.949 to the left, improve=2.734089, (0 missing) ## Surrogate splits: ## BMI &lt; 31.71078 to the right, agree=0.78, adj=0.267, (0 split) ## Leptin &lt; 31.16805 to the right, agree=0.76, adj=0.200, (0 split) ## MCP.1 &lt; 183.26 to the left, agree=0.76, adj=0.200, (0 split) ## Adiponectin &lt; 12.34449 to the right, agree=0.72, adj=0.067, (0 split) ## ## Node number 3: 66 observations, complexity param=0.03846154 ## predicted class=Patients expected loss=0.2575758 P(node) =0.5689655 ## class counts: 17 49 ## probabilities: 0.258 0.742 ## left son=6 (47 obs) right son=7 (19 obs) ## Primary splits: ## Age &lt; 48.5 to the right, improve=6.897003, (0 missing) ## Glucose &lt; 118.5 to the left, improve=4.016843, (0 missing) ## Leptin &lt; 7.93315 to the left, improve=3.666612, (0 missing) ## Resistin &lt; 11.92665 to the left, improve=3.111218, (0 missing) ## HOMA &lt; 2.173469 to the left, improve=2.979801, (0 missing) ## Surrogate splits: ## HOMA &lt; 0.6920566 to the right, agree=0.742, adj=0.105, (0 split) ## Adiponectin &lt; 15.105 to the left, agree=0.742, adj=0.105, (0 split) ## Insulin &lt; 3.057 to the right, agree=0.727, adj=0.053, (0 split) ## ## Node number 4: 15 observations ## predicted class=Controls expected loss=0 P(node) =0.1293103 ## class counts: 15 0 ## probabilities: 1.000 0.000 ## ## Node number 5: 35 observations, complexity param=0.08653846 ## predicted class=Controls expected loss=0.4285714 P(node) =0.3017241 ## class counts: 20 15 ## probabilities: 0.571 0.429 ## left son=10 (20 obs) right son=11 (15 obs) ## Primary splits: ## Resistin &lt; 13.24805 to the left, improve=7.941566, (0 missing) ## Age &lt; 74.5 to the right, improve=5.353821, (0 missing) ## BMI &lt; 30.13047 to the right, improve=1.622848, (0 missing) ## Glucose &lt; 85.5 to the left, improve=1.479354, (0 missing) ## Insulin &lt; 3.207 to the right, improve=1.464713, (0 missing) ## Surrogate splits: ## BMI &lt; 27.9125 to the left, agree=0.714, adj=0.333, (0 split) ## HOMA &lt; 1.408116 to the left, agree=0.714, adj=0.333, (0 split) ## Insulin &lt; 6.283 to the left, agree=0.686, adj=0.267, (0 split) ## Age &lt; 65.5 to the right, agree=0.657, adj=0.200, (0 split) ## Glucose &lt; 86.5 to the left, agree=0.657, adj=0.200, (0 split) ## ## Node number 6: 47 observations, complexity param=0.03846154 ## predicted class=Patients expected loss=0.3617021 P(node) =0.4051724 ## class counts: 17 30 ## probabilities: 0.362 0.638 ## left son=12 (37 obs) right son=13 (10 obs) ## Primary splits: ## Glucose &lt; 118.5 to the left, improve=5.231699, (0 missing) ## Leptin &lt; 9.2319 to the left, improve=4.358824, (0 missing) ## HOMA &lt; 2.222398 to the left, improve=3.016650, (0 missing) ## Insulin &lt; 5.673 to the left, improve=2.666489, (0 missing) ## MCP.1 &lt; 297.372 to the left, improve=2.089658, (0 missing) ## Surrogate splits: ## HOMA &lt; 5.177161 to the left, agree=0.894, adj=0.5, (0 split) ## Insulin &lt; 19.055 to the left, agree=0.872, adj=0.4, (0 split) ## Leptin &lt; 86.37605 to the left, agree=0.830, adj=0.2, (0 split) ## MCP.1 &lt; 905.523 to the left, agree=0.830, adj=0.2, (0 split) ## Age &lt; 84 to the left, agree=0.809, adj=0.1, (0 split) ## ## Node number 7: 19 observations ## predicted class=Patients expected loss=0 P(node) =0.1637931 ## class counts: 0 19 ## probabilities: 0.000 1.000 ## ## Node number 10: 20 observations ## predicted class=Controls expected loss=0.15 P(node) =0.1724138 ## class counts: 17 3 ## probabilities: 0.850 0.150 ## ## Node number 11: 15 observations ## predicted class=Patients expected loss=0.2 P(node) =0.1293103 ## class counts: 3 12 ## probabilities: 0.200 0.800 ## ## Node number 12: 37 observations, complexity param=0.03846154 ## predicted class=Patients expected loss=0.4594595 P(node) =0.3189655 ## class counts: 17 20 ## probabilities: 0.459 0.541 ## left son=24 (10 obs) right son=25 (27 obs) ## Primary splits: ## Age &lt; 72.5 to the right, improve=3.334784, (0 missing) ## MCP.1 &lt; 297.372 to the left, improve=3.104932, (0 missing) ## BMI &lt; 32.275 to the right, improve=2.939143, (0 missing) ## Leptin &lt; 9.2319 to the left, improve=2.939143, (0 missing) ## Resistin &lt; 12.7652 to the left, improve=2.201398, (0 missing) ## Surrogate splits: ## Leptin &lt; 6.76555 to the left, agree=0.811, adj=0.3, (0 split) ## Insulin &lt; 17.356 to the right, agree=0.757, adj=0.1, (0 split) ## HOMA &lt; 4.164391 to the right, agree=0.757, adj=0.1, (0 split) ## MCP.1 &lt; 166.3975 to the left, agree=0.757, adj=0.1, (0 split) ## ## Node number 13: 10 observations ## predicted class=Patients expected loss=0 P(node) =0.0862069 ## class counts: 0 10 ## probabilities: 0.000 1.000 ## ## Node number 24: 10 observations ## predicted class=Controls expected loss=0.2 P(node) =0.0862069 ## class counts: 8 2 ## probabilities: 0.800 0.200 ## ## Node number 25: 27 observations, complexity param=0.03846154 ## predicted class=Patients expected loss=0.3333333 P(node) =0.2327586 ## class counts: 9 18 ## probabilities: 0.333 0.667 ## left son=50 (7 obs) right son=51 (20 obs) ## Primary splits: ## BMI &lt; 32.275 to the right, improve=5.860887, (0 missing) ## Leptin &lt; 15.02435 to the right, improve=2.744531, (0 missing) ## MCP.1 &lt; 292.096 to the left, improve=1.158823, (0 missing) ## Insulin &lt; 5.673 to the left, improve=0.979303, (0 missing) ## Glucose &lt; 103.5 to the left, improve=0.854835, (0 missing) ## Surrogate splits: ## Leptin &lt; 68.09345 to the right, agree=0.815, adj=0.286, (0 split) ## MCP.1 &lt; 832.433 to the right, agree=0.815, adj=0.286, (0 split) ## ## Node number 50: 7 observations ## predicted class=Controls expected loss=0.1428571 P(node) =0.06034483 ## class counts: 6 1 ## probabilities: 0.857 0.143 ## ## Node number 51: 20 observations ## predicted class=Patients expected loss=0.15 P(node) =0.1724138 ## class counts: 3 17 ## probabilities: 0.150 0.850 Summary Gini summary(tree.gini) ## Call: ## rpart(formula = Classification ~ ., data = dataset) ## n= 116 ## ## CP nsplit rel error xerror xstd ## 1 0.38461538 0 1.0000000 1.0000000 0.10300524 ## 2 0.10576923 1 0.6153846 0.7307692 0.09720906 ## 3 0.06730769 3 0.4038462 0.7307692 0.09720906 ## 4 0.01000000 5 0.2692308 0.7500000 0.09784651 ## ## Variable importance ## Glucose HOMA BMI Age Leptin Insulin ## 21 14 14 13 12 12 ## Resistin Adiponectin MCP.1 ## 7 4 4 ## ## Node number 1: 116 observations, complexity param=0.3846154 ## predicted class=Patients expected loss=0.4482759 P(node) =1 ## class counts: 52 64 ## probabilities: 0.448 0.552 ## left son=2 (50 obs) right son=3 (66 obs) ## Primary splits: ## Glucose &lt; 91.5 to the left, improve=11.136890, (0 missing) ## HOMA &lt; 2.169985 to the left, improve= 7.526679, (0 missing) ## Resistin &lt; 13.71318 to the left, improve= 6.668751, (0 missing) ## Age &lt; 37 to the left, improve= 6.601118, (0 missing) ## Insulin &lt; 10.285 to the left, improve= 5.651907, (0 missing) ## Surrogate splits: ## HOMA &lt; 1.231021 to the left, agree=0.698, adj=0.30, (0 split) ## Age &lt; 37 to the left, agree=0.664, adj=0.22, (0 split) ## Insulin &lt; 4.8075 to the left, agree=0.629, adj=0.14, (0 split) ## Adiponectin &lt; 22.12789 to the right, agree=0.612, adj=0.10, (0 split) ## MCP.1 &lt; 532.317 to the right, agree=0.612, adj=0.10, (0 split) ## ## Node number 2: 50 observations, complexity param=0.1057692 ## predicted class=Controls expected loss=0.3 P(node) =0.4310345 ## class counts: 35 15 ## probabilities: 0.700 0.300 ## left son=4 (29 obs) right son=5 (21 obs) ## Primary splits: ## Resistin &lt; 13.24805 to the left, improve=5.334975, (0 missing) ## Age &lt; 44.5 to the left, improve=3.857143, (0 missing) ## Insulin &lt; 3.4445 to the right, improve=2.951220, (0 missing) ## BMI &lt; 31.01992 to the right, improve=2.842105, (0 missing) ## HOMA &lt; 0.8390145 to the right, improve=1.882353, (0 missing) ## Surrogate splits: ## BMI &lt; 27.9125 to the left, agree=0.74, adj=0.381, (0 split) ## Insulin &lt; 6.3155 to the left, agree=0.70, adj=0.286, (0 split) ## HOMA &lt; 1.408116 to the left, agree=0.70, adj=0.286, (0 split) ## Leptin &lt; 29.01205 to the left, agree=0.70, adj=0.286, (0 split) ## MCP.1 &lt; 587.3165 to the left, agree=0.68, adj=0.238, (0 split) ## ## Node number 3: 66 observations, complexity param=0.06730769 ## predicted class=Patients expected loss=0.2575758 P(node) =0.5689655 ## class counts: 17 49 ## probabilities: 0.258 0.742 ## left son=6 (26 obs) right son=7 (40 obs) ## Primary splits: ## Age &lt; 65.5 to the right, improve=3.569347, (0 missing) ## Leptin &lt; 7.93315 to the left, improve=3.266637, (0 missing) ## Resistin &lt; 11.92665 to the left, improve=2.201071, (0 missing) ## BMI &lt; 32.48096 to the right, improve=2.187879, (0 missing) ## HOMA &lt; 2.173469 to the left, improve=2.183601, (0 missing) ## Surrogate splits: ## Leptin &lt; 47.14355 to the right, agree=0.697, adj=0.231, (0 split) ## MCP.1 &lt; 207.996 to the left, agree=0.682, adj=0.192, (0 split) ## Resistin &lt; 4.558425 to the left, agree=0.667, adj=0.154, (0 split) ## Glucose &lt; 107 to the right, agree=0.652, adj=0.115, (0 split) ## HOMA &lt; 17.95804 to the right, agree=0.636, adj=0.077, (0 split) ## ## Node number 4: 29 observations ## predicted class=Controls expected loss=0.1034483 P(node) =0.25 ## class counts: 26 3 ## probabilities: 0.897 0.103 ## ## Node number 5: 21 observations, complexity param=0.1057692 ## predicted class=Patients expected loss=0.4285714 P(node) =0.1810345 ## class counts: 9 12 ## probabilities: 0.429 0.571 ## left son=10 (10 obs) right son=11 (11 obs) ## Primary splits: ## BMI &lt; 30.0273 to the right, improve=8.485714, (0 missing) ## Leptin &lt; 35.16835 to the right, improve=3.857143, (0 missing) ## Age &lt; 45.5 to the left, improve=2.670330, (0 missing) ## HOMA &lt; 0.8695296 to the right, improve=1.714286, (0 missing) ## Insulin &lt; 5.2615 to the right, improve=1.341270, (0 missing) ## Surrogate splits: ## Age &lt; 40.5 to the left, agree=0.810, adj=0.6, (0 split) ## Leptin &lt; 28.041 to the right, agree=0.810, adj=0.6, (0 split) ## Insulin &lt; 3.6505 to the right, agree=0.714, adj=0.4, (0 split) ## HOMA &lt; 0.8695296 to the right, agree=0.714, adj=0.4, (0 split) ## Adiponectin &lt; 6.456392 to the left, agree=0.667, adj=0.3, (0 split) ## ## Node number 6: 26 observations, complexity param=0.06730769 ## predicted class=Patients expected loss=0.4615385 P(node) =0.2241379 ## class counts: 12 14 ## probabilities: 0.462 0.538 ## left son=12 (7 obs) right son=13 (19 obs) ## Primary splits: ## Glucose &lt; 98.5 to the left, improve=5.554656, (0 missing) ## HOMA &lt; 1.483263 to the left, improve=2.998265, (0 missing) ## Resistin &lt; 12.7652 to the left, improve=2.983683, (0 missing) ## Insulin &lt; 5.923 to the left, improve=1.923077, (0 missing) ## Leptin &lt; 16.5075 to the left, improve=1.923077, (0 missing) ## Surrogate splits: ## Insulin &lt; 5.533 to the left, agree=0.885, adj=0.571, (0 split) ## HOMA &lt; 1.100691 to the left, agree=0.885, adj=0.571, (0 split) ## Leptin &lt; 9.2716 to the left, agree=0.846, adj=0.429, (0 split) ## BMI &lt; 21.31248 to the left, agree=0.769, adj=0.143, (0 split) ## ## Node number 7: 40 observations ## predicted class=Patients expected loss=0.125 P(node) =0.3448276 ## class counts: 5 35 ## probabilities: 0.125 0.875 ## ## Node number 10: 10 observations ## predicted class=Controls expected loss=0.1 P(node) =0.0862069 ## class counts: 9 1 ## probabilities: 0.900 0.100 ## ## Node number 11: 11 observations ## predicted class=Patients expected loss=0 P(node) =0.09482759 ## class counts: 0 11 ## probabilities: 0.000 1.000 ## ## Node number 12: 7 observations ## predicted class=Controls expected loss=0 P(node) =0.06034483 ## class counts: 7 0 ## probabilities: 1.000 0.000 ## ## Node number 13: 19 observations ## predicted class=Patients expected loss=0.2631579 P(node) =0.1637931 ## class counts: 5 14 ## probabilities: 0.263 0.737 Plot par(mfrow=c(1,2)) plot(tree.information,main=&quot;Information&quot;) text(tree.information) plot(tree.gini, main=&quot;Gini&quot;) text(tree.gini) 4.3.2 Random Forest library(caret) library(pROC) Run algorithms using 3 times 10-fold cross validation # set definitions metric &lt;- &quot;ROC&quot; control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE, repeats=3) train Tree and RF set.seed(7) fit.cart.rcv &lt;- train(Classification ~ ., data=dataset, method=&quot;rpart&quot;, metric=metric, trControl=control) set.seed(7) fit.rf.rcv &lt;- train(Classification ~ ., data=dataset, method=&quot;rf&quot;, metric=metric, trControl=control) summarize accuracy of models fit.models &lt;- list(rpart=fit.cart.rcv, rf=fit.rf.rcv) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, rf ## Number of resamples: 30 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.4428571 0.6297619 0.7166667 0.6827513 0.7482143 0.8571429 0 ## rf 0.6000000 0.7333333 0.8000000 0.8175132 0.9000000 1.0000000 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.4 0.5 0.6 0.6344444 0.8 1 0 ## rf 0.4 0.6 0.6 0.6577778 0.8 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.2857143 0.6666667 0.7142857 0.7341270 0.8333333 1 0 ## rf 0.3333333 0.6666667 0.8333333 0.7642857 0.8511905 1 0 ROC curves for models par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$P, main=paste(&quot;3x10-fold-CV&quot;,fit$method), debug=F, print.auc=T)}) Compare accuracy of models dotplot(results) Inspect models print(fit.cart.rcv) ## CART ## ## 116 samples ## 9 predictor ## 2 classes: &#39;Controls&#39;, &#39;Patients&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 105, 105, 104, 104, 103, 104, ... ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.06730769 0.6730556 0.5400000 0.7563492 ## 0.10576923 0.6827513 0.6344444 0.7341270 ## 0.38461538 0.5290476 0.1922222 0.8658730 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.1057692. #getModelInfo(fit.cart.rcv) #getModelInfo(fit.cart.rcv)$rpart getModelInfo(fit.cart.rcv)$rpart$parameters ## parameter class label ## 1 cp numeric Complexity Parameter Inspect models print(fit.rf.rcv) ## Random Forest ## ## 116 samples ## 9 predictor ## 2 classes: &#39;Controls&#39;, &#39;Patients&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 105, 105, 104, 104, 103, 104, ... ## Resampling results across tuning parameters: ## ## mtry ROC Sens Spec ## 2 0.8012037 0.6455556 0.7603175 ## 5 0.8175132 0.6577778 0.7642857 ## 9 0.8128439 0.6955556 0.7714286 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 5. #getModelInfo(fit.rf.rcv) #getModelInfo(fit.rf.rcv)$rf getModelInfo(fit.rf.rcv)$rf$parameters ## parameter class label ## 1 mtry numeric #Randomly Selected Predictors ROC complexity for models plot(fit.cart.rcv) plot(fit.rf.rcv) 4.3.3 Improve Random Forest myGrid&lt;-expand.grid(mtry=1:9) # numero de variaveis/colunas a considerar na RF set.seed(7) fit.rf.rcv.tune &lt;- train(Classification ~ ., data=dataset, method=&quot;rf&quot;, metric=metric, trControl=control, tuneGrid = myGrid) summarize accuracy of models fit.models&lt;-list(rpart=fit.cart.rcv,rf = fit.rf.rcv, rf.tune=fit.rf.rcv.tune) results&lt;-resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, rf, rf.tune ## Number of resamples: 30 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.4428571 0.6297619 0.7166667 0.6827513 0.7482143 0.8571429 0 ## rf 0.6000000 0.7333333 0.8000000 0.8175132 0.9000000 1.0000000 0 ## rf.tune 0.6285714 0.7333333 0.8452381 0.8229101 0.9226190 0.9857143 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.4 0.5 0.6 0.6344444 0.8 1 0 ## rf 0.4 0.6 0.6 0.6577778 0.8 1 0 ## rf.tune 0.4 0.6 0.6 0.6777778 0.8 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.2857143 0.6666667 0.7142857 0.7341270 0.8333333 1 0 ## rf 0.3333333 0.6666667 0.8333333 0.7642857 0.8511905 1 0 ## rf.tune 0.3333333 0.6666667 0.8333333 0.7706349 0.8571429 1 0 Roc curves for models par(mfrow=c(1,3)) rocs&lt;-lapply(fit.models, function(fit){plot.roc(fit$pred$obs, fit$pred$P, main=paste(&quot;3x10-fold CV-&quot;,fit$method),debug=F,print.auc=T)}) Compare accuracy of models dotplot(results) Inspect models getModelInfo(fit.rf.rcv.tune)$rf$parameters ## parameter class label ## 1 mtry numeric #Randomly Selected Predictors "],
["assignment-trees.html", "4.4 Assignment Trees", " 4.4 Assignment Trees 2020-02-21 Compare a decision tree with a random forest in the Cervical Cancer (Risk Factors) data set (available from UCI repository), trying to accurately classify Dx.Cancer knitr::opts_chunk$set(echo = TRUE, warning = F, message = F) 4.4.1 load data library(tidyverse) library(caret) library(rpart) library(rpart.plot) library(pROC) data &lt;- read.csv(&quot;2.UploadedData/risk_factors_cervical_cancer.csv&quot;, na.strings = &quot;?&quot;) ## select columns 0/1 colMax &lt;- function(data) sapply(data, max, na.rm = TRUE) cols&lt;-colMax(data) %&gt;% data.frame(max=.) %&gt;% rownames_to_column(&quot;cols&quot;) %&gt;% filter(max==1) %&gt;% .$cols # factorize columns 0/1 data&lt;-data %&gt;% mutate_at(cols, ~factor(.,levels = 0:1, labels = c(&quot;no&quot;,&quot;yes&quot;))) # excluded columns only with zeros and Dxs data&lt;-data %&gt;% select(-STDs.AIDS,-STDs.cervical.condylomatosis, -Dx.CIN, -Dx.HPV, -Dx) summarize the data summary(data) ## Age Number.of.sexual.partners First.sexual.intercourse ## Min. :13.00 Min. : 1.000 Min. :10 ## 1st Qu.:20.00 1st Qu.: 2.000 1st Qu.:15 ## Median :25.00 Median : 2.000 Median :17 ## Mean :26.82 Mean : 2.528 Mean :17 ## 3rd Qu.:32.00 3rd Qu.: 3.000 3rd Qu.:18 ## Max. :84.00 Max. :28.000 Max. :32 ## NA&#39;s :26 NA&#39;s :7 ## Num.of.pregnancies Smokes Smokes..years. Smokes..packs.year. ## Min. : 0.000 no :722 Min. : 0.00 Min. : 0.0000 ## 1st Qu.: 1.000 yes :123 1st Qu.: 0.00 1st Qu.: 0.0000 ## Median : 2.000 NA&#39;s: 13 Median : 0.00 Median : 0.0000 ## Mean : 2.276 Mean : 1.22 Mean : 0.4531 ## 3rd Qu.: 3.000 3rd Qu.: 0.00 3rd Qu.: 0.0000 ## Max. :11.000 Max. :37.00 Max. :37.0000 ## NA&#39;s :56 NA&#39;s :13 NA&#39;s :13 ## Hormonal.Contraceptives Hormonal.Contraceptives..years. IUD ## no :269 Min. : 0.000 no :658 ## yes :481 1st Qu.: 0.000 yes : 83 ## NA&#39;s:108 Median : 0.500 NA&#39;s:117 ## Mean : 2.256 ## 3rd Qu.: 3.000 ## Max. :30.000 ## NA&#39;s :108 ## IUD..years. STDs STDs..number. STDs.condylomatosis ## Min. : 0.0000 no :674 Min. :0.0000 no :709 ## 1st Qu.: 0.0000 yes : 79 1st Qu.:0.0000 yes : 44 ## Median : 0.0000 NA&#39;s:105 Median :0.0000 NA&#39;s:105 ## Mean : 0.5148 Mean :0.1766 ## 3rd Qu.: 0.0000 3rd Qu.:0.0000 ## Max. :19.0000 Max. :4.0000 ## NA&#39;s :117 NA&#39;s :105 ## STDs.vaginal.condylomatosis STDs.vulvo.perineal.condylomatosis ## no :749 no :710 ## yes : 4 yes : 43 ## NA&#39;s:105 NA&#39;s:105 ## ## ## ## ## STDs.syphilis STDs.pelvic.inflammatory.disease STDs.genital.herpes ## no :735 no :752 no :752 ## yes : 18 yes : 1 yes : 1 ## NA&#39;s:105 NA&#39;s:105 NA&#39;s:105 ## ## ## ## ## STDs.molluscum.contagiosum STDs.HIV STDs.Hepatitis.B STDs.HPV ## no :752 no :735 no :752 no :751 ## yes : 1 yes : 18 yes : 1 yes : 2 ## NA&#39;s:105 NA&#39;s:105 NA&#39;s:105 NA&#39;s:105 ## ## ## ## ## STDs..Number.of.diagnosis STDs..Time.since.first.diagnosis ## Min. :0.00000 Min. : 1.000 ## 1st Qu.:0.00000 1st Qu.: 2.000 ## Median :0.00000 Median : 4.000 ## Mean :0.08741 Mean : 6.141 ## 3rd Qu.:0.00000 3rd Qu.: 8.000 ## Max. :3.00000 Max. :22.000 ## NA&#39;s :787 ## STDs..Time.since.last.diagnosis Dx.Cancer Hinselmann Schiller Citology ## Min. : 1.000 no :840 no :823 no :784 no :814 ## 1st Qu.: 2.000 yes: 18 yes: 35 yes: 74 yes: 44 ## Median : 3.000 ## Mean : 5.817 ## 3rd Qu.: 7.500 ## Max. :22.000 ## NA&#39;s :787 ## Biopsy ## no :803 ## yes: 55 ## ## ## ## ## table(data$Dx.Cancer) ## ## no yes ## 840 18 missing data library(Amelia) missmap(data) new data # variables with a low number of missings vars&lt;-c(&quot;Age&quot;,&quot;Hinselmann&quot;,&quot;Schiller&quot;,&quot;Citology&quot;,&quot;Biopsy&quot;,&quot;Number.of.sexual.partners&quot; ,&quot;Smokes..packs.year.&quot;,&quot;Smokes..years.&quot;,&quot;Smokes&quot;, &quot;First.sexual.intercourse&quot;, &quot;STDs..Number.of.diagnosis&quot;, &quot;Dx.Cancer&quot;) data.new&lt;-data %&gt;% select(vars) 4.4.2 Classification tree model1 &lt;- rpart(Dx.Cancer ~ ., data = data.new, method=&quot;class&quot;, control =rpart.control(minsplit =1,minbucket=3, cp=0) ) par(xpd = NA) # otherwise on some devices the text is clipped plot(model1) text(model1, digits = 3) print(model1, digits = 2) ## n= 858 ## ## node), split, n, loss, yval, (yprob) ## * denotes terminal node ## ## 1) root 858 18 no (0.9790 0.0210) ## 2) Biopsy=no 803 12 no (0.9851 0.0149) ## 4) Smokes..packs.year.&lt; 20 800 11 no (0.9863 0.0138) ## 8) First.sexual.intercourse&lt; 18 646 4 no (0.9938 0.0062) * ## 9) First.sexual.intercourse&gt;=18 154 7 no (0.9545 0.0455) ## 18) Citology=no 148 5 no (0.9662 0.0338) ## 36) Schiller=no 145 4 no (0.9724 0.0276) ## 72) Age&lt; 44 141 3 no (0.9787 0.0213) ## 144) Number.of.sexual.partners&lt; 2.5 97 0 no (1.0000 0.0000) * ## 145) Number.of.sexual.partners&gt;=2.5 44 3 no (0.9318 0.0682) ## 290) First.sexual.intercourse&gt;=20 19 0 no (1.0000 0.0000) * ## 291) First.sexual.intercourse&lt; 20 25 3 no (0.8800 0.1200) ## 582) Age&lt; 30 10 0 no (1.0000 0.0000) * ## 583) Age&gt;=30 15 3 no (0.8000 0.2000) ## 1166) Age&gt;=32 12 1 no (0.9167 0.0833) * ## 1167) Age&lt; 32 3 1 yes (0.3333 0.6667) * ## 73) Age&gt;=44 4 1 no (0.7500 0.2500) * ## 37) Schiller=yes 3 1 no (0.6667 0.3333) * ## 19) Citology=yes 6 2 no (0.6667 0.3333) * ## 5) Smokes..packs.year.&gt;=20 3 1 no (0.6667 0.3333) * ## 3) Biopsy=yes 55 6 no (0.8909 0.1091) ## 6) First.sexual.intercourse&lt; 18 33 1 no (0.9697 0.0303) * ## 7) First.sexual.intercourse&gt;=18 22 5 no (0.7727 0.2273) ## 14) Hinselmann=no 12 1 no (0.9167 0.0833) * ## 15) Hinselmann=yes 10 4 no (0.6000 0.4000) ## 30) Age&gt;=22 7 2 no (0.7143 0.2857) ## 60) First.sexual.intercourse&lt; 18 4 0 no (1.0000 0.0000) * ## 61) First.sexual.intercourse&gt;=18 3 1 yes (0.3333 0.6667) * ## 31) Age&lt; 22 3 1 yes (0.3333 0.6667) * ## accuracy on train data predicted.classes &lt;- model1 %&gt;% predict(data.new, type = &quot;class&quot;) # Compute model accuracy mean(predicted.classes == data.new$Dx.Cancer) ## [1] 0.9825175 train model Tree with repeated 3*10-fold CV # set definitions metric &lt;- &quot;ROC&quot; control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE, repeats=3) # regression tree with repeated 3*10-fold CV set.seed(1) model2 &lt;- train( Dx.Cancer ~ ., data = data.new, method = &quot;rpart&quot;, trControl = control, metric = metric, na.action=na.exclude #tuneLength = 10 ) Accuracy of the model2 predicted.prob &lt;- model2 %&gt;% predict(data, type = &quot;prob&quot;) pred.class&lt;-ifelse(predicted.prob$yes &lt; 0.5, &quot;no&quot;, &quot;yes&quot;) mean(pred.class == data.new[complete.cases(data.new),]$Dx.Cancer) ## [1] 0.9791155 4.4.3 Random forest with repeated 3*10-fold CV set.seed(1) model3 &lt;- train( Dx.Cancer ~ ., data = data.new, method = &quot;rf&quot;, trControl = control, metric = metric, na.action=na.exclude ) compare models fit.models &lt;- list(rpart=model2, rf=model3) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, rf ## Number of resamples: 30 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.5000000 0.5000000 0.5000000 0.5000000 0.5000000 0.5 0 ## rf 0.3101266 0.6453125 0.8176424 0.7492932 0.9367089 1.0 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 1.000 1 1 1.0000000 1 1 0 ## rf 0.975 1 1 0.9974895 1 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0 0 0 0.00000000 0 0.0 0 ## rf 0 0 0 0.01666667 0 0.5 0 plot ROC par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$yes, main=paste(&quot;3x10-fold-CV&quot;,fit$method), debug=F, print.auc=T)}) compare accuracy of models dotplot(results) Inspect model Tree print(model2) ## CART ## ## 858 samples ## 11 predictor ## 2 classes: &#39;no&#39;, &#39;yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 733, 732, 732, 732, 732, 734, ... ## Resampling results: ## ## ROC Sens Spec ## 0.5 1 0 ## ## Tuning parameter &#39;cp&#39; was held constant at a value of 0 inspect model Random Forest print(model3) ## Random Forest ## ## 858 samples ## 11 predictor ## 2 classes: &#39;no&#39;, &#39;yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 733, 732, 732, 732, 732, 734, ... ## Resampling results across tuning parameters: ## ## mtry ROC Sens Spec ## 2 0.7410562 1.0000000 0.00000000 ## 6 0.7492932 0.9974895 0.01666667 ## 11 0.7430512 0.9891245 0.08333333 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 6. ROC / complexity for models Tree #plot(model2) RF plot(model3) 4.4.4 Improve Random Forest myGrid&lt;-expand.grid(mtry=1:11) # number of independent variables set.seed(1) model3.tune &lt;- train(Dx.Cancer ~ ., data=data.new, method=&quot;rf&quot;, metric=metric, trControl=control, na.action=na.exclude, tuneGrid = myGrid) compare models fit.models&lt;-list(rpart=model2,rf=model3,rf.tune=model3.tune) results&lt;-resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, rf, rf.tune ## Number of resamples: 30 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.5000000 0.5000000 0.5000000 0.5000000 0.5000000 0.5 0 ## rf 0.3101266 0.6453125 0.8176424 0.7492932 0.9367089 1.0 0 ## rf.tune 0.3101266 0.6421875 0.7687500 0.7562777 0.9462025 1.0 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 1.000 1.000000 1 1.0000000 1 1 0 ## rf 0.975 1.000000 1 0.9974895 1 1 0 ## rf.tune 0.975 0.990625 1 0.9962342 1 1 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0 0 0 0.00000000 0 0.0 0 ## rf 0 0 0 0.01666667 0 0.5 0 ## rf.tune 0 0 0 0.05000000 0 1.0 0 ROC curves par(mfrow=c(1,3)) rocs&lt;-lapply(fit.models, function(fit){plot.roc(fit$pred$obs, fit$pred$yes, main=paste(&quot;3x10-fold CV-&quot;,fit$method),debug=F,print.auc=T)}) ROC, sens and spec dotplot(results) http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/141-cart-model-decision-tree-essentials https://bradleyboehmke.github.io/HOML/DT.html https://www.machinelearningplus.com/machine-learning/logistic-regression-tutorial-examples-r/ "],
["assignment-trees-up-sampling.html", "4.5 Assignment Trees - Up Sampling", " 4.5 Assignment Trees - Up Sampling In this excercise we have a disparity in the frequencies of the observed classes we want to predict. For resolving such a class imbalance we can up sampling our data as described elsewhere: https://topepo.github.io/caret/subsampling-for-class-imbalances.html#subsampling-techniques knitr::opts_chunk$set(echo = TRUE, warning = F, message = F) 4.5.1 load data library(tidyverse) library(caret) library(rpart) library(rpart.plot) library(pROC) data &lt;- read.csv(&quot;2.UploadedData/risk_factors_cervical_cancer.csv&quot;, na.strings = &quot;?&quot;) ## select columns 0/1 colMax &lt;- function(data) sapply(data, max, na.rm = TRUE) cols&lt;-colMax(data) %&gt;% data.frame(max=.) %&gt;% rownames_to_column(&quot;cols&quot;) %&gt;% filter(max==1) %&gt;% .$cols # factorize columns 0/1 data&lt;-data %&gt;% mutate_at(cols, ~factor(.,levels = 0:1, labels = c(&quot;no&quot;,&quot;yes&quot;))) # excluded columns only with zeros and Dxs data&lt;-data %&gt;% select(-STDs.AIDS,-STDs.cervical.condylomatosis, -Dx.CIN, -Dx.HPV, -Dx) summarize the data summary(data) ## Age Number.of.sexual.partners First.sexual.intercourse ## Min. :13.00 Min. : 1.000 Min. :10 ## 1st Qu.:20.00 1st Qu.: 2.000 1st Qu.:15 ## Median :25.00 Median : 2.000 Median :17 ## Mean :26.82 Mean : 2.528 Mean :17 ## 3rd Qu.:32.00 3rd Qu.: 3.000 3rd Qu.:18 ## Max. :84.00 Max. :28.000 Max. :32 ## NA&#39;s :26 NA&#39;s :7 ## Num.of.pregnancies Smokes Smokes..years. Smokes..packs.year. ## Min. : 0.000 no :722 Min. : 0.00 Min. : 0.0000 ## 1st Qu.: 1.000 yes :123 1st Qu.: 0.00 1st Qu.: 0.0000 ## Median : 2.000 NA&#39;s: 13 Median : 0.00 Median : 0.0000 ## Mean : 2.276 Mean : 1.22 Mean : 0.4531 ## 3rd Qu.: 3.000 3rd Qu.: 0.00 3rd Qu.: 0.0000 ## Max. :11.000 Max. :37.00 Max. :37.0000 ## NA&#39;s :56 NA&#39;s :13 NA&#39;s :13 ## Hormonal.Contraceptives Hormonal.Contraceptives..years. IUD ## no :269 Min. : 0.000 no :658 ## yes :481 1st Qu.: 0.000 yes : 83 ## NA&#39;s:108 Median : 0.500 NA&#39;s:117 ## Mean : 2.256 ## 3rd Qu.: 3.000 ## Max. :30.000 ## NA&#39;s :108 ## IUD..years. STDs STDs..number. STDs.condylomatosis ## Min. : 0.0000 no :674 Min. :0.0000 no :709 ## 1st Qu.: 0.0000 yes : 79 1st Qu.:0.0000 yes : 44 ## Median : 0.0000 NA&#39;s:105 Median :0.0000 NA&#39;s:105 ## Mean : 0.5148 Mean :0.1766 ## 3rd Qu.: 0.0000 3rd Qu.:0.0000 ## Max. :19.0000 Max. :4.0000 ## NA&#39;s :117 NA&#39;s :105 ## STDs.vaginal.condylomatosis STDs.vulvo.perineal.condylomatosis ## no :749 no :710 ## yes : 4 yes : 43 ## NA&#39;s:105 NA&#39;s:105 ## ## ## ## ## STDs.syphilis STDs.pelvic.inflammatory.disease STDs.genital.herpes ## no :735 no :752 no :752 ## yes : 18 yes : 1 yes : 1 ## NA&#39;s:105 NA&#39;s:105 NA&#39;s:105 ## ## ## ## ## STDs.molluscum.contagiosum STDs.HIV STDs.Hepatitis.B STDs.HPV ## no :752 no :735 no :752 no :751 ## yes : 1 yes : 18 yes : 1 yes : 2 ## NA&#39;s:105 NA&#39;s:105 NA&#39;s:105 NA&#39;s:105 ## ## ## ## ## STDs..Number.of.diagnosis STDs..Time.since.first.diagnosis ## Min. :0.00000 Min. : 1.000 ## 1st Qu.:0.00000 1st Qu.: 2.000 ## Median :0.00000 Median : 4.000 ## Mean :0.08741 Mean : 6.141 ## 3rd Qu.:0.00000 3rd Qu.: 8.000 ## Max. :3.00000 Max. :22.000 ## NA&#39;s :787 ## STDs..Time.since.last.diagnosis Dx.Cancer Hinselmann Schiller Citology ## Min. : 1.000 no :840 no :823 no :784 no :814 ## 1st Qu.: 2.000 yes: 18 yes: 35 yes: 74 yes: 44 ## Median : 3.000 ## Mean : 5.817 ## 3rd Qu.: 7.500 ## Max. :22.000 ## NA&#39;s :787 ## Biopsy ## no :803 ## yes: 55 ## ## ## ## ## table(data$Dx.Cancer) ## ## no yes ## 840 18 4.5.2 Up sample data # upsample data set.seed(1) up_data &lt;- upSample(x = data[, !colnames(data) %in% &quot;Dx.Cancer&quot;], y = data$Dx.Cancer) # sumarize it # now the dependent variable is named &#39;Class&#39; table(up_data$Class) ## ## no yes ## 840 840 missing data library(Amelia) # missings on original data missmap(data) new data # select only variable with low number of missings vars&lt;-c(&quot;Age&quot;,&quot;Hinselmann&quot;,&quot;Schiller&quot;,&quot;Citology&quot;,&quot;Biopsy&quot;,&quot;Number.of.sexual.partners&quot; ,&quot;Smokes..packs.year.&quot;,&quot;Smokes..years.&quot;,&quot;Smokes&quot;, &quot;First.sexual.intercourse&quot;, &quot;STDs..Number.of.diagnosis&quot;, &quot;Class&quot;) up_data&lt;-up_data %&gt;% select(vars) 4.5.3 Classification tree by two methods # learn tree with Gini impurity tree.gini&lt;-rpart(Class~.,data = up_data, cp = 0.1) # learn tree with information gain tree.information&lt;-rpart(Class~., data=up_data, parms=list(split=&quot;information&quot;)) par(mfrow=c(1,2)) plot(tree.information,main=&quot;Information&quot;) text(tree.information) plot(tree.gini, main=&quot;Gini&quot;) text(tree.gini) 4.5.4 train models CART and RF with repeated 3*10-fold CV # # set definitions metric &lt;- &quot;ROC&quot; control &lt;- trainControl(method=&quot;repeatedcv&quot;, number=10, summaryFunction=twoClassSummary, classProbs=T, savePredictions = TRUE, repeats=3) set.seed(7) fit.cart.rcv &lt;- train(Class ~ ., data=up_data, method=&quot;rpart&quot;, metric=metric, trControl=control, na.action=na.exclude) set.seed(7) fit.rf.rcv &lt;- train(Class ~ ., data=up_data, method=&quot;rf&quot;, metric=metric, trControl=control, na.action=na.exclude) compare CART with RF fit.models &lt;- list(rpart=fit.cart.rcv, rf=fit.rf.rcv) results &lt;- resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, rf ## Number of resamples: 30 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.7135285 0.7658426 0.802456 0.7964733 0.8246275 0.8639241 0 ## rf 0.9937500 1.0000000 1.000000 0.9997116 1.0000000 1.0000000 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.4125 0.49375 0.55625 0.5578270 0.621875 0.725 0 ## rf 0.9500 0.97500 0.98750 0.9841086 0.996875 1.000 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 1 1 1 1 1 1 0 ## rf 1 1 1 1 1 1 0 par(mfrow=c(1,2)) rocs &lt;- lapply(fit.models, function(fit){plot.roc(fit$pred$obs,fit$pred$yes, main=paste(&quot;3x10-fold-CV&quot;,fit$method), debug=F, print.auc=T)}) dotplot(results) print(fit.cart.rcv) ## CART ## ## 1680 samples ## 11 predictor ## 2 classes: &#39;no&#39;, &#39;yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 1428, 1428, 1428, 1428, 1428, 1429, ... ## Resampling results across tuning parameters: ## ## cp ROC Sens Spec ## 0.07341772 0.7964733 0.5578270 1.0000000 ## 0.10632911 0.7422564 0.5414926 0.9299578 ## 0.42405063 0.6123734 0.7104008 0.5143460 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was cp = 0.07341772. print(fit.rf.rcv) ## Random Forest ## ## 1680 samples ## 11 predictor ## 2 classes: &#39;no&#39;, &#39;yes&#39; ## ## No pre-processing ## Resampling: Cross-Validated (10 fold, repeated 3 times) ## Summary of sample sizes: 1428, 1428, 1428, 1428, 1428, 1429, ... ## Resampling results across tuning parameters: ## ## mtry ROC Sens Spec ## 2 0.9953841 0.9259124 1 ## 6 0.9997116 0.9841086 1 ## 11 0.9993697 0.9811709 1 ## ## ROC was used to select the optimal model using the largest value. ## The final value used for the model was mtry = 6. plot(fit.cart.rcv) plot(fit.rf.rcv) tunning RF model myGrid&lt;-expand.grid(mtry=1:11) # number of variables when constructing the RF set.seed(7) fit.rf.rcv.tune &lt;- train(Class ~ ., data=up_data, method=&quot;rf&quot;, metric=metric, trControl=control, tuneGrid = myGrid, na.action=na.exclude) compare the 3 models fit.models&lt;-list(rpart=fit.cart.rcv,rf = fit.rf.rcv, rf.tune=fit.rf.rcv.tune) results&lt;-resamples(fit.models) summary(results) ## ## Call: ## summary.resamples(object = results) ## ## Models: rpart, rf, rf.tune ## Number of resamples: 30 ## ## ROC ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.7135285 0.7658426 0.802456 0.7964733 0.8246275 0.8639241 0 ## rf 0.9937500 1.0000000 1.000000 0.9997116 1.0000000 1.0000000 0 ## rf.tune 0.9978369 1.0000000 1.000000 0.9998749 1.0000000 1.0000000 0 ## ## Sens ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 0.4125000 0.49375 0.55625 0.5578270 0.6218750 0.725 0 ## rf 0.9500000 0.97500 0.98750 0.9841086 0.9968750 1.000 0 ## rf.tune 0.9240506 0.96250 0.97500 0.9719673 0.9873418 1.000 0 ## ## Spec ## Min. 1st Qu. Median Mean 3rd Qu. Max. NA&#39;s ## rpart 1 1 1 1 1 1 0 ## rf 1 1 1 1 1 1 0 ## rf.tune 1 1 1 1 1 1 0 ROCs from models par(mfrow=c(1,3)) rocs&lt;-lapply(fit.models, function(fit){plot.roc(fit$pred$obs, fit$pred$yes, main=paste(&quot;3x10-fold CV-&quot;,fit$method),debug=F,print.auc=T)}) Compare accuracy of models dotplot(results) http://www.sthda.com/english/articles/35-statistical-machine-learning-essentials/141-cart-model-decision-tree-essentials https://bradleyboehmke.github.io/HOML/DT.html https://topepo.github.io/caret/subsampling-for-class-imbalances.html#subsampling-techniques https://www.machinelearningplus.com/machine-learning/logistic-regression-tutorial-examples-r/ "],
["bayes.html", "5 BAYES", " 5 BAYES Modelação Estatística Bayesiana Conteúdos programáticos Inferência Bayesiana: Introdução à inferência Bayesiana: Probabilidade e parâmetros; Inferência frequentista clássica versus inferência Bayesiana; Fundamentos da inferência Bayesiana; Distribuições a priori; Distribuições a posteriori; Distribuições preditivas a posteriori; Modelos Bayesianos básicos; Modelação hierárquica; Avaliação dos modelos. Construção de modelos de inferência Bayesiana: Inferência Bayesiana com distribuições a priori conjugadas; Computação Bayesiana – métodos de Monte Carlo, métodos de Monte Carlo via Cadeias de Markov (MCMC), algoritmo de Metropolis-Hastings, algoritmo de Gibbs e outros algoritmos relacionados; Métodos de avaliação da qualidade dos modelos (escolha de valores iniciais, convergência, eficiência e precisão); Métodos de selecção de modelos; Aplicação dos métodos de inferência Bayesiana a problemas mais comuns de inferência estatística – modelos de regressão, análise de dados categóricos e modelos de síntese de evidência. Redes Bayesianas: Introdução às Redes Bayesianas: Motivação e exemplos; Probabilidade e aplicações médicas; Modelos gráficos de probabilidade; Semântica e factorização nas redes Bayesianas. Construção de redes Bayesianas a partir de dados: Aprendizagem automática; Estimação de parâmetros de redes Bayesianas; Aprendizagem da estrutura de redes Bayesianas; Aprendizagem com dados incompletos. "],
["top.html", "6 TOP.HIDA", " 6 TOP.HIDA Tópicos avançados em HIDA Conteúdos programáticos Os conteúdos abrangerão as diversas áreas da análise inteligente de dados, nomeadamente metodologias de visualização, pré-processamento, exploração e classificação de dados, proporcionando também a expansão dos conteúdos de outras UC relacionadas. A escolha dos tópicos será feita em cada edição desta unidade curricular, tendo em consideração não só o perfil e interesses dos alunos, como também os tópicos que no momento se considerem ser os mais relevantes na área. "],
["seminar1.html", "6.1 Seminar 1", " 6.1 Seminar 1 2020-02-04 Physiological signal processing in affective computing by Susana Brás Susana Brás is Postdoctoral researcher at Institute of Electronics and Telematics Engineering of Aveiro (IEETA), University of Aveiro, Portugal. At this moment, she is focused on ECG biometric identification, emotional modulated environments and information extraction from large biomedical databases. Before, she was a PhD student at Abel Salazar Biomedical Sciences Institute, at University of Porto, Portugal. Her thesis was focused on automation in anesthesia, basically the drug distribution and effect (alterations in the brain electrical activity) were studied, modeled and a controller was presented for the hypnotic effect. Her background is in Applied Mathematics, from the Sciences Faculty, at University of Porto. 6.1.1 Bruno Lima Podemos definir sinais como dados indexados a uma variável temporal e que podem ser oriundos de campos tão diversos como a biomedicina ou as comunicações sem fios. Neste seminário foi-nos apresentado o conceito de computação afectiva que consiste em identificar emoções a partir do processamento de sinais fisiológicos. O conceito de emoção é de difícil definição ainda que a caracterização das emoções seja algo mais perceptível. Conseguimos reconhecer emoções através de respostas fisiológicas como sejam o batimento cardíaco, a expressão facial, a tensão, o riso, a rubescência ou a sudação embora estas reacções do corpo variem de pessoa para pessoa. Com a ajuda de disciplinas como a psicofisiologia ou a psiconeuroimunologia pretende-se fazer a caracterização das emoções que permitam treinar algoritmos que identifiquem essas emoções e que assim auxiliem sistemas a tomar respostas adequadas às emoções identificadas. O desafio é o de criar um sistema que reconheça, interprete, processe ou simule afectos humanos. Esta computação afectiva procurará ser uma ajuda perante diferentes estados emocionais. Podemos imaginar uma robot que seja de facto uma companhia para os humanos e um prestador de cuidados emocionais. A criação de um sistema como descrito está sujeito, não só, a grandes desafios tecnológicos (como sejam: a monitorização, armazenamento, processamento, data mining e machine learning) mas também desafios éticos. Com a identificação de emoções identificam-se também preferências e consequentemente surge a possibilidade de manipular essas preferências. No campo da saúde, a chamada ‘emotional analytics’ permite prever diagnósticos perante a identificação de emoções e consequentemente actuar preventivamente minimizando danos mais graves. Em campos como a saúde mental ou a saúde geriátrica a correcta classificação das emoções dos doentes é a chave para prestar os melhores cuidados de saúde. "],
["references.html", "References", " References "]
]
