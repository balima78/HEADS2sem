@Book{xie2015,
  title = {Dynamic Documents with {R} and knitr},
  author = {Yihui Xie},
  publisher = {Chapman and Hall/CRC},
  address = {Boca Raton, Florida},
  year = {2015},
  edition = {2nd},
  note = {ISBN 978-1498716963},
  url = {http://yihui.name/knitr/},
}

@article{Chawla2002,
abstract = {An approach to the construction of classifiers from imbalanced datasets is described. A dataset is imbalanced if the classification categories are not approximately equally represented. Often real-world data sets are predominately composed of "normal" examples with only a small percentage of "abnormal" or "interesting" examples. It is also the case that the cost of misclassifying an abnormal (interesting) example as a normal example is often much higher than the cost of the reverse error. Under-sampling of the majority (nor-mal) class has been proposed as a good means of increasing the sensitivity of a classifier to the minority class. This paper shows that a combination of our method of over-sampling the minority (abnormal) class and under-sampling the majority (normal) class can achieve better classifier performance (in ROC space) than only under-sampling the majority class. This paper also shows that a combination of our method of over-sampling the minority class and under-sampling the majority class can achieve better classifier performance (in ROC space) than varying the loss ratios in Ripper or class priors in Naive Bayes. Our method of over-sampling the minority class involves creating synthetic minority class examples. Experiments are performed using C4.5, Ripper and a Naive Bayes classifier. The method is evaluated using the area under the Receiver Operating Characteristic curve (AUC) and the ROC convex hull strategy.},
archivePrefix = {arXiv},
arxivId = {1106.1813},
author = {Chawla, Nitesh V. and Bowyer, Kevin W. and Hall, Lawrence O. and Kegelmeyer, W. Philip},
doi = {10.1613/jair.953},
eprint = {1106.1813},
file = {:D$\backslash$:/PhD/HEADS/TOP.HIDA/2002, Chawla.pdf:pdf},
isbn = {013805326X},
issn = {10769757},
journal = {Journal of Artificial Intelligence Research},
number = {Sept. 28},
pages = {321--357},
pmid = {18190633},
title = {{snopes.com: Two-Striped Telamonia Spider}},
url = {https://arxiv.org/pdf/1106.1813.pdf{\%}0Ahttp://www.snopes.com/horrors/insects/telamonia.asp},
volume = {16},
year = {2002}
}
@article{He2009,
abstract = {In this chapter, we consider the classification of imbalanced data. When a dataset presents an imbalance between its classes, that is, an uneven distribution of observations among them, the classification task is inherently more challenging. Traditional classification algorithms (see Sect. 2.2) tend to favour majority over minority class elements due to their incorrect implicit assumption of an equal class representation during learning. As a consequence, the recognition of minority instances is hampered. Since minority classes are usually the ones of interest, custom techniques are required to deal with such data skewness. We study them in this chapter.},
author = {He, Haibo and Garcia, Edwardo A.},
doi = {10.1007/978-3-030-04663-7_4},
file = {:D$\backslash$:/PhD/HEADS/TOP.HIDA/2009, He.pdf:pdf},
issn = {1860949X},
journal = {IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING},
number = {9},
pages = {1263--1283},
title = {{Learning from imbalanced data}},
volume = {21},
year = {2009}
}
@article{Gao2014,
abstract = {This contribution proposes a novel probability density function (PDF) estimation based over-sampling (PDFOS) approach for two-class imbalanced classification problems. The classical Parzen-window kernel function is adopted to estimate the PDF of the positive class. Then according to the estimated PDF, synthetic instances are generated as the additional training data. The essential concept is to re-balance the class distribution of the original imbalanced data set under the principle that synthetic data sample follows the same statistical properties. Based on the over-sampled training data, the radial basis function (RBF) classifier is constructed by applying the orthogonal forward selection procedure, in which the classifier's structure and the parameters of RBF kernels are determined using a particle swarm optimisation algorithm based on the criterion of minimising the leave-one-out misclassification rate. The effectiveness of the proposed PDFOS approach is demonstrated by the empirical study on several imbalanced data sets. {\textcopyright} 2014 Elsevier B.V.},
author = {Gao, Ming and Hong, Xia and Chen, Sheng and Harris, Chris J. and Khalaf, Emad},
doi = {10.1016/j.neucom.2014.02.006},
file = {:D$\backslash$:/PhD/HEADS/TOP.HIDA/2014, Gao.pdf:pdf},
issn = {18728286},
journal = {Neurocomputing},
keywords = {Imbalanced classification,Orthogonal forward selection,Particle swarm optimisation,Probability density function based over-sampling,Radial basis function classifier},
pages = {248--259},
publisher = {Elsevier},
title = {{PDFOS: PDF estimation based over-sampling for imbalanced two-class problems}},
url = {http://dx.doi.org/10.1016/j.neucom.2014.02.006},
volume = {138},
year = {2014}
}
@Article{amelia,
    title = {{Amelia II}: A Program for Missing Data},
    author = {James Honaker and Gary King and Matthew Blackwell},
    journal = {Journal of Statistical Software},
    year = {2011},
    volume = {45},
    number = {7},
    pages = {1--47},
    url = {http://www.jstatsoft.org/v45/i07/},
}
@Manual{r,
    title = {R: A Language and Environment for Statistical Computing},
    author = {{R Core Team}},
    organization = {R Foundation for Statistical Computing},
    address = {Vienna, Austria},
    year = {2013},
    url = {http://www.R-project.org/},
}
@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" }